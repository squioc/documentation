{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"SEKOIA.IO Documentations"},{"location":"getting_started/","text":"Welcome to SEKOIA.IO SEKOIA.IO is a Cybersecurity Platform as a Service , based on our ability to leverage threat intelligence to dynamically integrate it into an innovative approach of the cyberdefense. SEKOIA.IO architecture couples the generation and operation of Threat Intelligence dynamic databases to a set of functions for analysis, guidance and treatment. This technology brings together automation, sharing of indicators and implementation of playbooks to treat, on a large scale and in real time, the flows exchanged and data from other tools already deployed on the networks (internal security logs). There are two main parts in SEKOIA.IO: The Intelligence Center , a Threat Intelligence knowledge base constantly updated by SEKOIA analysts The Operation Center ,a SIEM application that can trigger alerts from the security events received, based on two principles: CTI rules based on dedicated CTI indicator feeds generated by SEKOIA. Correlation rules between events. In addition to a web interface, SEKOIA.IO provides REST/API for external apps for almost all of its features. In order to test SEKOIA.IO, you just have to create an account and send your logs to the platform.","title":"Overview"},{"location":"getting_started/#welcome-to-sekoiaio","text":"SEKOIA.IO is a Cybersecurity Platform as a Service , based on our ability to leverage threat intelligence to dynamically integrate it into an innovative approach of the cyberdefense. SEKOIA.IO architecture couples the generation and operation of Threat Intelligence dynamic databases to a set of functions for analysis, guidance and treatment. This technology brings together automation, sharing of indicators and implementation of playbooks to treat, on a large scale and in real time, the flows exchanged and data from other tools already deployed on the networks (internal security logs). There are two main parts in SEKOIA.IO: The Intelligence Center , a Threat Intelligence knowledge base constantly updated by SEKOIA analysts The Operation Center ,a SIEM application that can trigger alerts from the security events received, based on two principles: CTI rules based on dedicated CTI indicator feeds generated by SEKOIA. Correlation rules between events. In addition to a web interface, SEKOIA.IO provides REST/API for external apps for almost all of its features. In order to test SEKOIA.IO, you just have to create an account and send your logs to the platform.","title":"Welcome to SEKOIA.IO"},{"location":"getting_started/2fa/","text":"Securing your SEKOIA account with two-factor authentication Two-factor authentication adds additional security to your SEKOIA account by requiring a second step to sign in. It requires you to give a 6-digit verification code generated from your phone in addition to your username and password login. When two-factor authentication is enabled, you will need your password and a verification code from your phone whenever you sign in on SEKOIA.IO platform. How do I enable two-factor authentication? Log in to SEKOIA.IO platform Click on your profile picture and select settings to access the User center Under User > Security, click on the button Enable Two-Factor Authentication Follow the steps to complete the process Enter your password Set up your authenticator app (see \u2018Get the code through an authenticator app\u2019 below for more information) Enter your 6-digit code Generate and save your backup codes Get the code through an authenticator app A time-based one-time password (TOTP) application automatically generates an authentication code that changes after a certain period of time. Here are a few we suggest: Google Authenticator Authy LastPass authenticator 1Password To use one of these apps: While enabling, you\u2019ll see a modal with a QR code you will use to register your SEKOIA account with your authenticator app. Open your authenticator and follow the instructions given to you and scan the QR code when asked by the app. Once your app is configured, enter the security code generated by your authenticator app to complete the two-factor authentication process. Generate backup codes If you lose your mobile device or cannot use your authenticator app, you can use backup codes provided by SEKOIA to access your account. Ten backup codes are generated. Each code can be used only one time . When enabling two-factor authentication, backup codes are generated automatically and you can either copy them or download them as a text file. You can also generate new backup codes but keep in mind that your old codes will not work anymore. We recommend you print off and store your codes in a safe location. Log in with backup codes To log in with your backup codes, you will need to: Locate your backup codes Sign into SEKOIA.IO Enter your username and password When asked for your verification code, enter the backup code and select verify How to disable two-factor authentication To disable two-factor authentication on your SEKOIA account: Log in to your SEKOIA account Click on your profile picture and select settings to access the User center Under User > Security, click on the button Disable Two-Factor Authentication Enter your password and select disable","title":"2-Factors Authentication"},{"location":"getting_started/2fa/#securing-your-sekoia-account-with-two-factor-authentication","text":"Two-factor authentication adds additional security to your SEKOIA account by requiring a second step to sign in. It requires you to give a 6-digit verification code generated from your phone in addition to your username and password login. When two-factor authentication is enabled, you will need your password and a verification code from your phone whenever you sign in on SEKOIA.IO platform.","title":"Securing your SEKOIA account with two-factor authentication"},{"location":"getting_started/2fa/#how-do-i-enable-two-factor-authentication","text":"Log in to SEKOIA.IO platform Click on your profile picture and select settings to access the User center Under User > Security, click on the button Enable Two-Factor Authentication Follow the steps to complete the process Enter your password Set up your authenticator app (see \u2018Get the code through an authenticator app\u2019 below for more information) Enter your 6-digit code Generate and save your backup codes","title":"How do I enable two-factor authentication?"},{"location":"getting_started/2fa/#get-the-code-through-an-authenticator-app","text":"A time-based one-time password (TOTP) application automatically generates an authentication code that changes after a certain period of time. Here are a few we suggest: Google Authenticator Authy LastPass authenticator 1Password To use one of these apps: While enabling, you\u2019ll see a modal with a QR code you will use to register your SEKOIA account with your authenticator app. Open your authenticator and follow the instructions given to you and scan the QR code when asked by the app. Once your app is configured, enter the security code generated by your authenticator app to complete the two-factor authentication process.","title":"Get the code through an authenticator app"},{"location":"getting_started/2fa/#generate-backup-codes","text":"If you lose your mobile device or cannot use your authenticator app, you can use backup codes provided by SEKOIA to access your account. Ten backup codes are generated. Each code can be used only one time . When enabling two-factor authentication, backup codes are generated automatically and you can either copy them or download them as a text file. You can also generate new backup codes but keep in mind that your old codes will not work anymore. We recommend you print off and store your codes in a safe location.","title":"Generate backup codes"},{"location":"getting_started/2fa/#log-in-with-backup-codes","text":"To log in with your backup codes, you will need to: Locate your backup codes Sign into SEKOIA.IO Enter your username and password When asked for your verification code, enter the backup code and select verify","title":"Log in with backup codes"},{"location":"getting_started/2fa/#how-to-disable-two-factor-authentication","text":"To disable two-factor authentication on your SEKOIA account: Log in to your SEKOIA account Click on your profile picture and select settings to access the User center Under User > Security, click on the button Disable Two-Factor Authentication Enter your password and select disable","title":"How to disable two-factor authentication"},{"location":"getting_started/apikey_creation/","text":"Creation of a new API KEY Before starting, click on 'Settings' on the top right corner in order to access the User Center. You can create API keys that will be useful for example to automate the extraction of data from the intelligence center. For this use, the role 'Intelligence Center Read-Only' should be given to the API key created.","title":"API Key creation"},{"location":"getting_started/apikey_creation/#creation-of-a-new-api-key","text":"Before starting, click on 'Settings' on the top right corner in order to access the User Center. You can create API keys that will be useful for example to automate the extraction of data from the intelligence center. For this use, the role 'Intelligence Center Read-Only' should be given to the API key created.","title":"Creation of a new API KEY"},{"location":"getting_started/first_steps/","text":"Getting started If you received an invitation to join a community and you do not already have an account on SEKOIA.IO, you will have to create one. When opening the invitation by clicking on 'OPEN INVITATION' button, you will arrive on the account creation page. Fill-out the form while keeping in mind that you won\u2019t be able to change your username afterwards . Check \u201cI agree with the terms of service\u201d and create your account. You will then have to accept the invitation to join the community. Note Your access will depend on the permissions you are granted in the community you joined. You can access the platform from any browser and on any device. How to navigate the platform Once you are logged in into SEKOIA.IO, you can navigate through the platform using two menus: A collapsed navigation sidebar on the left that gives you access to all the work areas available on the platform. These areas are divided into subcategories: analyze, detect, investigate, remediate and configure for Operation Center, Collection for Intelligence Center. A contextual menu accessible through your profile picture in the top right that gives you access to your Settings and Communities. Note Depending on the permissions granted to you, navigation between the Intelligence Center and the Operation Center is done by clicking on the name at the top of the sidebar. Set up your account User preferences In the \u201csettings\u201d page, accessible by clicking on your avatar picture, you can edit your profile information like your name, company\u2019s name and avatar. You can define your picture profile by either uploading a PNG file, asks to use your Gravatar or use your initials for your profile picture. You can also enable 2-Factor Authentication for your account or control your privacy by activating or deactivating an opt-out cookie. All of these preferences are available through the contextual menu by clicking on \u201cSettings\u201d. Communities When you first create an account, your personal community is created along with it and you can manage it as you please. A \u201cCommunity\u201d allows you to share events, alerts, rules and information with some people. You can manage the rights of each member. You can have access to more than one community and depending on the permissions you are granted, you can have different roles in each of them. When it comes to a community you manage, you can: Edit its name and description. Add new members and assign specific roles to them to enable them to access certain features. Determine which permissions you want to give them. Manage your API keys and configure delegations.","title":"First steps"},{"location":"getting_started/first_steps/#getting-started","text":"If you received an invitation to join a community and you do not already have an account on SEKOIA.IO, you will have to create one. When opening the invitation by clicking on 'OPEN INVITATION' button, you will arrive on the account creation page. Fill-out the form while keeping in mind that you won\u2019t be able to change your username afterwards . Check \u201cI agree with the terms of service\u201d and create your account. You will then have to accept the invitation to join the community. Note Your access will depend on the permissions you are granted in the community you joined. You can access the platform from any browser and on any device.","title":"Getting started"},{"location":"getting_started/first_steps/#how-to-navigate-the-platform","text":"Once you are logged in into SEKOIA.IO, you can navigate through the platform using two menus: A collapsed navigation sidebar on the left that gives you access to all the work areas available on the platform. These areas are divided into subcategories: analyze, detect, investigate, remediate and configure for Operation Center, Collection for Intelligence Center. A contextual menu accessible through your profile picture in the top right that gives you access to your Settings and Communities. Note Depending on the permissions granted to you, navigation between the Intelligence Center and the Operation Center is done by clicking on the name at the top of the sidebar.","title":"How to navigate the platform"},{"location":"getting_started/first_steps/#set-up-your-account","text":"","title":"Set up your account"},{"location":"getting_started/first_steps/#user-preferences","text":"In the \u201csettings\u201d page, accessible by clicking on your avatar picture, you can edit your profile information like your name, company\u2019s name and avatar. You can define your picture profile by either uploading a PNG file, asks to use your Gravatar or use your initials for your profile picture. You can also enable 2-Factor Authentication for your account or control your privacy by activating or deactivating an opt-out cookie. All of these preferences are available through the contextual menu by clicking on \u201cSettings\u201d.","title":"User preferences"},{"location":"getting_started/first_steps/#communities","text":"When you first create an account, your personal community is created along with it and you can manage it as you please. A \u201cCommunity\u201d allows you to share events, alerts, rules and information with some people. You can manage the rights of each member. You can have access to more than one community and depending on the permissions you are granted, you can have different roles in each of them. When it comes to a community you manage, you can: Edit its name and description. Add new members and assign specific roles to them to enable them to access certain features. Determine which permissions you want to give them. Manage your API keys and configure delegations.","title":"Communities"},{"location":"getting_started/inviting_users_to_join_your_community/","text":"Inviting users to join your community You can invite anyone to become a member of your community using their email address. Note You can invite as many users as you need in your community. In the top right corner of SEKOIA.IO, click your profile photo, then click on Settings . On the left side of your account page, click on \"Communities\" and then on the community where you want to invite someone. On the \"Members\" tag, click \"Invite\" In the invitation panel, type the email address of your invitee, assign a role to its future account and personalize the invitation with an optional note. Click \"Send\" invitation. The invited person will receive an email to join the community. They will need to accept the invitation before becoming a member of your community.","title":"Inviting users to join your community"},{"location":"getting_started/inviting_users_to_join_your_community/#inviting-users-to-join-your-community","text":"You can invite anyone to become a member of your community using their email address. Note You can invite as many users as you need in your community. In the top right corner of SEKOIA.IO, click your profile photo, then click on Settings . On the left side of your account page, click on \"Communities\" and then on the community where you want to invite someone. On the \"Members\" tag, click \"Invite\" In the invitation panel, type the email address of your invitee, assign a role to its future account and personalize the invitation with an optional note. Click \"Send\" invitation. The invited person will receive an email to join the community. They will need to accept the invitation before becoming a member of your community.","title":"Inviting users to join your community"},{"location":"integrations/","text":"Integrations In order to protect your business, you need to know what happens. The monitoring of your network and your devices is a prerequisite to their security. SEKOIA.IO rely on your log to identify threats and malicious activities. In this chapter, you will learn how to configure your log system to make it forward your events to SEKOIA.IO. SEKOIA.IO is able to collect logs through various mechanisms, configuration on your side should be easy! Here is an overview on how integration could be done with SEKOIA.IO. SEKOIA.IO supports the following log collectors: Syslog over TLS ( intake.sekoia.io:10514 ): you can forward your events by using the Syslog protocol specified in RFC 5424. RELP over TLS ( intake.sekoia.io:11514 ): your can forward your events by using Rsyslog\u2019s reliable protocol called RELP. HTTPS ( https://intake.sekoia.io ): you can POST your JSON events. Cloud hosting polling: you can configure SEKOIA.IO to regularly retrieve your logs. If these solutions do not meet your needs, we can also configure a dedicated secured network through a VPN and retrieve your logs directly (please contact us for more information). Syslog integration We are providing documentation and example configurations on how to configure your log system for Rsyslog, but it should be easy to configure other log collectors to forward their events to SEKOIA.IO. HTTPS integration To push your events through our HTTP log collector, you have to POST your logs in the JSON format. To send us events, you should set Content-Type HTTP header to application/json . Cloud & SaaS integration SEKOIA.IO is also able to retrieve logs and data from cloud platforms, such as Microsoft Azure, Amazon Web Services or Google Cloud. Log formats Cloud and SaaS AWS CloudTrail AWS Flow Logs Microsoft Azure Active Directory Microsoft Azure MySQL Microsoft Azure Linux machines Microsoft Azure Network Watcher Microsoft Azure Windows machines Microsoft Office 365 CISCO Umbrella Dns Logs CISCO Umbrella Ip Logs CISCO Umbrella Proxy Logs Operating Systems Linux Linux / AuditBeat Windows / NXLog Windows / Log Insight Applications Alsid Apache BIND Checkpoint Cisco F5 BigIP FortiGate FortiMail FortiWeb HAProxy HarfangLab ISC DHCP NetFilter Nginx OpenSSH PaloAlto Postfix Pulse Connect Secure Sophos SpamAssassin Squid Suricata Unbound Zeek Generic Common Event Format","title":"Overview"},{"location":"integrations/#integrations","text":"In order to protect your business, you need to know what happens. The monitoring of your network and your devices is a prerequisite to their security. SEKOIA.IO rely on your log to identify threats and malicious activities. In this chapter, you will learn how to configure your log system to make it forward your events to SEKOIA.IO. SEKOIA.IO is able to collect logs through various mechanisms, configuration on your side should be easy! Here is an overview on how integration could be done with SEKOIA.IO. SEKOIA.IO supports the following log collectors: Syslog over TLS ( intake.sekoia.io:10514 ): you can forward your events by using the Syslog protocol specified in RFC 5424. RELP over TLS ( intake.sekoia.io:11514 ): your can forward your events by using Rsyslog\u2019s reliable protocol called RELP. HTTPS ( https://intake.sekoia.io ): you can POST your JSON events. Cloud hosting polling: you can configure SEKOIA.IO to regularly retrieve your logs. If these solutions do not meet your needs, we can also configure a dedicated secured network through a VPN and retrieve your logs directly (please contact us for more information).","title":"Integrations"},{"location":"integrations/#syslog-integration","text":"We are providing documentation and example configurations on how to configure your log system for Rsyslog, but it should be easy to configure other log collectors to forward their events to SEKOIA.IO.","title":"Syslog integration"},{"location":"integrations/#https-integration","text":"To push your events through our HTTP log collector, you have to POST your logs in the JSON format. To send us events, you should set Content-Type HTTP header to application/json .","title":"HTTPS integration"},{"location":"integrations/#cloud-saas-integration","text":"SEKOIA.IO is also able to retrieve logs and data from cloud platforms, such as Microsoft Azure, Amazon Web Services or Google Cloud.","title":"Cloud &amp; SaaS integration"},{"location":"integrations/#log-formats","text":"","title":"Log formats"},{"location":"integrations/#cloud-and-saas","text":"AWS CloudTrail AWS Flow Logs Microsoft Azure Active Directory Microsoft Azure MySQL Microsoft Azure Linux machines Microsoft Azure Network Watcher Microsoft Azure Windows machines Microsoft Office 365 CISCO Umbrella Dns Logs CISCO Umbrella Ip Logs CISCO Umbrella Proxy Logs","title":"Cloud and SaaS"},{"location":"integrations/#operating-systems","text":"Linux Linux / AuditBeat Windows / NXLog Windows / Log Insight","title":"Operating Systems"},{"location":"integrations/#applications","text":"Alsid Apache BIND Checkpoint Cisco F5 BigIP FortiGate FortiMail FortiWeb HAProxy HarfangLab ISC DHCP NetFilter Nginx OpenSSH PaloAlto Postfix Pulse Connect Secure Sophos SpamAssassin Squid Suricata Unbound Zeek","title":"Applications"},{"location":"integrations/#generic","text":"Common Event Format","title":"Generic"},{"location":"integrations/alsid/","text":"Overview Alsid is an automated security solution that monitors the components of Active Directory infrastructures by detecting attacks in real time, identifying existing weaknesses and vulnerabilities. Setup This setup guide will show you how to forward logs produced by Alsid to SEKOIA.IO by means of a rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Alsid configuration file for rsyslog: sudo vim /etc/rsyslog.d/58-alsid.conf Paste the following rsyslog configuration to trigger the emission of Alsid logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOAlsidTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOAlsidTemplate template if $hostname == \"YOUR_ALSID_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOAlsidTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Alsid"},{"location":"integrations/alsid/#overview","text":"Alsid is an automated security solution that monitors the components of Active Directory infrastructures by detecting attacks in real time, identifying existing weaknesses and vulnerabilities.","title":"Overview"},{"location":"integrations/alsid/#setup","text":"This setup guide will show you how to forward logs produced by Alsid to SEKOIA.IO by means of a rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/alsid/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/alsid/#configure-the-rsyslog-server","text":"Open or create a new Alsid configuration file for rsyslog: sudo vim /etc/rsyslog.d/58-alsid.conf Paste the following rsyslog configuration to trigger the emission of Alsid logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOAlsidTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOAlsidTemplate template if $hostname == \"YOUR_ALSID_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOAlsidTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/alsid/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/alsid/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/alsid/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/apache/","text":"Overview The Apache HTTP Server, colloquially called Apache, is free and open-source cross-platform web server software, released under the terms of Apache License 2.0. Apache is developed and maintained by an open community of developers under the auspices of the Apache Software Foundation. Setup This setup guide will show you how to forward both your access and error logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server We can configure rsyslog to parse the access_log and error_log and report its entries to SEKOIA.IO. Open or create a new Apache configuration file for rsyslog: sudo vim /etc/rsyslog.d/45-apache.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor apache httpd access and error output files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # error log $InputFileName /var/log/apache2/error.log $InputFileTag apache: $InputFileStateFile stat-apache-error $InputFileSeverity error $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # access log $InputFileName /var/log/apache2/access.log $InputFileTag apache: $InputFileStateFile stat-apache-access $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOApacheTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOApacheTemplate template if $programname startswith 'apache' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOApacheTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading Apache documentation Rsyslog IMFile module","title":"Apache"},{"location":"integrations/apache/#overview","text":"The Apache HTTP Server, colloquially called Apache, is free and open-source cross-platform web server software, released under the terms of Apache License 2.0. Apache is developed and maintained by an open community of developers under the auspices of the Apache Software Foundation.","title":"Overview"},{"location":"integrations/apache/#setup","text":"This setup guide will show you how to forward both your access and error logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/apache/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/apache/#configure-the-rsyslog-server","text":"We can configure rsyslog to parse the access_log and error_log and report its entries to SEKOIA.IO. Open or create a new Apache configuration file for rsyslog: sudo vim /etc/rsyslog.d/45-apache.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor apache httpd access and error output files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # error log $InputFileName /var/log/apache2/error.log $InputFileTag apache: $InputFileStateFile stat-apache-error $InputFileSeverity error $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # access log $InputFileName /var/log/apache2/access.log $InputFileTag apache: $InputFileStateFile stat-apache-access $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOApacheTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOApacheTemplate template if $programname startswith 'apache' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOApacheTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/apache/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/apache/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/apache/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/apache/#further-reading","text":"Apache documentation Rsyslog IMFile module","title":"Further Reading"},{"location":"integrations/auditbeat_linux/","text":"Overview Auditbeat communicates directly with the Linux audit framework, collects the same data as auditd then the data can be stored in JSON inside a log file before being sent to a log concentrator. Auditbeat logs With Auditbeat it is possible to collect various activities such as: Authentication: From system module, login dataset retrieve User logins, logouts, and system boots File monitoring: From audit module, the file metricset sends events when a file is changed (created, updated, or deleted) on disk. The events contain file metadata and hashes Process command-line parameters: command-lines and arguments are extracted from Linux logs Process monitoring: From system module, process dataset retrieve Started and stopped processes Process use of network: From system module, socket dataset retrieve Opened and Closed sockets Transport to the collector Prerequisites The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the server Traffic towards the log collector sever which is using Rsyslog must be open on port TCP/514 Configure the client Install and configure Auditbeat To download and install Auditbeat on a Debian based distributions (including Ubuntu, Linux Mint, etc.). Use the commands that work with your system: curl -L -O https://artifacts.elastic.co/downloads/beats/auditbeat/auditbeat-7.13.1-amd64.deb sudo dpkg -i auditbeat-7.13.1-amd64.deb To download and install Auditbeat on Fedory, CentOS or Red Hat Enterprise Linux, use the commands that work with your system: curl -L -O https://artifacts.elastic.co/downloads/beats/auditbeat/auditbeat-7.13.1-x86_64.rpm sudo rpm -vi auditbeat-7.13.1-x86_64.rpm Modify the version number with the newest one. Auditbeat uses modules to collect audit information. By default, Auditbeat uses a configuration that\u2019s tailored to the operating system where Auditbeat is running. Replace the configuration file /etc/auditbeat/auditbeat.yml by the following content: ########################## Auditbeat Configuration ############################# # =========================== Modules configuration ============================ auditbeat.modules: # The auditd module collects events from the audit framework in the Linux # kernel. You need to specify audit rules for the events that you want to audit. - module: auditd resolve_ids: true failure_mode: silent backlog_limit: 8196 rate_limit: 0 include_raw_message: false include_warnings: false # Load audit rules audit_rules: | ## Example of audit rules here. Comment what is NOT needed ## Executions. -a always,exit -F arch = b64 -S execve,execveat -k exec ## External access (warning: these can be expensive to audit). -a always,exit -F arch = b64 -S accept,bind,connect -F key = external-access ## Identity changes. -w /etc/group -p wa -k identity -w /etc/passwd -p wa -k identity -w /etc/gshadow -p wa -k identity ## Unauthorized access attempts. -a always,exit -F arch = b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit = -EACCES -k access -a always,exit -F arch = b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit = -EPERM -k access # The file integrity module sends events when files are changed (created, # updated, deleted). The events contain file metadata and hashes. - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc scan_at_start: true scan_rate_per_sec: 50 MiB max_file_size: 100 MiB hash_types: [ sha1 ] # Detect changes to files included in subdirectories. Disabled by default. recursive: false - module: system datasets: - package # Installed, updated, and removed packages period: 2m - module: system datasets: - host # General host information, e.g. uptime, IPs - login # User logins, logouts, and system boots. - process # Started and stopped processes - socket # Opened and closed sockets - user # User information user.detect_password_changes: true # ================================== Outputs =================================== # ---------------------------- Elasticsearch Output ---------------------------- output.elasticsearch: enabled: false # -------------------------------- File Output --------------------------------- output.file: enabled: true # Configure JSON encoding codec.json: #pretty: false # Configure escaping HTML symbols in strings. #escape_html: false # Path to the directory where to save the generated files. path: \"/tmp/auditbeat\" filename: auditbeat rotate_every_kb: 10000 number_of_files: 7 permissions: 0600 # =================================== Paths ==================================== path.home: \"/usr/share/auditbeat/bin\" path.config: \"/etc/auditbeat\" path.data: \"/var/lib/auditbeat\" path.logs: \"/var/log/auditbeat\" # ================================== Template ================================== # Elasticsearch template settings setup.template.settings: # A dictionary of settings to place into the settings.index dictionary index: number_of_shards: 1 #codec: best_compression # ================================== Logging =================================== logging.level: info logging.to_files: true logging.files: path: /var/log/auditbeat name: auditbeat rotateeverybytes: 10485760 # = 10MB keepfiles: 7 permissions: 0600 In order to make it easier, you can download the auditbeat configuration file: $ wget -O /etc/auditbeat/auditbeat.yml https://docs.sekoia.io/integrations/configurations/auditbeat.yml Don't forget to set the right level of permission of the new auditbeat.yml if you edited a new one. sudo chmod 0600 auditbeat.yml Plus in this case ensure the owner of the file is root : sudo chown root:root auditbeat.yml Auditbeat comes with predefined assets for parsing, indexing, and visualizing your data. To load these assets: sudo auditbeat -e If no error occurs, use Ctrl + C to stop running it in the terminal. Start Auditbeat with the following command: sudo systemctl restart auditbeat.service Check your logs in /tmp/auditbeat/auditbeat in JSON format. It may be recommended to set up logrotate for the Auditbeat logs generated in /tmp/auditbeat . Configure local Rsyslog service In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog Settup a light client rsyslog by editing the /etc/rsyslog.conf file. ### Create a dedicated Rsyslog configuration file module ( load = \"imuxsock\" ) # provides support for local system logging module ( load = \"imklog\" permitnonkernelfacility = \"on\" ) # provides kernel logging support # Use traditional timestamp format. $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # Set the default permissions for all log files. $FileOwner root $FileGroup adm $FileCreateMode 0640 $DirCreateMode 0755 $Umask 0022 $ActionQueueType LinkedList # create a queue stored in the RAM $ActionQueueFileName sek_fwd # set up the prefix for writting $ActionQueueMaxDiskSpace 5g # allow 5 giga of storage for the buffer $ActionQueueSaveOnShutdown on # write on disk is the rsyslog is whut down $ActionResumeRetryCount -1 # prevent the rsyslog from droping the logs if the connexion is interrupted # Where to place spool and state files $WorkDirectory /var/spool/rsyslog $IncludeConfig /etc/rsyslog.d/*.conf # Rules *.* -/var/log/syslog And add a dedicated configuration file for the Auditbeat logs in /etc/rsyslog.d/8-linux_auditbeat.conf to be sent to a log concentrator. module ( load = \"imfile\" PollingInterval = \"10\" ) #needs to be done just once input ( type = \"imfile\" File = \"/tmp/auditbeat/auditbeat\" Tag = \"linux_auditbeat\" Severity = \"info\" Facility = \"local7\" ) if ( $syslogtag contains 'linux_auditbeat' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"YOUR_RSYSLOG_DESTINATION_SERVER\" port = \"514\" TCP_Framing = \"octet-counted\" ) } Don't forget to change the value of YOUR_RSYSLOG_DESTINATION_SERVER in the bottom of the rsyslog.conf file Restart Rsyslog $ sudo systemctl restart rsyslog.service Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allows in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog Server Open or create a new AuditBeat configuration file for Rsyslog: sudo vim /etc/rsyslog.d/9-auditbeat_linux.conf Paste the following Rsyslog configuration to trigger the emission of AuditBeat logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOAuditBeatLinuxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOAuditBeatLinuxTemplate template if ( $syslogtag contains 'linux_auditbeat' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOAuditBeatLinuxTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_LINUX_HOSTNAME variable with the correct value. Restart Rsyslog $ sudo systemctl restart rsyslog.service Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"Auditbeat Linux"},{"location":"integrations/auditbeat_linux/#overview","text":"Auditbeat communicates directly with the Linux audit framework, collects the same data as auditd then the data can be stored in JSON inside a log file before being sent to a log concentrator.","title":"Overview"},{"location":"integrations/auditbeat_linux/#auditbeat-logs","text":"With Auditbeat it is possible to collect various activities such as: Authentication: From system module, login dataset retrieve User logins, logouts, and system boots File monitoring: From audit module, the file metricset sends events when a file is changed (created, updated, or deleted) on disk. The events contain file metadata and hashes Process command-line parameters: command-lines and arguments are extracted from Linux logs Process monitoring: From system module, process dataset retrieve Started and stopped processes Process use of network: From system module, socket dataset retrieve Opened and Closed sockets","title":"Auditbeat logs"},{"location":"integrations/auditbeat_linux/#transport-to-the-collector","text":"","title":"Transport to the collector"},{"location":"integrations/auditbeat_linux/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the server Traffic towards the log collector sever which is using Rsyslog must be open on port TCP/514","title":"Prerequisites"},{"location":"integrations/auditbeat_linux/#configure-the-client","text":"","title":"Configure the client"},{"location":"integrations/auditbeat_linux/#install-and-configure-auditbeat","text":"To download and install Auditbeat on a Debian based distributions (including Ubuntu, Linux Mint, etc.). Use the commands that work with your system: curl -L -O https://artifacts.elastic.co/downloads/beats/auditbeat/auditbeat-7.13.1-amd64.deb sudo dpkg -i auditbeat-7.13.1-amd64.deb To download and install Auditbeat on Fedory, CentOS or Red Hat Enterprise Linux, use the commands that work with your system: curl -L -O https://artifacts.elastic.co/downloads/beats/auditbeat/auditbeat-7.13.1-x86_64.rpm sudo rpm -vi auditbeat-7.13.1-x86_64.rpm Modify the version number with the newest one. Auditbeat uses modules to collect audit information. By default, Auditbeat uses a configuration that\u2019s tailored to the operating system where Auditbeat is running. Replace the configuration file /etc/auditbeat/auditbeat.yml by the following content: ########################## Auditbeat Configuration ############################# # =========================== Modules configuration ============================ auditbeat.modules: # The auditd module collects events from the audit framework in the Linux # kernel. You need to specify audit rules for the events that you want to audit. - module: auditd resolve_ids: true failure_mode: silent backlog_limit: 8196 rate_limit: 0 include_raw_message: false include_warnings: false # Load audit rules audit_rules: | ## Example of audit rules here. Comment what is NOT needed ## Executions. -a always,exit -F arch = b64 -S execve,execveat -k exec ## External access (warning: these can be expensive to audit). -a always,exit -F arch = b64 -S accept,bind,connect -F key = external-access ## Identity changes. -w /etc/group -p wa -k identity -w /etc/passwd -p wa -k identity -w /etc/gshadow -p wa -k identity ## Unauthorized access attempts. -a always,exit -F arch = b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit = -EACCES -k access -a always,exit -F arch = b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit = -EPERM -k access # The file integrity module sends events when files are changed (created, # updated, deleted). The events contain file metadata and hashes. - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc scan_at_start: true scan_rate_per_sec: 50 MiB max_file_size: 100 MiB hash_types: [ sha1 ] # Detect changes to files included in subdirectories. Disabled by default. recursive: false - module: system datasets: - package # Installed, updated, and removed packages period: 2m - module: system datasets: - host # General host information, e.g. uptime, IPs - login # User logins, logouts, and system boots. - process # Started and stopped processes - socket # Opened and closed sockets - user # User information user.detect_password_changes: true # ================================== Outputs =================================== # ---------------------------- Elasticsearch Output ---------------------------- output.elasticsearch: enabled: false # -------------------------------- File Output --------------------------------- output.file: enabled: true # Configure JSON encoding codec.json: #pretty: false # Configure escaping HTML symbols in strings. #escape_html: false # Path to the directory where to save the generated files. path: \"/tmp/auditbeat\" filename: auditbeat rotate_every_kb: 10000 number_of_files: 7 permissions: 0600 # =================================== Paths ==================================== path.home: \"/usr/share/auditbeat/bin\" path.config: \"/etc/auditbeat\" path.data: \"/var/lib/auditbeat\" path.logs: \"/var/log/auditbeat\" # ================================== Template ================================== # Elasticsearch template settings setup.template.settings: # A dictionary of settings to place into the settings.index dictionary index: number_of_shards: 1 #codec: best_compression # ================================== Logging =================================== logging.level: info logging.to_files: true logging.files: path: /var/log/auditbeat name: auditbeat rotateeverybytes: 10485760 # = 10MB keepfiles: 7 permissions: 0600 In order to make it easier, you can download the auditbeat configuration file: $ wget -O /etc/auditbeat/auditbeat.yml https://docs.sekoia.io/integrations/configurations/auditbeat.yml Don't forget to set the right level of permission of the new auditbeat.yml if you edited a new one. sudo chmod 0600 auditbeat.yml Plus in this case ensure the owner of the file is root : sudo chown root:root auditbeat.yml Auditbeat comes with predefined assets for parsing, indexing, and visualizing your data. To load these assets: sudo auditbeat -e If no error occurs, use Ctrl + C to stop running it in the terminal. Start Auditbeat with the following command: sudo systemctl restart auditbeat.service Check your logs in /tmp/auditbeat/auditbeat in JSON format. It may be recommended to set up logrotate for the Auditbeat logs generated in /tmp/auditbeat .","title":"Install and configure Auditbeat"},{"location":"integrations/auditbeat_linux/#configure-local-rsyslog-service","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog Settup a light client rsyslog by editing the /etc/rsyslog.conf file. ### Create a dedicated Rsyslog configuration file module ( load = \"imuxsock\" ) # provides support for local system logging module ( load = \"imklog\" permitnonkernelfacility = \"on\" ) # provides kernel logging support # Use traditional timestamp format. $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # Set the default permissions for all log files. $FileOwner root $FileGroup adm $FileCreateMode 0640 $DirCreateMode 0755 $Umask 0022 $ActionQueueType LinkedList # create a queue stored in the RAM $ActionQueueFileName sek_fwd # set up the prefix for writting $ActionQueueMaxDiskSpace 5g # allow 5 giga of storage for the buffer $ActionQueueSaveOnShutdown on # write on disk is the rsyslog is whut down $ActionResumeRetryCount -1 # prevent the rsyslog from droping the logs if the connexion is interrupted # Where to place spool and state files $WorkDirectory /var/spool/rsyslog $IncludeConfig /etc/rsyslog.d/*.conf # Rules *.* -/var/log/syslog And add a dedicated configuration file for the Auditbeat logs in /etc/rsyslog.d/8-linux_auditbeat.conf to be sent to a log concentrator. module ( load = \"imfile\" PollingInterval = \"10\" ) #needs to be done just once input ( type = \"imfile\" File = \"/tmp/auditbeat/auditbeat\" Tag = \"linux_auditbeat\" Severity = \"info\" Facility = \"local7\" ) if ( $syslogtag contains 'linux_auditbeat' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"YOUR_RSYSLOG_DESTINATION_SERVER\" port = \"514\" TCP_Framing = \"octet-counted\" ) } Don't forget to change the value of YOUR_RSYSLOG_DESTINATION_SERVER in the bottom of the rsyslog.conf file","title":"Configure local Rsyslog service"},{"location":"integrations/auditbeat_linux/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/auditbeat_linux/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/auditbeat_linux/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/auditbeat_linux/#rsyslog-prerequisites","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allows in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/auditbeat_linux/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/auditbeat_linux/#configure-the-rsyslog-server","text":"Open or create a new AuditBeat configuration file for Rsyslog: sudo vim /etc/rsyslog.d/9-auditbeat_linux.conf Paste the following Rsyslog configuration to trigger the emission of AuditBeat logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOAuditBeatLinuxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOAuditBeatLinuxTemplate template if ( $syslogtag contains 'linux_auditbeat' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOAuditBeatLinuxTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_LINUX_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog Server"},{"location":"integrations/auditbeat_linux/#restart-rsyslog_1","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/auditbeat_linux/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/auditbeat_linux/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/aws_cloudtrail/","text":"Overview AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services ( source: AWS CloudTrail Overview ). Setup Please contact us to discuss about the AWS services in your organization in order to find the appropriate solution to forward CloudTrail events to SEKOIA.IO. CloudTrail trail As a prerequisite you need an existing CloudTrail trail and configure it to record activities from services that you want to monitor. In the AWS console, navigate to: Services > CloudTrail > Trails . From there, enable the events that you want to record: Management events: provide visibility into management operations that are performed on resources in your AWS account. Insights events: help AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events. Insights events are logged when CloudTrail detects unusual write management API activity in your account. Data events: provide visibility into the resource operations performed on or within a resource. Activate the logging on the trail through the switch button (On/Off) located on the top right hand corner of the trail page. Log forwarding configuration This part should be discuss with SEKOIA people to find an appropriate solution to forward CloudTrail events to SEKOIA.IO. Further Readings AWS CloudTrail Overview AWS CloudTrail Documentation","title":"CloudTrail"},{"location":"integrations/aws_cloudtrail/#overview","text":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services ( source: AWS CloudTrail Overview ).","title":"Overview"},{"location":"integrations/aws_cloudtrail/#setup","text":"Please contact us to discuss about the AWS services in your organization in order to find the appropriate solution to forward CloudTrail events to SEKOIA.IO.","title":"Setup"},{"location":"integrations/aws_cloudtrail/#cloudtrail-trail","text":"As a prerequisite you need an existing CloudTrail trail and configure it to record activities from services that you want to monitor. In the AWS console, navigate to: Services > CloudTrail > Trails . From there, enable the events that you want to record: Management events: provide visibility into management operations that are performed on resources in your AWS account. Insights events: help AWS users identify and respond to unusual activity associated with write API calls by continuously analyzing CloudTrail management events. Insights events are logged when CloudTrail detects unusual write management API activity in your account. Data events: provide visibility into the resource operations performed on or within a resource. Activate the logging on the trail through the switch button (On/Off) located on the top right hand corner of the trail page.","title":"CloudTrail trail"},{"location":"integrations/aws_cloudtrail/#log-forwarding-configuration","text":"This part should be discuss with SEKOIA people to find an appropriate solution to forward CloudTrail events to SEKOIA.IO.","title":"Log forwarding configuration"},{"location":"integrations/aws_cloudtrail/#further-readings","text":"AWS CloudTrail Overview AWS CloudTrail Documentation","title":"Further Readings"},{"location":"integrations/aws_flow_logs/","text":"Overview Amazon VPC Flow Logs is a feature that provides the ability to capture information about IP network traffic as it enters or exits from network interface in your Amazon VPC (Amazon Virtual Private Cloud). VPC Flow Logs can help you with a number of tasks, such as: Diagnosing overly restrictive security group rules. Monitoring the traffic that is reaching your instance. Determining the direction of the traffic to and from the network interfaces. Setup Please contact us to discuss about the AWS services in your organization in order to find the appropriate solution to forward VPC Flow Logs to SEKOIA.IO. VPC Flow Logs As a prerequisite you need an existing VPC, subnet or network interface (Elastic Load Balancing, Amazon RDS, Amazon ElastiCache, Amazon Redshift, Amazon WorkSpaces, NAT gateways, Transit gateways) to create a flow log. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. In the AWS console, navigate to: Services > VPC . From there, select the resource for which you want to capture information. The flow logs are available on the following resources: VPC, subnet, or network interfaces. For VPC and subnet: Select the specific resource to monitor Go to the tab Flow logs Click on Create flow log Set up the flow log: we recommend to capture all traffic (accepted and rejected). Log forwarding configuration This part should be discussed with SEKOIA.IO people to find an appropriate solution to forward your flow logs to SEKOIA.IO. The following default record format is integrated to the Operation Center: <version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport> <dstport> <protocol> <packets> <bytes> <start> <end> <action> <log-status> Again, you should discuss with SEKOIA.IO people if your flow logs records are captured with a custom format. Further Readings AWS VPC Overview AWS Flow Logs Documentation","title":"Flow Logs"},{"location":"integrations/aws_flow_logs/#overview","text":"Amazon VPC Flow Logs is a feature that provides the ability to capture information about IP network traffic as it enters or exits from network interface in your Amazon VPC (Amazon Virtual Private Cloud). VPC Flow Logs can help you with a number of tasks, such as: Diagnosing overly restrictive security group rules. Monitoring the traffic that is reaching your instance. Determining the direction of the traffic to and from the network interfaces.","title":"Overview"},{"location":"integrations/aws_flow_logs/#setup","text":"Please contact us to discuss about the AWS services in your organization in order to find the appropriate solution to forward VPC Flow Logs to SEKOIA.IO.","title":"Setup"},{"location":"integrations/aws_flow_logs/#vpc-flow-logs","text":"As a prerequisite you need an existing VPC, subnet or network interface (Elastic Load Balancing, Amazon RDS, Amazon ElastiCache, Amazon Redshift, Amazon WorkSpaces, NAT gateways, Transit gateways) to create a flow log. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. In the AWS console, navigate to: Services > VPC . From there, select the resource for which you want to capture information. The flow logs are available on the following resources: VPC, subnet, or network interfaces. For VPC and subnet: Select the specific resource to monitor Go to the tab Flow logs Click on Create flow log Set up the flow log: we recommend to capture all traffic (accepted and rejected).","title":"VPC Flow Logs"},{"location":"integrations/aws_flow_logs/#log-forwarding-configuration","text":"This part should be discussed with SEKOIA.IO people to find an appropriate solution to forward your flow logs to SEKOIA.IO. The following default record format is integrated to the Operation Center: <version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport> <dstport> <protocol> <packets> <bytes> <start> <end> <action> <log-status> Again, you should discuss with SEKOIA.IO people if your flow logs records are captured with a custom format.","title":"Log forwarding configuration"},{"location":"integrations/aws_flow_logs/#further-readings","text":"AWS VPC Overview AWS Flow Logs Documentation","title":"Further Readings"},{"location":"integrations/azure_ad/","text":"Overview Azure Active Directory is a cloud-based Identity and Rights management service. The service is developed and managed by Microsoft Corp. Setup This setup guide will show you how to forward events produced by Azure Active Directory service to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com). Event hubs As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. active-directory-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name active-directory-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > active-directory-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > active-directory-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi). Azure Active Directory You need to activate and configure the Azure Active Directory diagnostic settings (e.g. company-ad). Navigate to: Home > Azure Active Directory (e.g. company-ad) > Monitoring > Diagnostic settings : Add a new diagnostic setting, and select Stream to an event hub and click on configure. Select the previously created Event hubs , Event Hub and SharedAccessKey . In the log section, select AuditLogs and SignInLogs . Choose a name for this configuration and click on Save . Enjoy your events You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings Microsoft Github diagnostic Active Directory documentation","title":"Azure Active Directory"},{"location":"integrations/azure_ad/#overview","text":"Azure Active Directory is a cloud-based Identity and Rights management service. The service is developed and managed by Microsoft Corp.","title":"Overview"},{"location":"integrations/azure_ad/#setup","text":"This setup guide will show you how to forward events produced by Azure Active Directory service to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com).","title":"Setup"},{"location":"integrations/azure_ad/#event-hubs","text":"As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. active-directory-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name active-directory-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > active-directory-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > active-directory-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi).","title":"Event hubs"},{"location":"integrations/azure_ad/#azure-active-directory","text":"You need to activate and configure the Azure Active Directory diagnostic settings (e.g. company-ad). Navigate to: Home > Azure Active Directory (e.g. company-ad) > Monitoring > Diagnostic settings : Add a new diagnostic setting, and select Stream to an event hub and click on configure. Select the previously created Event hubs , Event Hub and SharedAccessKey . In the log section, select AuditLogs and SignInLogs . Choose a name for this configuration and click on Save .","title":"Azure Active Directory"},{"location":"integrations/azure_ad/#enjoy-your-events","text":"You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/azure_ad/#further-readings","text":"Microsoft Github diagnostic Active Directory documentation","title":"Further Readings"},{"location":"integrations/azure_linux/","text":"Overview Azure Virtual Machines service is developed and managed by Microsoft Corp. Setup This setup guide will show you how to forward events produced by a Linux Virtual Machine hosted on Azure platform to SEKOIA.IO. Event hubs These explanations are made from the Azure web portal (https://portal.azure.com). As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. linux-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name linux-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > linux-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > linux-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi). Linux Virtual Machine You need to activate and configure the diagnostic extension LinuxDiagnostic . Navigate to: Home > Virtual machines > virtual machine name (e.g. company-linux) > Settings > Extensions . Install it and note the new StorageAccount name created (e.g. company-storage-account). Navigate to: Home > Storage accounts > company-storage-account - Access keys . From there you can note the key value later used as the storageAccountKey . Navigate to: Home > Storage accounts > company-storage-account - Shared access signature . From there set the expiration date with caution, then click on Generate SAS and connection string . You should note the SAS token value later used (starting with sv?=). Navigate to: Home > All resources . From there you can note the resourceId associated to your linux virtual machine. You need to create two configuration files public_settings.json and protected_settings.json . Once again you need Azure powershell to do it using your favorite text editor: PS Azure :\\> vim public_settings . json Adapt the public settings configuration file with the value of theses variables: StorageAccount , resourceId and sinks and the syslog configuration. { \"StorageAccount\" : \"company-storage-account\" , \"ladCfg\" : { \"diagnosticMonitorConfiguration\" : { \"eventVolume\" : \"Medium\" , \"metrics\" : { \"metricAggregation\" : [ { \"scheduledTransferPeriod\" : \"PT1H\" }, { \"scheduledTransferPeriod\" : \"PT1M\" } ], \"resourceId\" : \"/subscriptions/128ed5ce-4f50-4b5f-a3b0-08233b5a86b6/resourceGroups/company-resource-group/providers/Microsoft.Compute/virtualMachines/company-linux\" }, \"performanceCounters\" : { \"performanceCounterConfiguration\" : [] }, \"syslogEvents\" : { \"sinks\" : \"linux-event\" , \"syslogEventConfiguration\" : { \"LOG_AUTH\" : \"LOG_INFO\" , \"LOG_AUTHPRIV\" : \"LOG_INFO\" , \"LOG_CRON\" : \"LOG_INFO\" , \"LOG_DAEMON\" : \"LOG_INFO\" , \"LOG_FTP\" : \"LOG_INFO\" , \"LOG_KERN\" : \"LOG_INFO\" , \"LOG_LOCAL0\" : \"LOG_INFO\" , \"LOG_LOCAL1\" : \"LOG_INFO\" , \"LOG_LOCAL2\" : \"LOG_INFO\" , \"LOG_LOCAL3\" : \"LOG_INFO\" , \"LOG_LOCAL4\" : \"LOG_INFO\" , \"LOG_LOCAL5\" : \"LOG_INFO\" , \"LOG_LOCAL6\" : \"LOG_INFO\" , \"LOG_LOCAL7\" : \"LOG_INFO\" , \"LOG_LPR\" : \"LOG_INFO\" , \"LOG_MAIL\" : \"LOG_INFO\" , \"LOG_NEWS\" : \"LOG_INFO\" , \"LOG_SYSLOG\" : \"LOG_INFO\" , \"LOG_USER\" : \"LOG_INFO\" , \"LOG_UUCP\" : \"LOG_INFO\" } } }, \"sampleRateInSeconds\" : 15 } } You need to generate an authentication token for the access to the linux-event hub. First we'll convert the expiration date we set before into a unix timestamp. Extract the se= value from storageAccountSasTokenv and use it as a parameter to this command: PS Azure :\\> date -d '2021-07-09T23:09:19' + % s 1625872159 Then you could create this python script: PS Azure :\\> vim get_token . py Adapt theses variables: sb_name , eh_name , Url , sas_name , sas_value , and expiry : from urllib.parse import quote_plus , quote import hmac import hashlib import base64 def get_auth_token ( sb_name , eh_name , sas_name , sas_value , expiry ): \"\"\" Returns an authorization token dictionary for making calls to Event Hubs REST API. \"\"\" uri = quote_plus ( \"https:// {} .servicebus.windows.net/ {} \" \\ . format ( sb_name , eh_name )) sas = sas_value . encode ( 'utf-8' ) string_to_sign = ( uri + ' \\n ' + expiry ) . encode ( 'utf-8' ) signed_hmac_sha256 = hmac . HMAC ( sas , string_to_sign , hashlib . sha256 ) signature = quote ( base64 . b64encode ( signed_hmac_sha256 . digest ())) return { \"sb_name\" : sb_name , \"eh_name\" : eh_name , \"token\" : 'SharedAccessSignature sr= {} &sig= {} &se= {} &skn= {} ' \\ . format ( uri , signature , expiry , sas_name ) } print ( get_auth_token ( sb_name = \"company-eventhub\" , eh_name = \"linux-event\" , sas_name = \"RootManageSharedAccessKey\" , sas_value = \"base64string\" , expiry = \"unix_timestamp\" )) Execute this python script and note the token variable value only from the sr= . PS Azure :\\> python get_token . py { 'token' : 'SharedAccessSignature sr=https%3A%2F%2Fcompany-eventhub.servicebus.windows.net%2Flinux-event&sig=9%2BOwFlfqBVEcVg2c5G1wztIjG22GtsMZN5g4NYEu6p0%3D&se=1561569146&skn=RootManageSharedAccessKey' , 'eh_name' : 'linux-event' , 'sb_name' : 'company-eventhub' } Then edit the protected settings configuration file: PS Azure :\\> vim protected_settings . json Adapt the public protected settings configuration file with the value of theses variables: storageAccountName , storageAccountSasToken (starting with sv= and previously refered as SAS token ), sasURL (replace the different values, company-eventhub, linux-event, sr=, and RootManageSharedAccessKey), and SharedAccessKeyName : { \"storageAccountName\" : \"company-storage-account\" , \"storageAccountSasToken\" : \"sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-06-21T15:52:44Z&st=2019-06-21T07:52:44Z&spr=https&sig=Dewc7mP63E4xrwqttVcOrChgDIpm6Trp%2FR4dfvSo4vg%3D\" , \"sinksConfig\" : { \"sink\" : [ { \"name\" : \"SyslogJsonBlob\" , \"type\" : \"JsonBlob\" }, { \"name\" : \"linux-event\" , \"type\" : \"EventHub\" , \"sasURL\" : \"https://company-eventhub.servicebus.windows.net/linux-event?sr=https%3A%2F%2Fcompany-eventhub.servicebus.windows.net%2Flinux-event&sig=W86ldfWlPKW0sutGWM7shYGlg%2BbwnbtyVJ7eMsBs840%3D&se=1561137701&skn=RootManageSharedAccessKey\" } ] } } Finally you could push the change of the diagnostic extension configuration (adapt the parameters resource-group, vm-name): PS Azure :\\> az vm extension set - -publisher Microsoft . Azure . Diagnostics - -name LinuxDiagnostic - -version 3 . 0 - -resource-group company-resource-group - -vm-name company-linux - -protected-settings protected_settings . json - -settings public_settings . json - -subscription uuid Enjoy your events You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings Microsoft Github linux diagnostic extension documentation Linkedin post","title":"Azure Linux machines"},{"location":"integrations/azure_linux/#overview","text":"Azure Virtual Machines service is developed and managed by Microsoft Corp.","title":"Overview"},{"location":"integrations/azure_linux/#setup","text":"This setup guide will show you how to forward events produced by a Linux Virtual Machine hosted on Azure platform to SEKOIA.IO.","title":"Setup"},{"location":"integrations/azure_linux/#event-hubs","text":"These explanations are made from the Azure web portal (https://portal.azure.com). As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. linux-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name linux-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > linux-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > linux-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi).","title":"Event hubs"},{"location":"integrations/azure_linux/#linux-virtual-machine","text":"You need to activate and configure the diagnostic extension LinuxDiagnostic . Navigate to: Home > Virtual machines > virtual machine name (e.g. company-linux) > Settings > Extensions . Install it and note the new StorageAccount name created (e.g. company-storage-account). Navigate to: Home > Storage accounts > company-storage-account - Access keys . From there you can note the key value later used as the storageAccountKey . Navigate to: Home > Storage accounts > company-storage-account - Shared access signature . From there set the expiration date with caution, then click on Generate SAS and connection string . You should note the SAS token value later used (starting with sv?=). Navigate to: Home > All resources . From there you can note the resourceId associated to your linux virtual machine. You need to create two configuration files public_settings.json and protected_settings.json . Once again you need Azure powershell to do it using your favorite text editor: PS Azure :\\> vim public_settings . json Adapt the public settings configuration file with the value of theses variables: StorageAccount , resourceId and sinks and the syslog configuration. { \"StorageAccount\" : \"company-storage-account\" , \"ladCfg\" : { \"diagnosticMonitorConfiguration\" : { \"eventVolume\" : \"Medium\" , \"metrics\" : { \"metricAggregation\" : [ { \"scheduledTransferPeriod\" : \"PT1H\" }, { \"scheduledTransferPeriod\" : \"PT1M\" } ], \"resourceId\" : \"/subscriptions/128ed5ce-4f50-4b5f-a3b0-08233b5a86b6/resourceGroups/company-resource-group/providers/Microsoft.Compute/virtualMachines/company-linux\" }, \"performanceCounters\" : { \"performanceCounterConfiguration\" : [] }, \"syslogEvents\" : { \"sinks\" : \"linux-event\" , \"syslogEventConfiguration\" : { \"LOG_AUTH\" : \"LOG_INFO\" , \"LOG_AUTHPRIV\" : \"LOG_INFO\" , \"LOG_CRON\" : \"LOG_INFO\" , \"LOG_DAEMON\" : \"LOG_INFO\" , \"LOG_FTP\" : \"LOG_INFO\" , \"LOG_KERN\" : \"LOG_INFO\" , \"LOG_LOCAL0\" : \"LOG_INFO\" , \"LOG_LOCAL1\" : \"LOG_INFO\" , \"LOG_LOCAL2\" : \"LOG_INFO\" , \"LOG_LOCAL3\" : \"LOG_INFO\" , \"LOG_LOCAL4\" : \"LOG_INFO\" , \"LOG_LOCAL5\" : \"LOG_INFO\" , \"LOG_LOCAL6\" : \"LOG_INFO\" , \"LOG_LOCAL7\" : \"LOG_INFO\" , \"LOG_LPR\" : \"LOG_INFO\" , \"LOG_MAIL\" : \"LOG_INFO\" , \"LOG_NEWS\" : \"LOG_INFO\" , \"LOG_SYSLOG\" : \"LOG_INFO\" , \"LOG_USER\" : \"LOG_INFO\" , \"LOG_UUCP\" : \"LOG_INFO\" } } }, \"sampleRateInSeconds\" : 15 } } You need to generate an authentication token for the access to the linux-event hub. First we'll convert the expiration date we set before into a unix timestamp. Extract the se= value from storageAccountSasTokenv and use it as a parameter to this command: PS Azure :\\> date -d '2021-07-09T23:09:19' + % s 1625872159 Then you could create this python script: PS Azure :\\> vim get_token . py Adapt theses variables: sb_name , eh_name , Url , sas_name , sas_value , and expiry : from urllib.parse import quote_plus , quote import hmac import hashlib import base64 def get_auth_token ( sb_name , eh_name , sas_name , sas_value , expiry ): \"\"\" Returns an authorization token dictionary for making calls to Event Hubs REST API. \"\"\" uri = quote_plus ( \"https:// {} .servicebus.windows.net/ {} \" \\ . format ( sb_name , eh_name )) sas = sas_value . encode ( 'utf-8' ) string_to_sign = ( uri + ' \\n ' + expiry ) . encode ( 'utf-8' ) signed_hmac_sha256 = hmac . HMAC ( sas , string_to_sign , hashlib . sha256 ) signature = quote ( base64 . b64encode ( signed_hmac_sha256 . digest ())) return { \"sb_name\" : sb_name , \"eh_name\" : eh_name , \"token\" : 'SharedAccessSignature sr= {} &sig= {} &se= {} &skn= {} ' \\ . format ( uri , signature , expiry , sas_name ) } print ( get_auth_token ( sb_name = \"company-eventhub\" , eh_name = \"linux-event\" , sas_name = \"RootManageSharedAccessKey\" , sas_value = \"base64string\" , expiry = \"unix_timestamp\" )) Execute this python script and note the token variable value only from the sr= . PS Azure :\\> python get_token . py { 'token' : 'SharedAccessSignature sr=https%3A%2F%2Fcompany-eventhub.servicebus.windows.net%2Flinux-event&sig=9%2BOwFlfqBVEcVg2c5G1wztIjG22GtsMZN5g4NYEu6p0%3D&se=1561569146&skn=RootManageSharedAccessKey' , 'eh_name' : 'linux-event' , 'sb_name' : 'company-eventhub' } Then edit the protected settings configuration file: PS Azure :\\> vim protected_settings . json Adapt the public protected settings configuration file with the value of theses variables: storageAccountName , storageAccountSasToken (starting with sv= and previously refered as SAS token ), sasURL (replace the different values, company-eventhub, linux-event, sr=, and RootManageSharedAccessKey), and SharedAccessKeyName : { \"storageAccountName\" : \"company-storage-account\" , \"storageAccountSasToken\" : \"sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2019-06-21T15:52:44Z&st=2019-06-21T07:52:44Z&spr=https&sig=Dewc7mP63E4xrwqttVcOrChgDIpm6Trp%2FR4dfvSo4vg%3D\" , \"sinksConfig\" : { \"sink\" : [ { \"name\" : \"SyslogJsonBlob\" , \"type\" : \"JsonBlob\" }, { \"name\" : \"linux-event\" , \"type\" : \"EventHub\" , \"sasURL\" : \"https://company-eventhub.servicebus.windows.net/linux-event?sr=https%3A%2F%2Fcompany-eventhub.servicebus.windows.net%2Flinux-event&sig=W86ldfWlPKW0sutGWM7shYGlg%2BbwnbtyVJ7eMsBs840%3D&se=1561137701&skn=RootManageSharedAccessKey\" } ] } } Finally you could push the change of the diagnostic extension configuration (adapt the parameters resource-group, vm-name): PS Azure :\\> az vm extension set - -publisher Microsoft . Azure . Diagnostics - -name LinuxDiagnostic - -version 3 . 0 - -resource-group company-resource-group - -vm-name company-linux - -protected-settings protected_settings . json - -settings public_settings . json - -subscription uuid","title":"Linux Virtual Machine"},{"location":"integrations/azure_linux/#enjoy-your-events","text":"You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/azure_linux/#further-readings","text":"Microsoft Github linux diagnostic extension documentation Linkedin post","title":"Further Readings"},{"location":"integrations/azure_mysql/","text":"Overview Azure Database for MySQL provides fully managed, enterprise-ready community MySQL database as a service. The service is developed and managed by Microsoft Corp. Setup This setup guide will show you how to forward events produced by Azure MySQL service to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com). Event Hubs As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. mysql-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name mysql-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > mysql-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > mysql-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi). Azure MySQL You need to activate and configure the Azure MySQL diagnostic settings (e.g. company-mysql). Navigate to: Home > SQL databases (e.g. company-mysql) > Monitoring > Diagnostic settings : Add a new diagnostic setting, and select Stream to an event hub and click on configure. Select the previously created Event hubs , Event Hub and SharedAccessKey . In the log section, select MySqlAuditLogs and MySqlSlowLogs . Choose a name for this configuration and click on Save . Enjoy your events You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Azure Mysql"},{"location":"integrations/azure_mysql/#overview","text":"Azure Database for MySQL provides fully managed, enterprise-ready community MySQL database as a service. The service is developed and managed by Microsoft Corp.","title":"Overview"},{"location":"integrations/azure_mysql/#setup","text":"This setup guide will show you how to forward events produced by Azure MySQL service to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com).","title":"Setup"},{"location":"integrations/azure_mysql/#event-hubs","text":"As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. mysql-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name mysql-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > mysql-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > mysql-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi).","title":"Event Hubs"},{"location":"integrations/azure_mysql/#azure-mysql","text":"You need to activate and configure the Azure MySQL diagnostic settings (e.g. company-mysql). Navigate to: Home > SQL databases (e.g. company-mysql) > Monitoring > Diagnostic settings : Add a new diagnostic setting, and select Stream to an event hub and click on configure. Select the previously created Event hubs , Event Hub and SharedAccessKey . In the log section, select MySqlAuditLogs and MySqlSlowLogs . Choose a name for this configuration and click on Save .","title":"Azure MySQL"},{"location":"integrations/azure_mysql/#enjoy-your-events","text":"You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/azure_network_watcher/","text":"Overview Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. It also allows to log information about IP traffic flowing through a network security group: NSG flow logs. Setup Please contact us to discuss about the network security group to monitor in your Azure infrastructure in order to find the appropriate solution to forward your logs to SEKOIA.IO. This setup guide will show you a method to enable and give us access to NSG flow logs produced by Azure Network Watcher service to SEKOIA.IO. Enable NSG flow logs The following instructions are provided for the Azure web portal (https://portal.azure.com). As a prerequisite you need at least one virtual machine with a network security group, to enable Network Watcher and to register the Microsoft.Insights provider. Navigate to the Network Watcher service, and select NSG flow logs under LOGS . From the list of NSGs, select your VM(s), and under Flow logs settings , select On to enable the NSG flow logs. Please, select the Version 2 NSG flow log format sample which is integrated to the Operation Center. These instructions are illustrated and more detailled here . Share access to logs This part should be discussed with SEKOIA.IO people to find an appropriate solution to forward your flow logs to SEKOIA.IO. A possible solution consists to share us: - An access key for the Azure Blob Storage - A storage token associated with the resources to share - The name of the container where the NSG flow logs are stored From this information, we will be able to retrieve each PT1h.json blob which contains the flow logs. Further Readings Azure Network Watcher overview Azure Network Watcher NSG flow logging overview","title":"Azure Network Watcher"},{"location":"integrations/azure_network_watcher/#overview","text":"Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. It also allows to log information about IP traffic flowing through a network security group: NSG flow logs.","title":"Overview"},{"location":"integrations/azure_network_watcher/#setup","text":"Please contact us to discuss about the network security group to monitor in your Azure infrastructure in order to find the appropriate solution to forward your logs to SEKOIA.IO. This setup guide will show you a method to enable and give us access to NSG flow logs produced by Azure Network Watcher service to SEKOIA.IO.","title":"Setup"},{"location":"integrations/azure_network_watcher/#enable-nsg-flow-logs","text":"The following instructions are provided for the Azure web portal (https://portal.azure.com). As a prerequisite you need at least one virtual machine with a network security group, to enable Network Watcher and to register the Microsoft.Insights provider. Navigate to the Network Watcher service, and select NSG flow logs under LOGS . From the list of NSGs, select your VM(s), and under Flow logs settings , select On to enable the NSG flow logs. Please, select the Version 2 NSG flow log format sample which is integrated to the Operation Center. These instructions are illustrated and more detailled here .","title":"Enable NSG flow logs"},{"location":"integrations/azure_network_watcher/#share-access-to-logs","text":"This part should be discussed with SEKOIA.IO people to find an appropriate solution to forward your flow logs to SEKOIA.IO. A possible solution consists to share us: - An access key for the Azure Blob Storage - A storage token associated with the resources to share - The name of the container where the NSG flow logs are stored From this information, we will be able to retrieve each PT1h.json blob which contains the flow logs.","title":"Share access to logs"},{"location":"integrations/azure_network_watcher/#further-readings","text":"Azure Network Watcher overview Azure Network Watcher NSG flow logging overview","title":"Further Readings"},{"location":"integrations/azure_windows/","text":"Overview Azure Virtual Machines service is developed and managed by Microsoft Corp. Setup This setup guide will show you how to forward events produced by a Windows Virtual Machine hosted on Azure platform to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com). Event hubs As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. windows-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name windows-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > windows-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > windows-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi). Windows Virtual Machine You need to activate and configure the diagnostic extension Microsoft.Insights.VMDiagnosticsSettings . Navigate to: Home > Virtual machines > virtual machine name (e.g. company-windows) > Settings > Extensions . Install it and note the new StorageAccount name created (e.g. company-storage-account). Navigate to: Home > Storage accounts > company-storage-account - Access keys . From there you can note the key value later used as the storageAccountKey . You need to create two configuration files public_settings.json and protected_settings.json . Once again you need Azure powershell to do it using your favorite text editor: PS Azure :\\> vim public_settings . json Adapt the public settings configuration file with the value oh theses variables: Url , SharedAccessKeyName , StorageAccount . { \"WadCfg\" : { \"DiagnosticMonitorConfiguration\" : { \"overallQuotaInMB\" : 4096 , \"sinks\" : \"applicationInsights.errors\" , \"DiagnosticInfrastructureLogs\" : { \"scheduledTransferLogLevelFilter\" : \"Error\" }, \"WindowsEventLog\" : { \"scheduledTransferPeriod\" : \"PT1M\" , \"DataSource\" : [ { \"name\" : \"Application!*\" }, { \"name\" : \"System!*\" }, { \"name\" : \"Security!*\" } ], \"sinks\" : \"HotPath\" }, \"Logs\" : { \"scheduledTransferPeriod\" : \"PT1M\" , \"scheduledTransferLogLevelFilter\" : \"Error\" , \"sinks\" : \"HotPath\" } }, \"SinksConfig\" : { \"Sink\" : [ { \"name\" : \"HotPath\" , \"type\" : \"JsonBlob\" , \"EventHub\" : { \"Url\" : \"https://company-eventhub.servicebus.windows.net/windows-event\" , \"SharedAccessKeyName\" : \"RootManageSharedAccessKey\" } }, { \"name\" : \"applicationInsights\" , \"ApplicationInsights\" : \"\" , \"Channels\" : { \"Channel\" : [ { \"logLevel\" : \"Error\" , \"name\" : \"errors\" } ] } } ] } }, \"StorageAccount\" : \"company-storage-account\" } A more specific windows event log can be added by specifying the event log filename (e.g for Sysmon: \"name\": \"Microsoft-Windows-Sysmon/Operational!*\" ). Then edit the protected settings configuration file: PS Azure :\\> vim protected_settings . json Adapt the public protected settings configuration file with the value of theses variables: storageAccountName , storageAccountKey , Url , SharedAccessKeyName , SharedAccessKey : { \"storageAccountName\" : \"company-storage-account\" , \"storageAccountKey\" : \"base64-string\" , \"storageAccountEndPoint\" : \"https://core.windows.net\" , \"EventHub\" : { \"Url\" : \"https://company-eventhub.servicebus.windows.net/windows-event\" , \"SharedAccessKeyName\" : \"RootManageSharedAccessKey\" , \"SharedAccessKey\" : \"base64-string\" } } Finally you could push the change of the diagnostic extension configuration (adapt the parameters resource-group, vm-name): PS Azure :\\> az vm extension set - -publisher Microsoft . Azure . Diagnostics - -name IaaSDiagnostics - -version 1 . 5 - -resource-group company-resource-group - -vm-name company-windows - -protected-settings protected_settings . json - -settings public_settings . json - -subscription uuid Sysmon Sysmon tool from Microsoft could improve the detection on Windows computers. You could download the tool on Microsoft website . If you do not know how to use and configure it, please check SwiftOnSecurity github . Enjoy your events You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings Microsoft Github windows diagnostic extension documentation","title":"Azure Windows machines"},{"location":"integrations/azure_windows/#overview","text":"Azure Virtual Machines service is developed and managed by Microsoft Corp.","title":"Overview"},{"location":"integrations/azure_windows/#setup","text":"This setup guide will show you how to forward events produced by a Windows Virtual Machine hosted on Azure platform to SEKOIA.IO. Theses changes have to be made from the Azure web portal (https://portal.azure.com).","title":"Setup"},{"location":"integrations/azure_windows/#event-hubs","text":"As a prerequisite you need an Event Hubs (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure powershell (within Cloud Shell interface for example): you will a create a global Event Hubs , then specific Event Hub (e.g. windows-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name windows-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > windows-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > windows-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi).","title":"Event hubs"},{"location":"integrations/azure_windows/#windows-virtual-machine","text":"You need to activate and configure the diagnostic extension Microsoft.Insights.VMDiagnosticsSettings . Navigate to: Home > Virtual machines > virtual machine name (e.g. company-windows) > Settings > Extensions . Install it and note the new StorageAccount name created (e.g. company-storage-account). Navigate to: Home > Storage accounts > company-storage-account - Access keys . From there you can note the key value later used as the storageAccountKey . You need to create two configuration files public_settings.json and protected_settings.json . Once again you need Azure powershell to do it using your favorite text editor: PS Azure :\\> vim public_settings . json Adapt the public settings configuration file with the value oh theses variables: Url , SharedAccessKeyName , StorageAccount . { \"WadCfg\" : { \"DiagnosticMonitorConfiguration\" : { \"overallQuotaInMB\" : 4096 , \"sinks\" : \"applicationInsights.errors\" , \"DiagnosticInfrastructureLogs\" : { \"scheduledTransferLogLevelFilter\" : \"Error\" }, \"WindowsEventLog\" : { \"scheduledTransferPeriod\" : \"PT1M\" , \"DataSource\" : [ { \"name\" : \"Application!*\" }, { \"name\" : \"System!*\" }, { \"name\" : \"Security!*\" } ], \"sinks\" : \"HotPath\" }, \"Logs\" : { \"scheduledTransferPeriod\" : \"PT1M\" , \"scheduledTransferLogLevelFilter\" : \"Error\" , \"sinks\" : \"HotPath\" } }, \"SinksConfig\" : { \"Sink\" : [ { \"name\" : \"HotPath\" , \"type\" : \"JsonBlob\" , \"EventHub\" : { \"Url\" : \"https://company-eventhub.servicebus.windows.net/windows-event\" , \"SharedAccessKeyName\" : \"RootManageSharedAccessKey\" } }, { \"name\" : \"applicationInsights\" , \"ApplicationInsights\" : \"\" , \"Channels\" : { \"Channel\" : [ { \"logLevel\" : \"Error\" , \"name\" : \"errors\" } ] } } ] } }, \"StorageAccount\" : \"company-storage-account\" } A more specific windows event log can be added by specifying the event log filename (e.g for Sysmon: \"name\": \"Microsoft-Windows-Sysmon/Operational!*\" ). Then edit the protected settings configuration file: PS Azure :\\> vim protected_settings . json Adapt the public protected settings configuration file with the value of theses variables: storageAccountName , storageAccountKey , Url , SharedAccessKeyName , SharedAccessKey : { \"storageAccountName\" : \"company-storage-account\" , \"storageAccountKey\" : \"base64-string\" , \"storageAccountEndPoint\" : \"https://core.windows.net\" , \"EventHub\" : { \"Url\" : \"https://company-eventhub.servicebus.windows.net/windows-event\" , \"SharedAccessKeyName\" : \"RootManageSharedAccessKey\" , \"SharedAccessKey\" : \"base64-string\" } } Finally you could push the change of the diagnostic extension configuration (adapt the parameters resource-group, vm-name): PS Azure :\\> az vm extension set - -publisher Microsoft . Azure . Diagnostics - -name IaaSDiagnostics - -version 1 . 5 - -resource-group company-resource-group - -vm-name company-windows - -protected-settings protected_settings . json - -settings public_settings . json - -subscription uuid","title":"Windows Virtual Machine"},{"location":"integrations/azure_windows/#sysmon","text":"Sysmon tool from Microsoft could improve the detection on Windows computers. You could download the tool on Microsoft website . If you do not know how to use and configure it, please check SwiftOnSecurity github .","title":"Sysmon"},{"location":"integrations/azure_windows/#enjoy-your-events","text":"You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/azure_windows/#further-readings","text":"Microsoft Github windows diagnostic extension documentation","title":"Further Readings"},{"location":"integrations/bind/","text":"Overview BIND is an implementation of the Domain Name System (DNS) of the Internet. It performs both of the main DNS server roles, acting as an authoritative name server for domains, and acting as a recursive resolver in the network. Setup This setup guide will show you how to forward logs produced by your BIND server to SEKOIA.IO by means of an rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls . Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure BIND to log queries First, you need to configure your BIND daemon to log queries and forward them to your rsyslog instance. If rsyslog and BIND are installed on the same box, you can simply add the following statement in your BIND\u2019s main configuration file: logging { channel syslog_chan { syslog daemon; severity dynamic; }; category default { syslog_chan; }; category queries { syslog_chan; }; category config { syslog_chan; }; category security { syslog_chan; }; }; You can find more informations on how to configure your BIND instance on its official website . Configure the rsyslog server Open or create a new BIND configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-bind.conf Paste the following rsyslog configuration to trigger the emission of BIND logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOBINDTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOBINDTemplate template if $programname startswith 'named' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOBINDTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"BIND"},{"location":"integrations/bind/#overview","text":"BIND is an implementation of the Domain Name System (DNS) of the Internet. It performs both of the main DNS server roles, acting as an authoritative name server for domains, and acting as a recursive resolver in the network.","title":"Overview"},{"location":"integrations/bind/#setup","text":"This setup guide will show you how to forward logs produced by your BIND server to SEKOIA.IO by means of an rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls .","title":"Setup"},{"location":"integrations/bind/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/bind/#configure-bind-to-log-queries","text":"First, you need to configure your BIND daemon to log queries and forward them to your rsyslog instance. If rsyslog and BIND are installed on the same box, you can simply add the following statement in your BIND\u2019s main configuration file: logging { channel syslog_chan { syslog daemon; severity dynamic; }; category default { syslog_chan; }; category queries { syslog_chan; }; category config { syslog_chan; }; category security { syslog_chan; }; }; You can find more informations on how to configure your BIND instance on its official website .","title":"Configure BIND to log queries"},{"location":"integrations/bind/#configure-the-rsyslog-server","text":"Open or create a new BIND configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-bind.conf Paste the following rsyslog configuration to trigger the emission of BIND logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOBINDTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOBINDTemplate template if $programname startswith 'named' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOBINDTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the rsyslog server"},{"location":"integrations/bind/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/bind/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/bind/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/cef/","text":"Overview ArcSight's Common Event Format (CEF) is an open log management standard. If one of your applications or devices is not covered by one of the other intakes we support but can produce logs in CEF you can use this intake. Still we recommend using an intake tailored to your specific application or device, even with CEF, in order to ensure you get the most out of your logs. If an intake is missing, please contact us . Setup This setup guide will show you how to forward your CEF logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new configuration file for rsyslog: sudo vim /etc/rsyslog.d/xx-yyyyy.conf Customize the following rsyslog configuration to trigger the emission of logs by your rsyslog server to SEKOIA.IO. # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Collects logs out of the dedicated socket, uncomment if needed # $AddUnixListenSocket ... # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCefTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOCefTemplate template @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOCefTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading CEF specification","title":"Common Event Format"},{"location":"integrations/cef/#overview","text":"ArcSight's Common Event Format (CEF) is an open log management standard. If one of your applications or devices is not covered by one of the other intakes we support but can produce logs in CEF you can use this intake. Still we recommend using an intake tailored to your specific application or device, even with CEF, in order to ensure you get the most out of your logs. If an intake is missing, please contact us .","title":"Overview"},{"location":"integrations/cef/#setup","text":"This setup guide will show you how to forward your CEF logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/cef/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/cef/#configure-the-rsyslog-server","text":"Open or create a new configuration file for rsyslog: sudo vim /etc/rsyslog.d/xx-yyyyy.conf Customize the following rsyslog configuration to trigger the emission of logs by your rsyslog server to SEKOIA.IO. # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Collects logs out of the dedicated socket, uncomment if needed # $AddUnixListenSocket ... # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCefTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOCefTemplate template @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOCefTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/cef/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/cef/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/cef/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/cef/#further-reading","text":"CEF specification","title":"Further Reading"},{"location":"integrations/checkpoint/","text":"Overview Check Point\u2019s Next Generation Firewalls (NGFW\u2019s) are trusted by customers for their highest security effectiveness and their ability to keep organizations protected from sophisticated fifth generation cyber-attacks. Check Point\u2019s NGFW includes 23 Firewall models optimized for running all threat prevention technologies simultaneously, including full SSL traffic inspection, without compromising on security or performance. Setup This setup guide will show you how to forward your Checkpoint firewall logs to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. We are currently supporting the following firewall versions: R77.30, R80.10, R80.20, R80.30. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure Checkpoint The first step is to configure Checkpoint to log the awaited traffic. This could be done differently depending on your current setup, one solution is to export logs with the Log Exporter . Configure the Rsyslog server You can configure your Rsyslog server to forward your checkpoint logs to SEKOIA.IO. Open or create a new Checkpoint configuration file for rsyslog: sudo vim /etc/rsyslog.d/52-checkpoint.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCheckpointTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] CEF:%msg%\\n\" ) # Send your Checkpoint events to SEKOIA.IO intake servers under SEKOIAIOCheckpointTemplate template if $hostname == \"YOUR_CHECKPOINT_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOCheckpointTemplate In the above template instruction, change the YOUR_CHECKPOINT_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"Checkpoint"},{"location":"integrations/checkpoint/#overview","text":"Check Point\u2019s Next Generation Firewalls (NGFW\u2019s) are trusted by customers for their highest security effectiveness and their ability to keep organizations protected from sophisticated fifth generation cyber-attacks. Check Point\u2019s NGFW includes 23 Firewall models optimized for running all threat prevention technologies simultaneously, including full SSL traffic inspection, without compromising on security or performance.","title":"Overview"},{"location":"integrations/checkpoint/#setup","text":"This setup guide will show you how to forward your Checkpoint firewall logs to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. We are currently supporting the following firewall versions: R77.30, R80.10, R80.20, R80.30.","title":"Setup"},{"location":"integrations/checkpoint/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/checkpoint/#configure-checkpoint","text":"The first step is to configure Checkpoint to log the awaited traffic. This could be done differently depending on your current setup, one solution is to export logs with the Log Exporter .","title":"Configure Checkpoint"},{"location":"integrations/checkpoint/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your checkpoint logs to SEKOIA.IO. Open or create a new Checkpoint configuration file for rsyslog: sudo vim /etc/rsyslog.d/52-checkpoint.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCheckpointTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] CEF:%msg%\\n\" ) # Send your Checkpoint events to SEKOIA.IO intake servers under SEKOIAIOCheckpointTemplate template if $hostname == \"YOUR_CHECKPOINT_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOCheckpointTemplate In the above template instruction, change the YOUR_CHECKPOINT_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/checkpoint/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/checkpoint/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/checkpoint/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/cisco_asa/","text":"Overview The Cisco ASA is a security device that combines firewall, antivirus, intrusion prevention, and virtual private network (VPN) capabilities. It provides proactive threat defense that stops attacks before they spread through the network. Therefore, the Cisco ASA firewall is the whole package, so to speak. In this documentation we will explain one way to collect and send CISCO ASA logs to SEKOIA.IO. From the CISCO ASA machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO CISCO ASA logs On CISCO appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are enable by one simple command. To enable logging, enter the following commands: hostname ( config ) # logging enable hostname ( config ) # logging timestamp hostname ( config ) # logging trap informational Transport to the concentrator Prerequisites The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the CISCO ASA Traffic towards the Rsyslog must be open on UDP 514 Configure the CISCO ASA In ordre to forward the logs to a Rsyslog, please follow those commands: Note the interface name hostname ( config ) # show interface Note the host name hostname ( config ) # show hostname You then have to configure an output destination for logs. Here, we choose to send syslog messages to an external syslog server. hostname ( config ) # logging host interface_name syslog_ip [ tcp[/ port ] udp [/ port ] Example: hostname ( config ) # logging host interface_1 127.0.0.1 udp Explanations: The interface_name argument specifies the interface through which you access the syslog server. The syslog_ip argument specifies the IP address of the syslog server. The tcp[/ port ] or udp[/ port ] keyword and argument pair specify that the ASA and ASASM should use TCP or UDP to send syslog messages to the syslog server. You can configure the ASA to send data to a syslog server using either UDP or TCP, but not both. The default protocol is UDP if you do not specify a protocol. If you specify TCP, the ASA discovers when the syslog server fails and as a security protection, new connections through the ASA are blocked. If you specify UDP, the ASA continues to allow new connections whether or not the syslog server is operational. Valid port values for either protocol are 1025 through 65535. The default UDP port is 514. The default TCP port is 1470. For more information about Cisco ASA logging, please refer to your Cisco documentation . Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the Rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allowed in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new cisco-asa configuration file for Rsyslog: sudo vim /etc/rsyslog.d/11-cisco-asa.conf Paste the following Rsyslog configuration to trigger the emission of CISCO logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCiscoAsaTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOCiscoAsaTemplate template if ( $hostname == \"YOUR_CISCO_ASA_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOCiscoAsaTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_CISCO_ASA_HOSTNAME variable with the correct value. Restart Rsyslog sudo systemctl restart rsyslog.service Enjoy your events Go to the events page to watch your incoming events.","title":"Cisco"},{"location":"integrations/cisco_asa/#overview","text":"The Cisco ASA is a security device that combines firewall, antivirus, intrusion prevention, and virtual private network (VPN) capabilities. It provides proactive threat defense that stops attacks before they spread through the network. Therefore, the Cisco ASA firewall is the whole package, so to speak. In this documentation we will explain one way to collect and send CISCO ASA logs to SEKOIA.IO. From the CISCO ASA machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO","title":"Overview"},{"location":"integrations/cisco_asa/#cisco-asa-logs","text":"On CISCO appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are enable by one simple command. To enable logging, enter the following commands: hostname ( config ) # logging enable hostname ( config ) # logging timestamp hostname ( config ) # logging trap informational","title":"CISCO ASA logs"},{"location":"integrations/cisco_asa/#transport-to-the-concentrator","text":"","title":"Transport to the concentrator"},{"location":"integrations/cisco_asa/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the CISCO ASA Traffic towards the Rsyslog must be open on UDP 514","title":"Prerequisites"},{"location":"integrations/cisco_asa/#configure-the-cisco-asa","text":"In ordre to forward the logs to a Rsyslog, please follow those commands: Note the interface name hostname ( config ) # show interface Note the host name hostname ( config ) # show hostname You then have to configure an output destination for logs. Here, we choose to send syslog messages to an external syslog server. hostname ( config ) # logging host interface_name syslog_ip [ tcp[/ port ] udp [/ port ] Example: hostname ( config ) # logging host interface_1 127.0.0.1 udp Explanations: The interface_name argument specifies the interface through which you access the syslog server. The syslog_ip argument specifies the IP address of the syslog server. The tcp[/ port ] or udp[/ port ] keyword and argument pair specify that the ASA and ASASM should use TCP or UDP to send syslog messages to the syslog server. You can configure the ASA to send data to a syslog server using either UDP or TCP, but not both. The default protocol is UDP if you do not specify a protocol. If you specify TCP, the ASA discovers when the syslog server fails and as a security protection, new connections through the ASA are blocked. If you specify UDP, the ASA continues to allow new connections whether or not the syslog server is operational. Valid port values for either protocol are 1025 through 65535. The default UDP port is 514. The default TCP port is 1470. For more information about Cisco ASA logging, please refer to your Cisco documentation .","title":"Configure the CISCO ASA"},{"location":"integrations/cisco_asa/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/cisco_asa/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/cisco_asa/#rsyslog-prerequisites","text":"In order to allow the Rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allowed in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/cisco_asa/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/cisco_asa/#configure-the-rsyslog-server","text":"Open or create a new cisco-asa configuration file for Rsyslog: sudo vim /etc/rsyslog.d/11-cisco-asa.conf Paste the following Rsyslog configuration to trigger the emission of CISCO logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOCiscoAsaTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOCiscoAsaTemplate template if ( $hostname == \"YOUR_CISCO_ASA_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOCiscoAsaTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_CISCO_ASA_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog server"},{"location":"integrations/cisco_asa/#restart-rsyslog","text":"sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/cisco_asa/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/dhcpd/","text":"Overview ISC DHCP offers a complete open source solution for implementing DHCP servers. Setup This setup guide will show you how to forward logs produced by your DHCP servers to SEKOIA.IO by means of an rsyslog transport channel. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new DHCP configuration file for rsyslog: sudo vim /etc/rsyslog.d/39-dhcp.conf Paste the following rsyslog configuration to trigger the emission of DHCP logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOdhcpTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIODhcpdTemplate template if $programname startswith 'dhcpd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOdhcpTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"ISC DHCP"},{"location":"integrations/dhcpd/#overview","text":"ISC DHCP offers a complete open source solution for implementing DHCP servers.","title":"Overview"},{"location":"integrations/dhcpd/#setup","text":"This setup guide will show you how to forward logs produced by your DHCP servers to SEKOIA.IO by means of an rsyslog transport channel.","title":"Setup"},{"location":"integrations/dhcpd/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/dhcpd/#configure-the-rsyslog-server","text":"Open or create a new DHCP configuration file for rsyslog: sudo vim /etc/rsyslog.d/39-dhcp.conf Paste the following rsyslog configuration to trigger the emission of DHCP logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOdhcpTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIODhcpdTemplate template if $programname startswith 'dhcpd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOdhcpTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/dhcpd/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/dhcpd/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/dhcpd/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/f5-big-ip/","text":"Overview F5's BIG-IP is a family of products covering software and hardware designed around application availability, access control, and security solutions. Setup We expect logs formated in priority as CEF, and encapsulated in a syslog message including your intake key, to be sent on intake.sekoia.io:10514 using the following certificate: SEKOIA-IO-intake.pem . The other reporting format (key/value pairs) is also supported, but the required configuration will not be detailed in the following example. In this setup guide you will set up an rsyslog server to add your intake key and forward securely your BIG-IP logs to our servers. We first explain how to configure your rsyslog server, then we show how to configure a Log Publisher to format your logs as CEF and send them to your rsyslog server. Most BIG-IP modules can use Log Publishers, some can directly log messages as CEF to an rsyslog server. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Configure the Rsyslog server In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem You can now configure your Rsyslog server to forward your logs to SEKOIA.IO. Open or create a new configuration file for rsyslog: sudo vim /etc/rsyslog.d/22-big-ip.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver, make sure rsyslog-gnutls is installed $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOBigIpTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # send the logs corresponding to a given module, as an example for ASM: if $programname startswith 'ASM' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOBigIpTemplate In the above template instruction, adapt the line corresponding to your use case in the last block, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog: $ sudo service rsyslog restart Configure a Log Publisher Before creating a log publisher, you first need a management port log destination: System -> Logs -> Configuration -> Log Destinations -> Create... Give it a name, choose type Management Port , fill the address and port of your rsyslog server and select protocol UDP (alternatively you can define a pool of rsyslog servers and use it as a remote high speed log destination). Then you need another log destination to format your logs in CEF: System -> Logs -> Configuration -> Log Destinations -> Create... Give it a name, choose type ArcSight , and forward to the log destination you just created. You can now create a log publisher: System -> Logs -> Configuration -> Log Publishers -> Create... Give it a name, and select the ArcSight log destination you just created. You can now use this log publisher to define logging profiles in your BIG-IP modules. As an example in: Access -> Overview -> Event Logs -> Settings -> Create -> Access System Logs -> Publisher or Security -> Event Logs -> Logging Profiles -> Create... -> Publisher Direct Configuration Some modules allow direct configuration to the rsyslog server. As an example: Security -> Event Logs -> Logging Profiles -> Create... Then choose Application Security , select Remote Storage as a storage destination, Common Event Format (ArcSight) as a logging format, and fill in your rsyslog server info. The resulting logging profile can be applied to a given virtual server in: Local Traffic -> Virtual Servers -> Virtual Server List Then choose a virtual server, go to the Security -> Policies tab and apply the log profile. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"F5 BigIP"},{"location":"integrations/f5-big-ip/#overview","text":"F5's BIG-IP is a family of products covering software and hardware designed around application availability, access control, and security solutions.","title":"Overview"},{"location":"integrations/f5-big-ip/#setup","text":"We expect logs formated in priority as CEF, and encapsulated in a syslog message including your intake key, to be sent on intake.sekoia.io:10514 using the following certificate: SEKOIA-IO-intake.pem . The other reporting format (key/value pairs) is also supported, but the required configuration will not be detailed in the following example. In this setup guide you will set up an rsyslog server to add your intake key and forward securely your BIG-IP logs to our servers. We first explain how to configure your rsyslog server, then we show how to configure a Log Publisher to format your logs as CEF and send them to your rsyslog server. Most BIG-IP modules can use Log Publishers, some can directly log messages as CEF to an rsyslog server. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/f5-big-ip/#configure-the-rsyslog-server","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem You can now configure your Rsyslog server to forward your logs to SEKOIA.IO. Open or create a new configuration file for rsyslog: sudo vim /etc/rsyslog.d/22-big-ip.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver, make sure rsyslog-gnutls is installed $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOBigIpTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # send the logs corresponding to a given module, as an example for ASM: if $programname startswith 'ASM' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOBigIpTemplate In the above template instruction, adapt the line corresponding to your use case in the last block, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog: $ sudo service rsyslog restart","title":"Configure the Rsyslog server"},{"location":"integrations/f5-big-ip/#configure-a-log-publisher","text":"Before creating a log publisher, you first need a management port log destination: System -> Logs -> Configuration -> Log Destinations -> Create... Give it a name, choose type Management Port , fill the address and port of your rsyslog server and select protocol UDP (alternatively you can define a pool of rsyslog servers and use it as a remote high speed log destination). Then you need another log destination to format your logs in CEF: System -> Logs -> Configuration -> Log Destinations -> Create... Give it a name, choose type ArcSight , and forward to the log destination you just created. You can now create a log publisher: System -> Logs -> Configuration -> Log Publishers -> Create... Give it a name, and select the ArcSight log destination you just created. You can now use this log publisher to define logging profiles in your BIG-IP modules. As an example in: Access -> Overview -> Event Logs -> Settings -> Create -> Access System Logs -> Publisher or Security -> Event Logs -> Logging Profiles -> Create... -> Publisher","title":"Configure a Log Publisher"},{"location":"integrations/f5-big-ip/#direct-configuration","text":"Some modules allow direct configuration to the rsyslog server. As an example: Security -> Event Logs -> Logging Profiles -> Create... Then choose Application Security , select Remote Storage as a storage destination, Common Event Format (ArcSight) as a logging format, and fill in your rsyslog server info. The resulting logging profile can be applied to a given virtual server in: Local Traffic -> Virtual Servers -> Virtual Server List Then choose a virtual server, go to the Security -> Policies tab and apply the log profile.","title":"Direct Configuration"},{"location":"integrations/f5-big-ip/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/f5-big-ip/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/fortigate/","text":"Overview The range of Fortigate firewalls is a complete appliance solution whose security functions are highly developed. The firewalls run on the FortiOS operating system. In this documentation we will explain one way to collect and send Fortigate logs to SEKOIA.IO. From the Fortigate machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO Fortigate logs On Fortigate appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Local out traffic, Denied traffic, Allowed traffic Web Url-Filtering VPN Transport to the concentrator Prerequisites The following prerequisites are needed in order to setup efficient log concentration: Have administrator writes on the Fortigate Traffic towards the Rsyslog must be open on TCP/514 Configure Fortigate The first step is to configure Fortigate to log the awaited traffic. You can configure FortiOS to send log messages to remote syslog servers in standard, CSV or CEF (Common Event Format) format. These three formats are accepted by the SEKOIA.IO intake. To enable syslog, log into the CLI and enter the following commands: config log syslogd setting set status enable set port 514 set mode reliable set server [ IP address of syslog server ] set facility user set format rfc5424 end Most FortiGate features are enabled for logging by default. Ensure they are enabled by executing the following command: show full-configuration Make sure the Traffic, Web and URL Filtering features are enabled for logging with the following commands: config log syslogd filter set forward-traffic enable set local-traffic enable set multicast-traffic enable .... set vpn enable set web enable set url-filter enable end With some Fortigate appliance, it may not be possible to do the above configuration through the command line. An alternative method is to use the graphical interface and go to the Log Settings menu. From there you can choose every logging options within Event Logging and Local Traffic Log except for the Denied options. Then in order to use CEF format, use the following commands : config log syslogd setting set format cef end Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the TCP incoming events are allows in the /etc/rsyslog.conf .... # provides TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Fortigate configuration file for Rsyslog: sudo vim /etc/rsyslog.d/12-fortigate.conf Paste the following Rsyslog configuration to trigger the emission of Fortigate logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortigateTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] CEF:%msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortigateTemplate template if ( $hostname == \"YOUR_FORTIGATE_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortigateTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIGATE_HOSTNAME variable with the correct value. Restart Rsyslog $ sudo systemctl restart rsyslog.service Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"FortiGate"},{"location":"integrations/fortigate/#overview","text":"The range of Fortigate firewalls is a complete appliance solution whose security functions are highly developed. The firewalls run on the FortiOS operating system. In this documentation we will explain one way to collect and send Fortigate logs to SEKOIA.IO. From the Fortigate machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO","title":"Overview"},{"location":"integrations/fortigate/#fortigate-logs","text":"On Fortigate appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Local out traffic, Denied traffic, Allowed traffic Web Url-Filtering VPN","title":"Fortigate logs"},{"location":"integrations/fortigate/#transport-to-the-concentrator","text":"","title":"Transport to the concentrator"},{"location":"integrations/fortigate/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: Have administrator writes on the Fortigate Traffic towards the Rsyslog must be open on TCP/514","title":"Prerequisites"},{"location":"integrations/fortigate/#configure-fortigate","text":"The first step is to configure Fortigate to log the awaited traffic. You can configure FortiOS to send log messages to remote syslog servers in standard, CSV or CEF (Common Event Format) format. These three formats are accepted by the SEKOIA.IO intake. To enable syslog, log into the CLI and enter the following commands: config log syslogd setting set status enable set port 514 set mode reliable set server [ IP address of syslog server ] set facility user set format rfc5424 end Most FortiGate features are enabled for logging by default. Ensure they are enabled by executing the following command: show full-configuration Make sure the Traffic, Web and URL Filtering features are enabled for logging with the following commands: config log syslogd filter set forward-traffic enable set local-traffic enable set multicast-traffic enable .... set vpn enable set web enable set url-filter enable end With some Fortigate appliance, it may not be possible to do the above configuration through the command line. An alternative method is to use the graphical interface and go to the Log Settings menu. From there you can choose every logging options within Event Logging and Local Traffic Log except for the Denied options. Then in order to use CEF format, use the following commands : config log syslogd setting set format cef end","title":"Configure Fortigate"},{"location":"integrations/fortigate/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/fortigate/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/fortigate/#rsyslog-prerequisites","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the TCP incoming events are allows in the /etc/rsyslog.conf .... # provides TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/fortigate/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/fortigate/#configure-the-rsyslog-server","text":"Open or create a new Fortigate configuration file for Rsyslog: sudo vim /etc/rsyslog.d/12-fortigate.conf Paste the following Rsyslog configuration to trigger the emission of Fortigate logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortigateTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] CEF:%msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortigateTemplate template if ( $hostname == \"YOUR_FORTIGATE_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortigateTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIGATE_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog server"},{"location":"integrations/fortigate/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/fortigate/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/fortigate/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/fortimail/","text":"Overview Fortinet cybersecurity solutions sells physical products such as firewalls, plus software and services such as anti-virus protection, intrusion prevention systems and endpoint security components. FortiMail logs On FortiMail appliances, most of the important hardware and software activities that are relevant for security detection and analysis, are logged into six files. History (statistics): Records all email traffic going through the FortiMail unit. System Event (kevent): Records system management activities, including changes to the system configuration as well as administrator and user log in and log outs. Mail Event (event): Records mail activities. Antispam (spam): Records spam detection events. Antivirus (virus): Records virus intrusion events. Encryption (encrypt): Records detection of IBE-related events. Transport to the collector Prerequisites The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the FortiMail appliance Traffic towards the Rsyslog must be open on UDP 514 Configure FortiMail Configure logging to a RSYSLOG server Go to Log and Report > Log Settings > Remote . Click New to create a new entry OR double-click an existing entry to modify it. A dialog appears . Select Enable to allow logging to a remote host. In Profile name , enter a profile name. In IP , enter the IP address of the Syslog server where the FortiMail unit will store the logs. In Port , enter the UDP port number on which the Syslog server listens for connections (by default, UDP 514 ). From Level , select the severity level that a log message must equal or exceed in order to be recorded to this storage location. From Facility , select the facility identifier that the FortiMail unit will use to identify itself when sending log messages. To easily identify log messages from the FortiMail unit when they are stored on a remote logging server, enter a unique facility identifier, and verify that no other network devices use the same facility identifier. From Log protocol , select Syslog . In Logging Policy Configuration , enable the types of logs you want to record to this storage location. Click the arrow to review the options . Click Create . For detailed information about configuring a log forwarding, see Configure FortiMail Log Forwarding Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allows in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new FortiMail configuration file for Rsyslog: sudo vim /etc/rsyslog.d/13-fortimail.conf Paste the following Rsyslog configuration to trigger the emission of FortiMail logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortMailTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortMailTemplate template if ( $hostname == \"YOUR_FORTIMAIL_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortMailTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIMAIL_HOSTNAME variable with the correct value. Restart Rsyslog $ sudo systemctl restart rsyslog.service Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"FortiMail"},{"location":"integrations/fortimail/#overview","text":"Fortinet cybersecurity solutions sells physical products such as firewalls, plus software and services such as anti-virus protection, intrusion prevention systems and endpoint security components.","title":"Overview"},{"location":"integrations/fortimail/#fortimail-logs","text":"On FortiMail appliances, most of the important hardware and software activities that are relevant for security detection and analysis, are logged into six files. History (statistics): Records all email traffic going through the FortiMail unit. System Event (kevent): Records system management activities, including changes to the system configuration as well as administrator and user log in and log outs. Mail Event (event): Records mail activities. Antispam (spam): Records spam detection events. Antivirus (virus): Records virus intrusion events. Encryption (encrypt): Records detection of IBE-related events.","title":"FortiMail logs"},{"location":"integrations/fortimail/#transport-to-the-collector","text":"","title":"Transport to the collector"},{"location":"integrations/fortimail/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: Have administrator privileges on the FortiMail appliance Traffic towards the Rsyslog must be open on UDP 514","title":"Prerequisites"},{"location":"integrations/fortimail/#configure-fortimail","text":"","title":"Configure FortiMail"},{"location":"integrations/fortimail/#configure-logging-to-a-rsyslog-server","text":"Go to Log and Report > Log Settings > Remote . Click New to create a new entry OR double-click an existing entry to modify it. A dialog appears . Select Enable to allow logging to a remote host. In Profile name , enter a profile name. In IP , enter the IP address of the Syslog server where the FortiMail unit will store the logs. In Port , enter the UDP port number on which the Syslog server listens for connections (by default, UDP 514 ). From Level , select the severity level that a log message must equal or exceed in order to be recorded to this storage location. From Facility , select the facility identifier that the FortiMail unit will use to identify itself when sending log messages. To easily identify log messages from the FortiMail unit when they are stored on a remote logging server, enter a unique facility identifier, and verify that no other network devices use the same facility identifier. From Log protocol , select Syslog . In Logging Policy Configuration , enable the types of logs you want to record to this storage location. Click the arrow to review the options . Click Create . For detailed information about configuring a log forwarding, see Configure FortiMail Log Forwarding","title":"Configure logging to a RSYSLOG server"},{"location":"integrations/fortimail/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/fortimail/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/fortimail/#rsyslog-prerequisites","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allows in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/fortimail/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/fortimail/#configure-the-rsyslog-server","text":"Open or create a new FortiMail configuration file for Rsyslog: sudo vim /etc/rsyslog.d/13-fortimail.conf Paste the following Rsyslog configuration to trigger the emission of FortiMail logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortMailTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortMailTemplate template if ( $hostname == \"YOUR_FORTIMAIL_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortMailTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIMAIL_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog server"},{"location":"integrations/fortimail/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/fortimail/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/fortimail/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/fortiweb/","text":"Overview In this documentation we will explain one way to collect and send FortiWeb logs to SEKOIA.IO. - From the FortiWeb machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO FortiWeb logs On FortiWeb appliances, most of the important hardware and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Displays traffic flow information, such as HTTP/HTTPS requests and responses. Event: Displays administrative events, such as downloading a backup copy of the configuration, and hardware failures. Attack: Displays attack and intrusion attempt events. Transport to the concentrator Prerequisites The following prerequisites are needed in order to setup efficient log concentration: - Have administrator writes on the FortiWeb (read & write permission) - Traffic towards the Rsyslog must be open on UDP 514 Configure FortiWeb Enable logging via trigger mechanism Go to Log&Report > Log Config > Other Log Settings Tick the boxes : Enable Attack Log / Enable Traffic Log / Enable Event Log Configure Syslog policies Go to Log&Report > Log Policy > Syslog Policy . To access this part of the web UI, your administrator\u2019s account access profile must have Read and Write permission to items in the Log & Report category. For details, see Permissions. Click Create New . If the policy is new, in Policy Name, type the name of the policy as it will be referenced in the configuration. Click Create New. In IP Address , enter the address of the remote Syslog server. In Port , enter the listening port number of the Syslog server. The default is 514. Click OK . Configure log destinations Go to Log&Report > Log Config > Global Log Settings Tick the syslog box Select the relevant Syslog Policy, Log Level and Facility Click Apply For more information please refer to the official documentation of FortiWeb Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allowed in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new FortiWeb configuration file for Rsyslog: sudo vim /etc/rsyslog.d/14-fortiweb.conf Paste the following Rsyslog configuration to trigger the emission of FortiWeb logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortiwebTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortiwebTemplate template if ( $hostname == \"YOUR_FORTIWEB_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortiwebTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIWEB_HOSTNAME variable with the correct value. Restart Rsyslog $ sudo systemctl restart rsyslog.service Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) IV. Enjoy your events Go to the events page to watch your incoming events.","title":"FortiWeb"},{"location":"integrations/fortiweb/#overview","text":"In this documentation we will explain one way to collect and send FortiWeb logs to SEKOIA.IO. - From the FortiWeb machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO","title":"Overview"},{"location":"integrations/fortiweb/#fortiweb-logs","text":"On FortiWeb appliances, most of the important hardware and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Displays traffic flow information, such as HTTP/HTTPS requests and responses. Event: Displays administrative events, such as downloading a backup copy of the configuration, and hardware failures. Attack: Displays attack and intrusion attempt events.","title":"FortiWeb logs"},{"location":"integrations/fortiweb/#transport-to-the-concentrator","text":"","title":"Transport to the concentrator"},{"location":"integrations/fortiweb/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: - Have administrator writes on the FortiWeb (read & write permission) - Traffic towards the Rsyslog must be open on UDP 514","title":"Prerequisites"},{"location":"integrations/fortiweb/#configure-fortiweb","text":"","title":"Configure FortiWeb"},{"location":"integrations/fortiweb/#enable-logging-via-trigger-mechanism","text":"Go to Log&Report > Log Config > Other Log Settings Tick the boxes : Enable Attack Log / Enable Traffic Log / Enable Event Log","title":"Enable logging via trigger mechanism"},{"location":"integrations/fortiweb/#configure-syslog-policies","text":"Go to Log&Report > Log Policy > Syslog Policy . To access this part of the web UI, your administrator\u2019s account access profile must have Read and Write permission to items in the Log & Report category. For details, see Permissions. Click Create New . If the policy is new, in Policy Name, type the name of the policy as it will be referenced in the configuration. Click Create New. In IP Address , enter the address of the remote Syslog server. In Port , enter the listening port number of the Syslog server. The default is 514. Click OK .","title":"Configure Syslog policies"},{"location":"integrations/fortiweb/#configure-log-destinations","text":"Go to Log&Report > Log Config > Global Log Settings Tick the syslog box Select the relevant Syslog Policy, Log Level and Facility Click Apply For more information please refer to the official documentation of FortiWeb","title":"Configure log destinations"},{"location":"integrations/fortiweb/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/fortiweb/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/fortiweb/#rsyslog-prerequisites","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the UDP incoming events are allowed in the /etc/rsyslog.conf .... # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/fortiweb/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/fortiweb/#configure-the-rsyslog-server","text":"Open or create a new FortiWeb configuration file for Rsyslog: sudo vim /etc/rsyslog.d/14-fortiweb.conf Paste the following Rsyslog configuration to trigger the emission of FortiWeb logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOFortiwebTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOFortiwebTemplate template if ( $hostname == \"YOUR_FORTIWEB_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOFortiwebTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_FORTIWEB_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog server"},{"location":"integrations/fortiweb/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/fortiweb/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/fortiweb/#iv-enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"IV. Enjoy your events"},{"location":"integrations/haproxy/","text":"Overview HAProxy is a free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. HAProxy has a lot of features and because it is located between your infrastructure and your clients, it can give you a lot of information about either of them. Setup This setup guide will show you how to forward your HAProxy logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Haproxy configuration file for rsyslog: sudo vim /etc/rsyslog.d/49-haproxy.conf Paste the following rsyslog configuration to trigger the emission of haproxy logs by your rsyslog server to SEKOIA.IO. # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Collects haproxy logs out of the dedicated HAProxy socket $AddUnixListenSocket /var/lib/haproxy/dev/log # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOHaproxyTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your HAProxy events to SEKOIA.IO intake servers under SEKOIAIOHaproxyTemplate template if $programname startswith 'haproxy' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOHaproxyTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading HAProxy Official Documentation","title":"HAProxy"},{"location":"integrations/haproxy/#overview","text":"HAProxy is a free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. HAProxy has a lot of features and because it is located between your infrastructure and your clients, it can give you a lot of information about either of them.","title":"Overview"},{"location":"integrations/haproxy/#setup","text":"This setup guide will show you how to forward your HAProxy logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/haproxy/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/haproxy/#configure-the-rsyslog-server","text":"Open or create a new Haproxy configuration file for rsyslog: sudo vim /etc/rsyslog.d/49-haproxy.conf Paste the following rsyslog configuration to trigger the emission of haproxy logs by your rsyslog server to SEKOIA.IO. # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Collects haproxy logs out of the dedicated HAProxy socket $AddUnixListenSocket /var/lib/haproxy/dev/log # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOHaproxyTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your HAProxy events to SEKOIA.IO intake servers under SEKOIAIOHaproxyTemplate template if $programname startswith 'haproxy' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOHaproxyTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/haproxy/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/haproxy/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/haproxy/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/haproxy/#further-reading","text":"HAProxy Official Documentation","title":"Further Reading"},{"location":"integrations/harfanglab/","text":"Overview HarfangLab is an Endpoint detection and response (EDR) solution certified by ANSSI since 2020. This setup guide will show you how to forward events produced by HarfangLab EDR to SEKOIA.IO. Theses changes have to be made from your HarfangLab instance web portal (provided by HarfangLab). HarfangLab EDR logs Firstable your need to navigate to Personal Settings , and in the Api token get your token or generate a new one. Then you need to navigate to Administration > Configuration , and switch to the Connectors tab. In the Syslog connector panel, select the logs you want to export: Process Network Event log Remote thread InjectedThread Security Event Configure the syslog information with the following details: Host: intake.sekoia.io Port: 10514 App name: name of your choice Source host: name of your choice Structured data: [SEKOIA@53288 intake_key=\"YOUR_INTAKE_KEY\"] Protocol: TCP/SSL In the above field Structured data , please replace YOUR_INTAKE_KEY variable with your intake key generated in SEKOIA.IO's intake page . Finaly select the Protocol option: TCP/SSL , leave the other options to default. Enjoy your events Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"HarfangLab"},{"location":"integrations/harfanglab/#overview","text":"HarfangLab is an Endpoint detection and response (EDR) solution certified by ANSSI since 2020. This setup guide will show you how to forward events produced by HarfangLab EDR to SEKOIA.IO. Theses changes have to be made from your HarfangLab instance web portal (provided by HarfangLab).","title":"Overview"},{"location":"integrations/harfanglab/#harfanglab-edr-logs","text":"Firstable your need to navigate to Personal Settings , and in the Api token get your token or generate a new one. Then you need to navigate to Administration > Configuration , and switch to the Connectors tab. In the Syslog connector panel, select the logs you want to export: Process Network Event log Remote thread InjectedThread Security Event Configure the syslog information with the following details: Host: intake.sekoia.io Port: 10514 App name: name of your choice Source host: name of your choice Structured data: [SEKOIA@53288 intake_key=\"YOUR_INTAKE_KEY\"] Protocol: TCP/SSL In the above field Structured data , please replace YOUR_INTAKE_KEY variable with your intake key generated in SEKOIA.IO's intake page . Finaly select the Protocol option: TCP/SSL , leave the other options to default.","title":"HarfangLab EDR logs"},{"location":"integrations/harfanglab/#enjoy-your-events","text":"Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/linux/","text":"Overview Linux is a family of free and open-source software operating systems built around the Linux kernel. Setup This setup guide will show you how to forward logs produced by your Linux servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server By default, a Linux server logs common daemons activities, the above configuration forwards everything. Depending of the linux distribution and installed software, more configuration tuning could be required. Open or create a new Linux configuration file for rsyslog: sudo vim /etc/rsyslog.d/8-linux.conf Paste the following rsyslog configuration to trigger the emission of Linux logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOLinuxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOLinuxTemplate template if $hostname == \"YOUR_LINUX_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOLinuxTemplate In the above template instruction, change the YOUR_LINUX_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Readings Logging with rsyslog on Redhat","title":"Linux"},{"location":"integrations/linux/#overview","text":"Linux is a family of free and open-source software operating systems built around the Linux kernel.","title":"Overview"},{"location":"integrations/linux/#setup","text":"This setup guide will show you how to forward logs produced by your Linux servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/linux/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/linux/#configure-the-rsyslog-server","text":"By default, a Linux server logs common daemons activities, the above configuration forwards everything. Depending of the linux distribution and installed software, more configuration tuning could be required. Open or create a new Linux configuration file for rsyslog: sudo vim /etc/rsyslog.d/8-linux.conf Paste the following rsyslog configuration to trigger the emission of Linux logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOLinuxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOLinuxTemplate template if $hostname == \"YOUR_LINUX_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOLinuxTemplate In the above template instruction, change the YOUR_LINUX_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/linux/#restart-rsyslog","text":"sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/linux/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/linux/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/linux/#further-readings","text":"Logging with rsyslog on Redhat","title":"Further Readings"},{"location":"integrations/log_insight_windows/","text":"Overview Microsoft Windows is a popular operating system developed by Microsoft since 1985. It's available in three variants: Windows for desktop/laptop computers, tablets and smartphones Windows Server for servers Windows PE as a lightweight version. Setup This setup guide will show you how to forward events produced by a Windows system, collected by Log Insight agent and forward to SEKOIA.IO through your local rsyslog server. Configure the forwarder through rsyslog Rsyslog setup on Linux Server Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new windows configuration file for rsyslog: sudo vim /etc/rsyslog.d/15-windows.conf Paste the following rsyslog configuration to trigger the emission of windows logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOWindowsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOWindowsTemplate template if $programname contains 'Microsoft-Windows' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOWindowsTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog sudo service rsyslog restart 3. Enjoy your events Go to the events page to watch your incoming events.","title":"Log Insight Windows"},{"location":"integrations/log_insight_windows/#overview","text":"Microsoft Windows is a popular operating system developed by Microsoft since 1985. It's available in three variants: Windows for desktop/laptop computers, tablets and smartphones Windows Server for servers Windows PE as a lightweight version.","title":"Overview"},{"location":"integrations/log_insight_windows/#setup","text":"This setup guide will show you how to forward events produced by a Windows system, collected by Log Insight agent and forward to SEKOIA.IO through your local rsyslog server.","title":"Setup"},{"location":"integrations/log_insight_windows/#configure-the-forwarder-through-rsyslog","text":"","title":"Configure the forwarder through rsyslog"},{"location":"integrations/log_insight_windows/#rsyslog-setup-on-linux-server","text":"","title":"Rsyslog setup on Linux Server"},{"location":"integrations/log_insight_windows/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/log_insight_windows/#configure-the-rsyslog-server","text":"Open or create a new windows configuration file for rsyslog: sudo vim /etc/rsyslog.d/15-windows.conf Paste the following rsyslog configuration to trigger the emission of windows logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOWindowsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOWindowsTemplate template if $programname contains 'Microsoft-Windows' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOWindowsTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/log_insight_windows/#restart-rsyslog","text":"sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/log_insight_windows/#3-enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"3. Enjoy your events"},{"location":"integrations/netfilter/","text":"Overview Netfilter is a framework provided by the Linux kernel that offers various functions and operations for packet filtering, network address translation, and port translation. The firewall ruleset might be configured by using several user-space tools such as iptables or nftables or even with high level configuration tools such as firewalld, UFW or ferm. The framework also provides several ways to log events ( LOG , NFLOG , ULOG , etc.). In this example, we only cover the iptables syntax and the basic LOG targets that sends messages to your local syslog server. The only requirement from SEKOIA.IO\u2019s perspective is that your iptables uses the following prefix: IPTables/XXX: ( XXX is user configurable and represents the action, could be Dropped for example). Setup This setup guide will show you how to forward your Linux firewall logs to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure Netfilter The first step is to configure Netfilter to log the awaited trafic. For example, if you want to only allow HTTP and HTTPS trafic and log everything else, you could use the following iptables commands: # Create a chain that logs packets and then drops them. $ iptables -N DROP_LOGGING $ iptables -A DROP_LOGGING -m limit --limit 60 /min -j LOG --log-prefix \"IPTables/Dropped: \" --log-level 4 $ iptables -A DROP_LOGGING -j DROP # Allow established communications. $ iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT # Allow HTTP communications. $ iptables -A INPUT -p tcp --dport 80 ,443 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT # Log and drop everything else. $ iptables -A INPUT -j DROP_LOGGING Configure the Rsyslog server You can configure your Rsyslog server to forward your iptables logs to SEKOIA.IO. Open or create a new Netfilter configuration file for rsyslog: sudo vim /etc/rsyslog.d/11-netfilter.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIONetfilterTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Netfilter events to SEKOIA.IO intake servers under SEKOIAIONetfilterTemplate template if $programname == \"kernel\" and $msg contains \"IPTables/\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIONetfilterTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading netfilter/iptables project homepage","title":"NetFilter"},{"location":"integrations/netfilter/#overview","text":"Netfilter is a framework provided by the Linux kernel that offers various functions and operations for packet filtering, network address translation, and port translation. The firewall ruleset might be configured by using several user-space tools such as iptables or nftables or even with high level configuration tools such as firewalld, UFW or ferm. The framework also provides several ways to log events ( LOG , NFLOG , ULOG , etc.). In this example, we only cover the iptables syntax and the basic LOG targets that sends messages to your local syslog server. The only requirement from SEKOIA.IO\u2019s perspective is that your iptables uses the following prefix: IPTables/XXX: ( XXX is user configurable and represents the action, could be Dropped for example).","title":"Overview"},{"location":"integrations/netfilter/#setup","text":"This setup guide will show you how to forward your Linux firewall logs to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/netfilter/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/netfilter/#configure-netfilter","text":"The first step is to configure Netfilter to log the awaited trafic. For example, if you want to only allow HTTP and HTTPS trafic and log everything else, you could use the following iptables commands: # Create a chain that logs packets and then drops them. $ iptables -N DROP_LOGGING $ iptables -A DROP_LOGGING -m limit --limit 60 /min -j LOG --log-prefix \"IPTables/Dropped: \" --log-level 4 $ iptables -A DROP_LOGGING -j DROP # Allow established communications. $ iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT # Allow HTTP communications. $ iptables -A INPUT -p tcp --dport 80 ,443 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT # Log and drop everything else. $ iptables -A INPUT -j DROP_LOGGING","title":"Configure Netfilter"},{"location":"integrations/netfilter/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your iptables logs to SEKOIA.IO. Open or create a new Netfilter configuration file for rsyslog: sudo vim /etc/rsyslog.d/11-netfilter.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIONetfilterTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Netfilter events to SEKOIA.IO intake servers under SEKOIAIONetfilterTemplate template if $programname == \"kernel\" and $msg contains \"IPTables/\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIONetfilterTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/netfilter/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/netfilter/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/netfilter/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/netfilter/#further-reading","text":"netfilter/iptables project homepage","title":"Further Reading"},{"location":"integrations/nginx/","text":"Overview Nginx is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server. It has a lot of features and because it is located between your application and your clients, it can give you a lot of information about either of them. Setup This setup guide will show you how to forward both your NGINX access and error logs to SEKOIA.IO by means of an rsyslog transport channel. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server We can configure rsyslog to parse the access_log and error_log and report its entries to SEKOIA.IO. Open or create a new Nginx configuration file for rsyslog: sudo vim /etc/rsyslog.d/28-nginx.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor Nginx access and error output files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # error log $InputFileName /var/log/nginx/error.log $InputFileTag nginx: $InputFileStateFile stat-nginx-error $InputFileSeverity error $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # access log $InputFileName /var/log/nginx/access.log $InputFileTag nginx: $InputFileStateFile stat-nginx-access $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIONginxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIONginxTemplate template if $programname startswith 'nginx' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIONginxTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading Nginx Wiki Rsyslog IMFile module","title":"Nginx"},{"location":"integrations/nginx/#overview","text":"Nginx is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server. It has a lot of features and because it is located between your application and your clients, it can give you a lot of information about either of them.","title":"Overview"},{"location":"integrations/nginx/#setup","text":"This setup guide will show you how to forward both your NGINX access and error logs to SEKOIA.IO by means of an rsyslog transport channel.","title":"Setup"},{"location":"integrations/nginx/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/nginx/#configure-the-rsyslog-server","text":"We can configure rsyslog to parse the access_log and error_log and report its entries to SEKOIA.IO. Open or create a new Nginx configuration file for rsyslog: sudo vim /etc/rsyslog.d/28-nginx.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor Nginx access and error output files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # error log $InputFileName /var/log/nginx/error.log $InputFileTag nginx: $InputFileStateFile stat-nginx-error $InputFileSeverity error $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # access log $InputFileName /var/log/nginx/access.log $InputFileTag nginx: $InputFileStateFile stat-nginx-access $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIONginxTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIONginxTemplate template if $programname startswith 'nginx' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIONginxTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/nginx/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/nginx/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/nginx/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/nginx/#further-reading","text":"Nginx Wiki Rsyslog IMFile module","title":"Further Reading"},{"location":"integrations/o365/","text":"Overview Office 365 is a line of subscription services offered by Microsoft as part of the Microsoft Office product line. Office365 logs Microsoft Office 365 API logs four categories of logs: Audit.AzureActiveDirectory Audit.Exchange Audit.SharePoint Audit.General Transport to SEKOIA.IO via API Sekoia has developed an automatical setup for collecting Microsoft Office 365 logs from its dedicated API. Prerequisite Have access to the Operation Center in order to create an intake Be Administrator of the MS O365 Configure the MS O365 logging in the GUI Interconnexion set-up In order to exploit the automatic interconnection method, please follow these steps: Log to the Operation Center Go to Configure > Intakes, and click on + INTAKE Choose Office 365 intake by clicking on CREATE Enter the Intake name and the related Enity, then click on Automatically Click on LOG IN TO OFFICE 365 , then ADD PERMISSION INTO OFFICE 365 Choose your Office account Optionnal mode Manual mode Prerequisites This setup guide will show you how to generate, store and forward events produced by Office 365 service to SEKOIA.IO. Theses changes have to be made from the Azure web portal ( https://portal.azure.com ). A. Event Hubs As a prerequisite you need an Event Hub (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure PowerShell (within Cloud Shell interface for example): you will create a global Event Hubs , then specific Event Hub (e.g. o365-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name o365-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > o365-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > o365-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi). B. Office 365 Office 365 has to be added through Azure portal following the Microsoft documentation Then you need to activate and configure the Office 365 diagnostic settings. Navigate to: Home > Office 365 > Monitoring > Diagnostic settings : - Add a new diagnostic setting, and select Stream to an event hub and click on configure. - Select the previously created Event hubs , Event Hub and SharedAccessKey . - Choose a name for this configuration and click on Save . Enjoy your events You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings Microsoft Stream Azure monitoring data to an event hub","title":"Microsoft Office 365"},{"location":"integrations/o365/#overview","text":"Office 365 is a line of subscription services offered by Microsoft as part of the Microsoft Office product line.","title":"Overview"},{"location":"integrations/o365/#office365-logs","text":"Microsoft Office 365 API logs four categories of logs: Audit.AzureActiveDirectory Audit.Exchange Audit.SharePoint Audit.General","title":"Office365 logs"},{"location":"integrations/o365/#transport-to-sekoiaio-via-api","text":"Sekoia has developed an automatical setup for collecting Microsoft Office 365 logs from its dedicated API.","title":"Transport to SEKOIA.IO via API"},{"location":"integrations/o365/#prerequisite","text":"Have access to the Operation Center in order to create an intake Be Administrator of the MS O365 Configure the MS O365 logging in the GUI","title":"Prerequisite"},{"location":"integrations/o365/#interconnexion-set-up","text":"In order to exploit the automatic interconnection method, please follow these steps: Log to the Operation Center Go to Configure > Intakes, and click on + INTAKE Choose Office 365 intake by clicking on CREATE Enter the Intake name and the related Enity, then click on Automatically Click on LOG IN TO OFFICE 365 , then ADD PERMISSION INTO OFFICE 365 Choose your Office account","title":"Interconnexion set-up"},{"location":"integrations/o365/#optionnal-mode","text":"","title":"Optionnal mode"},{"location":"integrations/o365/#manual-mode","text":"","title":"Manual mode"},{"location":"integrations/o365/#prerequisites","text":"This setup guide will show you how to generate, store and forward events produced by Office 365 service to SEKOIA.IO. Theses changes have to be made from the Azure web portal ( https://portal.azure.com ).","title":"Prerequisites"},{"location":"integrations/o365/#a-event-hubs","text":"As a prerequisite you need an Event Hub (e.g. company-eventhub) and to choose an existing resourceGroup or create a new one (e.g. company-resource-group). You also need your Subscription ID if you don't have a default one. Navigate to: Home > Cost Management + Billing > Subscriptions . From there, copy the relevant Subscription ID that will be used in the command line (e.g. uuid) Then you use Azure PowerShell (within Cloud Shell interface for example): you will create a global Event Hubs , then specific Event Hub (e.g. o365-event). PS Azure :\\> az eventhubs namespace create - -name company-eventhub - -resource-group company-resource-group - -enable-kafka true - -subscription uuid PS Azure :\\> az eventhubs eventhub create - -resource-group company-resource-group - -namespace-name company-eventhub - -name o365-event - -message-retention 3 - -partition-count 4 - -subscription uuid Navigate to: Home > Event Hubs > company-eventhub - Shared access policies . From there, you can create a policy (e.g. RootManageSharedAccessKey) with the claims Manage , Send and Listen , and note the Primary Key that will be used as the SharedAccessKey . Navigate to: Home > Event Hubs > company-eventhub > o365-event - Shared access policies . From there, you can create a policy (e.g. sekoiaio-nifi) with the claims Listen . Once created, click on the policy and save the Connection string-primary key , to be sent to SEKOIA.IO. Navigate to: Home > Event Hubs > company-eventhub > o365-event - Consumer groups . From there, you can create a consumer group (e.g. sekoiaio-nifi).","title":"A. Event Hubs"},{"location":"integrations/o365/#b-office-365","text":"Office 365 has to be added through Azure portal following the Microsoft documentation Then you need to activate and configure the Office 365 diagnostic settings. Navigate to: Home > Office 365 > Monitoring > Diagnostic settings : - Add a new diagnostic setting, and select Stream to an event hub and click on configure. - Select the previously created Event hubs , Event Hub and SharedAccessKey . - Choose a name for this configuration and click on Save .","title":"B. Office 365"},{"location":"integrations/o365/#enjoy-your-events","text":"You can send to Sekoia the Connection string-primary key previously mentioned. Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/o365/#further-readings","text":"Microsoft Stream Azure monitoring data to an event hub","title":"Further Readings"},{"location":"integrations/openssh/","text":"Overview OpenSSH is the premier connectivity tool for remote login with the SSH protocol. It encrypts all traffic to eliminate eavesdropping, connection hijacking, and other attacks. In addition, OpenSSH provides a large suite of secure tunneling capabilities, several authentication methods, and sophisticated configuration options. Setup This setup guide will show you how to forward logs produced by your OpenSSH servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server By default, the OpenSSH server ( sshd ) leverages the log level INFO and the system log facility AUTH . Open or create a new OpenSSH configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-openssh.conf Paste the following rsyslog configuration to trigger the emission of OpenSSH logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOOpenSSHTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOOpenSSHTemplate template if $programname startswith 'sshd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOOpenSSHTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Readings OpenSSH Manuals OpenSSH Logging and Troubleshooting","title":"OpenSSH"},{"location":"integrations/openssh/#overview","text":"OpenSSH is the premier connectivity tool for remote login with the SSH protocol. It encrypts all traffic to eliminate eavesdropping, connection hijacking, and other attacks. In addition, OpenSSH provides a large suite of secure tunneling capabilities, several authentication methods, and sophisticated configuration options.","title":"Overview"},{"location":"integrations/openssh/#setup","text":"This setup guide will show you how to forward logs produced by your OpenSSH servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/openssh/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/openssh/#configure-the-rsyslog-server","text":"By default, the OpenSSH server ( sshd ) leverages the log level INFO and the system log facility AUTH . Open or create a new OpenSSH configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-openssh.conf Paste the following rsyslog configuration to trigger the emission of OpenSSH logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOOpenSSHTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOOpenSSHTemplate template if $programname startswith 'sshd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOOpenSSHTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/openssh/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/openssh/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/openssh/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/openssh/#further-readings","text":"OpenSSH Manuals OpenSSH Logging and Troubleshooting","title":"Further Readings"},{"location":"integrations/paloalto/","text":"Overview Palo Alto Networks offers an enterprise cybersecurity platform which provides network security, cloud security, endpoint protection, and various cloud-delivered security services. Palo Alto logs On Palo Alto appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Local out traffic, Denied traffic, Allowed traffic System Security Transport to the collector Prerequisites The following prerequisites are needed in order to setup efficient log concentration: Have administrator priviledge on the Palo Alto Traffic towards the Rsyslog must be open on TCP/514 Configure Palo Alto Configure a Syslog server profile In the GUI, go to Device > Serve Profiles > Syslog . Click Add and enter a name for the profile such as Syslog server . If the firewall has more than one virtual system (vsys), select the Location (vsys or Shared ) where this profile is available. Click Add and enter the information that the firewall requires to connect to it: Name \u2014 Unique name for the server profile. Syslog Server \u2014 IP address or fully qualified domain name (FQDN) of the syslog server. Transport \u2014 Select TCP. Port \u2014 Select the default is TCP on port 514. Format \u2014 Select the syslog message format to use: IETF Facility \u2014 Select a syslog standard value (default is LOG_USER) to calculate the priority (PRI) field. Click OK to save the server profile. Configure syslog forwarding for Traffic logs Select Objects > Log Forwarding , click Add , and enter a Name to identify the profile. For each log type (here Traffic, Threat and WileFire Malicious) and each severity level, select the Syslog server profile and click OK . Select Policies > Security and select a policy rule. Select the Actions tab and select the Log Forwarding profile you created. In the Profile Type drop-down, select Profiles or Groups , and then select the security profiles or Group Profiles required to trigger log generation and forwarding. Select both of the Log at Session Start and Log At Session End check boxes, and click OK . For detailed information about configuring a log forwarding profile and assigning the profile to a policy rule, see Configure Log Forwarding Configure syslog forwarding for System and User-ID logs In the GUI, go to Device > Log Settings . Click each Severity level (High and Critical if also fine), select the Syslog server profile, and click OK . Transport to SEKOIA.IO Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the TCP incoming events are allows in the /etc/rsyslog.conf .... # provides TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) .... Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Palo Alto configuration file for Rsyslog: sudo vim /etc/rsyslog.d/38-paloalto.conf Paste the following Rsyslog configuration to trigger the emission of Palo Alto logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPaloAltoTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPaloAltoTemplate template if ( $hostname == \"YOUR_PALOALTO_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOPaloAltoTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_PALOALTO_HOSTNAME variable with the correct value. Restart Rsyslog $ sudo systemctl restart rsyslog.service Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"PaloAlto"},{"location":"integrations/paloalto/#overview","text":"Palo Alto Networks offers an enterprise cybersecurity platform which provides network security, cloud security, endpoint protection, and various cloud-delivered security services.","title":"Overview"},{"location":"integrations/paloalto/#palo-alto-logs","text":"On Palo Alto appliances, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Traffic: Local out traffic, Denied traffic, Allowed traffic System Security","title":"Palo Alto logs"},{"location":"integrations/paloalto/#transport-to-the-collector","text":"","title":"Transport to the collector"},{"location":"integrations/paloalto/#prerequisites","text":"The following prerequisites are needed in order to setup efficient log concentration: Have administrator priviledge on the Palo Alto Traffic towards the Rsyslog must be open on TCP/514","title":"Prerequisites"},{"location":"integrations/paloalto/#configure-palo-alto","text":"","title":"Configure Palo Alto"},{"location":"integrations/paloalto/#configure-a-syslog-server-profile","text":"In the GUI, go to Device > Serve Profiles > Syslog . Click Add and enter a name for the profile such as Syslog server . If the firewall has more than one virtual system (vsys), select the Location (vsys or Shared ) where this profile is available. Click Add and enter the information that the firewall requires to connect to it: Name \u2014 Unique name for the server profile. Syslog Server \u2014 IP address or fully qualified domain name (FQDN) of the syslog server. Transport \u2014 Select TCP. Port \u2014 Select the default is TCP on port 514. Format \u2014 Select the syslog message format to use: IETF Facility \u2014 Select a syslog standard value (default is LOG_USER) to calculate the priority (PRI) field. Click OK to save the server profile.","title":"Configure a Syslog server profile"},{"location":"integrations/paloalto/#configure-syslog-forwarding-for-traffic-logs","text":"Select Objects > Log Forwarding , click Add , and enter a Name to identify the profile. For each log type (here Traffic, Threat and WileFire Malicious) and each severity level, select the Syslog server profile and click OK . Select Policies > Security and select a policy rule. Select the Actions tab and select the Log Forwarding profile you created. In the Profile Type drop-down, select Profiles or Groups , and then select the security profiles or Group Profiles required to trigger log generation and forwarding. Select both of the Log at Session Start and Log At Session End check boxes, and click OK . For detailed information about configuring a log forwarding profile and assigning the profile to a policy rule, see Configure Log Forwarding","title":"Configure syslog forwarding for Traffic logs"},{"location":"integrations/paloalto/#configure-syslog-forwarding-for-system-and-user-id-logs","text":"In the GUI, go to Device > Log Settings . Click each Severity level (High and Critical if also fine), select the Syslog server profile, and click OK .","title":"Configure syslog forwarding for System and User-ID logs"},{"location":"integrations/paloalto/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/paloalto/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/paloalto/#rsyslog-prerequisites","text":"In order to allow the rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Please ensure the TCP incoming events are allows in the /etc/rsyslog.conf .... # provides TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) ....","title":"Rsyslog prerequisites"},{"location":"integrations/paloalto/#download-the-certificate","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/paloalto/#configure-the-rsyslog-server","text":"Open or create a new Palo Alto configuration file for Rsyslog: sudo vim /etc/rsyslog.d/38-paloalto.conf Paste the following Rsyslog configuration to trigger the emission of Palo Alto logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPaloAltoTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPaloAltoTemplate template if ( $hostname == \"YOUR_PALOALTO_HOSTNAME\" ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOPaloAltoTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes And change the YOUR_PALOALTO_HOSTNAME variable with the correct value.","title":"Configure the Rsyslog server"},{"location":"integrations/paloalto/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/paloalto/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/paloalto/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/postfix/","text":"Overview Postfix is a free and open-source mail transfer agent that routes and delivers electronic mail. Setup This setup guide will show you how to forward logs produced by your Postfix servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Postfix configuration file for rsyslog: sudo vim /etc/rsyslog.d/36-postfix.conf Paste the following rsyslog configuration to trigger the emission of Postfix logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPostfixTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPostfixTemplate template if $programname startswith 'postfix' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOPostfixTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Enjoy your events Go to the events page to watch your incoming events.","title":"Postfix"},{"location":"integrations/postfix/#overview","text":"Postfix is a free and open-source mail transfer agent that routes and delivers electronic mail.","title":"Overview"},{"location":"integrations/postfix/#setup","text":"This setup guide will show you how to forward logs produced by your Postfix servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/postfix/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/postfix/#configure-the-rsyslog-server","text":"Open or create a new Postfix configuration file for rsyslog: sudo vim /etc/rsyslog.d/36-postfix.conf Paste the following rsyslog configuration to trigger the emission of Postfix logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPostfixTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPostfixTemplate template if $programname startswith 'postfix' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOPostfixTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/postfix/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/postfix/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/postfix/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/prove-it/","text":"Overview PROVE IT by rubycat is a privileged access management solution. Setup This setup guide will show you how to forward logs produced by PROVE IT to SEKOIA.IO by means of an Rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls . Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the rsyslog server Open or create a new PROVE IT configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-proveit.conf Paste the following rsyslog configuration to trigger the emission of Pulse Connect Secure logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOProveITTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPulseTemplate template if $hostname == \"YOUR_PROVEIT_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOProveITTemplate In the above template instruction, change the YOUR_PROVEIT_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart Rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"PROVE IT"},{"location":"integrations/prove-it/#overview","text":"PROVE IT by rubycat is a privileged access management solution.","title":"Overview"},{"location":"integrations/prove-it/#setup","text":"This setup guide will show you how to forward logs produced by PROVE IT to SEKOIA.IO by means of an Rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls .","title":"Setup"},{"location":"integrations/prove-it/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/prove-it/#configure-the-rsyslog-server","text":"Open or create a new PROVE IT configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-proveit.conf Paste the following rsyslog configuration to trigger the emission of Pulse Connect Secure logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOProveITTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPulseTemplate template if $hostname == \"YOUR_PROVEIT_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOProveITTemplate In the above template instruction, change the YOUR_PROVEIT_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the rsyslog server"},{"location":"integrations/prove-it/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart Rsyslog"},{"location":"integrations/prove-it/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/prove-it/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/pulse/","text":"Overview Pulse Connect Secure is SSL VPN solution for remote and mobile users. Setup This setup guide will show you how to forward logs produced by Pulse Connect Secure logs to SEKOIA.IO by means of an Rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls . Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the rsyslog server Open or create a new Pulse Connect Secure configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-pulse.conf Paste the following rsyslog configuration to trigger the emission of Pulse Connect Secure logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPulseTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPulseTemplate template if $hostname == \"YOUR_PULSE_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOPulseTemplate In the above template instruction, change the YOUR_PULSE_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key. Restart Rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Pulse Connect Secure"},{"location":"integrations/pulse/#overview","text":"Pulse Connect Secure is SSL VPN solution for remote and mobile users.","title":"Overview"},{"location":"integrations/pulse/#setup","text":"This setup guide will show you how to forward logs produced by Pulse Connect Secure logs to SEKOIA.IO by means of an Rsyslog transport channel. On most GNU/Linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls .","title":"Setup"},{"location":"integrations/pulse/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/pulse/#configure-the-rsyslog-server","text":"Open or create a new Pulse Connect Secure configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-pulse.conf Paste the following rsyslog configuration to trigger the emission of Pulse Connect Secure logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOPulseTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOPulseTemplate template if $hostname == \"YOUR_PULSE_HOSTNAME\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOPulseTemplate In the above template instruction, change the YOUR_PULSE_HOSTNAME variable with the correct value, and please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the rsyslog server"},{"location":"integrations/pulse/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart Rsyslog"},{"location":"integrations/pulse/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/pulse/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/sophos/","text":"Overview Sophos firewalls offer an integrated software solution that provides superior performance in an all-in-one firewall. Its hardened operating system, stateful packet inspection, content filtering (virus & surf protection), application proxies and IPsec based VPN provides powerful solutions to today's security issues. It is designed to maximise networks security without compromising its performance enabling telecommuters, branch offices, customers and suppliers to safely share critical business information. Setup This setup guide will show you how to forward your Sophos logs to SEKOIA.IO by means of an Rsyslog transport channel. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure Sophos Firewall You can configure a syslog server in Sophos Firewall by following the instructions below (Which is appropriate for an XG Firewall, please refer to your documentation in other cases). Go to System Services > Log Settings and click Add to configure a syslog server. Enter a name for the syslog server. Enter the IP Address of the syslog server. Messages from the device will be sent to the entered IP address. Enter a Port number that the device will use for communicating with the syslog server. Device will send messages using the selected port. Select the Facility from the available options. Note: Facility informs the syslog server of the log message's source. It is defined by the syslog protocol. You can configure the facility to distinguish log messages from different devices. This parameter helps you identify the device that recorded a specific log file. Select the Severity Level from the available options. Click Save to save the configuration. Configure the Rsyslog server You can configure your Rsyslog server to forward your Sophos logs to SEKOIA.IO. Open or create a new Sophos configuration file for rsyslog: sudo vim /etc/rsyslog.d/23-sophos.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSophosTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Sophos events to SEKOIA.IO intake servers under SEKOIAIOSophosTemplate template if $hostname == \\\" YOUR_SOPHOS_HOSTNAME \\\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSophosTemplate In the above template instruction, change YOUR_SOPHOS_HOSTNAME and YOUR_INTAKE_KEY with the correct values. Restart Rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events.","title":"Sophos"},{"location":"integrations/sophos/#overview","text":"Sophos firewalls offer an integrated software solution that provides superior performance in an all-in-one firewall. Its hardened operating system, stateful packet inspection, content filtering (virus & surf protection), application proxies and IPsec based VPN provides powerful solutions to today's security issues. It is designed to maximise networks security without compromising its performance enabling telecommuters, branch offices, customers and suppliers to safely share critical business information.","title":"Overview"},{"location":"integrations/sophos/#setup","text":"This setup guide will show you how to forward your Sophos logs to SEKOIA.IO by means of an Rsyslog transport channel.","title":"Setup"},{"location":"integrations/sophos/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/sophos/#configure-sophos-firewall","text":"You can configure a syslog server in Sophos Firewall by following the instructions below (Which is appropriate for an XG Firewall, please refer to your documentation in other cases). Go to System Services > Log Settings and click Add to configure a syslog server. Enter a name for the syslog server. Enter the IP Address of the syslog server. Messages from the device will be sent to the entered IP address. Enter a Port number that the device will use for communicating with the syslog server. Device will send messages using the selected port. Select the Facility from the available options. Note: Facility informs the syslog server of the log message's source. It is defined by the syslog protocol. You can configure the facility to distinguish log messages from different devices. This parameter helps you identify the device that recorded a specific log file. Select the Severity Level from the available options. Click Save to save the configuration.","title":"Configure Sophos Firewall"},{"location":"integrations/sophos/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your Sophos logs to SEKOIA.IO. Open or create a new Sophos configuration file for rsyslog: sudo vim /etc/rsyslog.d/23-sophos.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSophosTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Sophos events to SEKOIA.IO intake servers under SEKOIAIOSophosTemplate template if $hostname == \\\" YOUR_SOPHOS_HOSTNAME \\\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSophosTemplate In the above template instruction, change YOUR_SOPHOS_HOSTNAME and YOUR_INTAKE_KEY with the correct values.","title":"Configure the Rsyslog server"},{"location":"integrations/sophos/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart Rsyslog"},{"location":"integrations/sophos/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/spamassassin/","text":"Overview SpamAssassin is a computer program used for e-mail spam filtering. SpamAssassin uses a variety of spam-detection techniques, including DNS-based and fuzzy-checksum-based spam detection, Bayesian filtering, external programs, blacklists and online databases. It is released under the Apache License 2.0 and is now part of the Apache Foundation. Setup This setup guide will show you how to forward logs produced by your SpamAssassin servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new SpamAssassin configuration file for rsyslog: sudo vim /etc/rsyslog.d/34-spamassassin.conf Paste the following rsyslog configuration to trigger the emission of SpamAssassin logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSpamAssassinTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOSpamAssassinTemplate template if $programname startswith 'spamd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSpamAssassinTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) 4. Enjoy your events Go to the events page to watch your incoming events.","title":"SpamAssassin"},{"location":"integrations/spamassassin/#overview","text":"SpamAssassin is a computer program used for e-mail spam filtering. SpamAssassin uses a variety of spam-detection techniques, including DNS-based and fuzzy-checksum-based spam detection, Bayesian filtering, external programs, blacklists and online databases. It is released under the Apache License 2.0 and is now part of the Apache Foundation.","title":"Overview"},{"location":"integrations/spamassassin/#setup","text":"This setup guide will show you how to forward logs produced by your SpamAssassin servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/spamassassin/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/spamassassin/#configure-the-rsyslog-server","text":"Open or create a new SpamAssassin configuration file for rsyslog: sudo vim /etc/rsyslog.d/34-spamassassin.conf Paste the following rsyslog configuration to trigger the emission of SpamAssassin logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSpamAssassinTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOSpamAssassinTemplate template if $programname startswith 'spamd' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSpamAssassinTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/spamassassin/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/spamassassin/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/spamassassin/#4-enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"4. Enjoy your events"},{"location":"integrations/squid/","text":"Overview Squid is a caching proxy for the Web supporting HTTP, HTTPS, FTP, and more. It reduces bandwidth and improves response times by caching and reusing frequently-requested web pages. Squid has extensive access controls and makes a great server accelerator. Setup This setup guide will show you how to forward logs produced by your SQUID servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure Squid We can configure SQUID to report access logs to your syslog server. Open the squid configuration file (please note that the path to the configuration file may change depending on the OS and your configuration): sudo vim /etc/squid/squid.conf Then update the access_log directive like the following line: access_log syslog:local5.info squid Configure the Rsyslog server Open or create a new Squid configuration file for rsyslog: sudo vim /etc/rsyslog.d/17-squid.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSquidTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOSquidTemplate template if $programname contains 'squid' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSquidTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Readings Squid Wiki - Log configuration","title":"Squid"},{"location":"integrations/squid/#overview","text":"Squid is a caching proxy for the Web supporting HTTP, HTTPS, FTP, and more. It reduces bandwidth and improves response times by caching and reusing frequently-requested web pages. Squid has extensive access controls and makes a great server accelerator.","title":"Overview"},{"location":"integrations/squid/#setup","text":"This setup guide will show you how to forward logs produced by your SQUID servers to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/squid/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/squid/#configure-squid","text":"We can configure SQUID to report access logs to your syslog server. Open the squid configuration file (please note that the path to the configuration file may change depending on the OS and your configuration): sudo vim /etc/squid/squid.conf Then update the access_log directive like the following line: access_log syslog:local5.info squid","title":"Configure Squid"},{"location":"integrations/squid/#configure-the-rsyslog-server","text":"Open or create a new Squid configuration file for rsyslog: sudo vim /etc/rsyslog.d/17-squid.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSquidTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOSquidTemplate template if $programname contains 'squid' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSquidTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/squid/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/squid/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/squid/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/squid/#further-readings","text":"Squid Wiki - Log configuration","title":"Further Readings"},{"location":"integrations/suricata/","text":"Overview Suricata is a free and open source, mature, fast and robust network threat detection engine. Suricata inspects the network traffic using a powerful and extensive rules and signature language, and has powerful Lua scripting support for detection of complex threats. Setup Suricata leverages its EVE output module to report alerts, metadata, file info and protocol records in JSON. As described in the official documentation, this module can report its findings through the syslog facility. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure Suricata to forward events to rsyslog Open the Suricata configuration file (please note that the path to the configuration file may change depending on the OS and your configuration): sudo vim /etc/suricata/suricata.yaml Paste the following declaration in your suricata configuration to trigger the production of syslog entries under the local5 facility: outputs: - eve-log: enabled: yes type:syslog identity: suricata facility: local5 level: Info types: - alert - http - dns - tls Configure the Rsyslog server Given this Suricata configuration, your local rsyslog server will handle produced records. To report these to SEKOIA.IO, open or create a new suricata configuration file for rsyslog: sudo vim /etc/rsyslog.d/11-suricata.conf Then paste the following rsyslog configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSuricataTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Suricata events to SEKOIA.IO intake servers under SEKOIAIOSuricataTemplate template if $app -name == 'suricata' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSuricataTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Readings Suricata User Guide","title":"Suricata"},{"location":"integrations/suricata/#overview","text":"Suricata is a free and open source, mature, fast and robust network threat detection engine. Suricata inspects the network traffic using a powerful and extensive rules and signature language, and has powerful Lua scripting support for detection of complex threats.","title":"Overview"},{"location":"integrations/suricata/#setup","text":"Suricata leverages its EVE output module to report alerts, metadata, file info and protocol records in JSON. As described in the official documentation, this module can report its findings through the syslog facility. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/suricata/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/suricata/#configure-suricata-to-forward-events-to-rsyslog","text":"Open the Suricata configuration file (please note that the path to the configuration file may change depending on the OS and your configuration): sudo vim /etc/suricata/suricata.yaml Paste the following declaration in your suricata configuration to trigger the production of syslog entries under the local5 facility: outputs: - eve-log: enabled: yes type:syslog identity: suricata facility: local5 level: Info types: - alert - http - dns - tls","title":"Configure Suricata to forward events to rsyslog"},{"location":"integrations/suricata/#configure-the-rsyslog-server","text":"Given this Suricata configuration, your local rsyslog server will handle produced records. To report these to SEKOIA.IO, open or create a new suricata configuration file for rsyslog: sudo vim /etc/rsyslog.d/11-suricata.conf Then paste the following rsyslog configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOSuricataTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Suricata events to SEKOIA.IO intake servers under SEKOIAIOSuricataTemplate template if $app -name == 'suricata' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOSuricataTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/suricata/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/suricata/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/suricata/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/suricata/#further-readings","text":"Suricata User Guide","title":"Further Readings"},{"location":"integrations/umbrella_dns/","text":"Overview Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers. Setup This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Collect proxylogs files and send them to rsyslog After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: dnslogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t dnslogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv Configure the Rsyslog server You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaDnslogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaDnslogsTemplate template if $syslogtag contains \"dnslogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaDnslogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings CISCO Umbrella User Guide - Logs Management","title":"Dns Logs"},{"location":"integrations/umbrella_dns/#overview","text":"Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers.","title":"Overview"},{"location":"integrations/umbrella_dns/#setup","text":"This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/umbrella_dns/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/umbrella_dns/#collect-proxylogs-files-and-send-them-to-rsyslog","text":"After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: dnslogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t dnslogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv","title":"Collect proxylogs files and send them to rsyslog"},{"location":"integrations/umbrella_dns/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaDnslogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaDnslogsTemplate template if $syslogtag contains \"dnslogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaDnslogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO.","title":"Configure the Rsyslog server"},{"location":"integrations/umbrella_dns/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/umbrella_dns/#enjoy-your-events","text":"Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/umbrella_dns/#further-readings","text":"CISCO Umbrella User Guide - Logs Management","title":"Further Readings"},{"location":"integrations/umbrella_ip/","text":"Overview Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers. Setup This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Collect proxylogs files and send them to rsyslog After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: iplogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t iplogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv Configure the Rsyslog server You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaIplogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaIplogsTemplate template if $syslogtag contains \"iplogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaIplogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings CISCO Umbrella User Guide - Logs Management","title":"Ip Logs"},{"location":"integrations/umbrella_ip/#overview","text":"Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers.","title":"Overview"},{"location":"integrations/umbrella_ip/#setup","text":"This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/umbrella_ip/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/umbrella_ip/#collect-proxylogs-files-and-send-them-to-rsyslog","text":"After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: iplogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t iplogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv","title":"Collect proxylogs files and send them to rsyslog"},{"location":"integrations/umbrella_ip/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaIplogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaIplogsTemplate template if $syslogtag contains \"iplogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaIplogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO.","title":"Configure the Rsyslog server"},{"location":"integrations/umbrella_ip/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/umbrella_ip/#enjoy-your-events","text":"Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/umbrella_ip/#further-readings","text":"CISCO Umbrella User Guide - Logs Management","title":"Further Readings"},{"location":"integrations/umbrella_proxy/","text":"Overview Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers. Setup This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Collect proxylogs files and send them to rsyslog After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: proxylogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t proxylogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv Configure the Rsyslog server You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaProxylogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaProxylogsTemplate template if $syslogtag contains \"proxylogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaProxylogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events. Further Readings CISCO Umbrella User Guide - Logs Management","title":"Proxy Logs"},{"location":"integrations/umbrella_proxy/#overview","text":"Cisco Umbrella offers flexible, cloud-delivered security. It combines multiple security functions into one solution, so that protection can be extended to devices, remote users, and distributed locations anywhere. CISCO Umbrella is a leading provider of network security and recursive DNS services. With the intelligent proxy, if a site is considered potentially suspicious or could host malicious content, Umbrella returns the intelligent proxy's IP address. The request to that domain is then routed through their cloud-based secure gateway, and malicious content is found and stopped before it's sent to their customers.","title":"Overview"},{"location":"integrations/umbrella_proxy/#setup","text":"This setup guide will show you how to forward logs produced by CISCO Umbrella service to SEKOIA.IO by means of an Rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/umbrella_proxy/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/umbrella_proxy/#collect-proxylogs-files-and-send-them-to-rsyslog","text":"After configuring Umbrella Log Management with AWS S3, the logs you download will be gzipped CSVs in appropriate subfolder with the following naming format: proxylogs/<year>-<month>-<day>/<year>-<month>-<day>-<hour>-<minute>.csv.gz To send these logs to SEKOIA.IO, we suggest the use of the logger Unix command. For each unzipped file, use the following command line: logger -t proxylogs -f <YYYY>-<MM>-<DD>-<hh>-<mm>-<xxxx>.csv","title":"Collect proxylogs files and send them to rsyslog"},{"location":"integrations/umbrella_proxy/#configure-the-rsyslog-server","text":"You can configure your Rsyslog server to forward your fortigate logs to SEKOIA.IO. Open or create a new Umbrella configuration file for rsyslog: sudo vim /etc/rsyslog.d/6-umbrella.conf Then paste the following configuration: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUmbrellaProxylogsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your Umbrella events to SEKOIA.IO intake servers under SEKOIAIOUmbrellaProxylogsTemplate template if $syslogtag contains \"proxylogs\" then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUmbrellaProxylogsTemplate In the above template instruction, replace YOUR_INTAKE_KEY variable with the intake key available in the Intakes menu of SEKOIA.IO.","title":"Configure the Rsyslog server"},{"location":"integrations/umbrella_proxy/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/umbrella_proxy/#enjoy-your-events","text":"Once the configuration has been done on Sekoia side, you can go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/umbrella_proxy/#further-readings","text":"CISCO Umbrella User Guide - Logs Management","title":"Further Readings"},{"location":"integrations/unbound/","text":"Overview Unbound is a validating, recursive, and caching DNS resolver product from NLnet Labs. It is distributed free of charge in open-source form under the BSD license. Setup This setup guide will show you how to forward logs produced by your Unbound server to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new Unbound configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-unbound.conf Paste the following rsyslog configuration to trigger the emission of Unbound logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUnboundTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOUnboundTemplate template if $programname startswith 'unbound' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUnboundTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Unbound"},{"location":"integrations/unbound/#overview","text":"Unbound is a validating, recursive, and caching DNS resolver product from NLnet Labs. It is distributed free of charge in open-source form under the BSD license.","title":"Overview"},{"location":"integrations/unbound/#setup","text":"This setup guide will show you how to forward logs produced by your Unbound server to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/unbound/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/unbound/#configure-the-rsyslog-server","text":"Open or create a new Unbound configuration file for rsyslog: sudo vim /etc/rsyslog.d/46-unbound.conf Paste the following rsyslog configuration to trigger the emission of Unbound logs by your rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOUnboundTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOUnboundTemplate template if $programname startswith 'unbound' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOUnboundTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/unbound/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/unbound/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/unbound/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/windows/","text":"Overview Microsoft Windows is a popular operating system developed by Microsoft since 1985. In this documenation we will explain 2 ways to collect and send Windows logs to SEKOIA.IO. From the Windows machine directly to SEKOIA.IO using the NXLog agent From the Windows machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO In addition, this documentation will explain how to collect and send Windows logs from sensitive assets such as Domain Controllers, if you don't want to install a third party agent. Windows Event logs On Microsoft Windows workstations and servers, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Application: for Windows components such as drivers and built-in interface elements System: records the events related to programs installed on a system Security: records the events related to security, such as logon attempts and ressource access Those logs a readable locally in the Windows Event Viewer, in the section Windows Logs. If you want to improve detection and analysis, you may want to enable Sysmon. Warning: The installation of this tool will generate more logs, so it will consume more CPU ressources. Install it on equipements that are correctly dimensioned, or try it on low risk assets at first. Sysmon is a Microsoft tool you can download on their website . A common installation instruction and configuration file is available on SwiftOnSecurity's Github . You will find dedicated NXLog configuration file for Sysmon usage at the end of this page. This setup guide will show you how to forward events produced by a Windows system either directly to SEKOIA.IO, or through your local rsyslog server. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Transport to the concentrator NXLog agent to the Rsyslog NXLog setup on Windows This section describes how to configure NXLog to forward your Windows events by means of a syslog transport channel. First of all, download NXLog at the following link : https://nxlog.co/products/all/download. Then, open the NXLog configuration file at C:\\Program Files (x86)\\nxlog\\conf\\nxlog.conf and update it with the following instructions: ## This is a sample configuration file. See the nxlog reference manual about the ## configuration options. It should be installed locally and is also available ## online at http://nxlog.org/nxlog-docs/en/nxlog-reference-manual.html ## Please set the ROOT to the folder your nxlog was installed into, ## otherwise it will not start. #define ROOT C:\\Program Files\\nxlog define ROOT C:\\Program Files (x86)\\nxlog define CERTDIR %ROOT%\\cert Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog> # Use 'im_mseventlog' for Windows XP, 2000 and 2003 Module im_msvistalog Exec $Message = to_json(); </Input> <Output rsyslog> Module om_tcp Host RSYSLOG_HOST Port 514 OutputType Syslog_TLS Exec to_syslog_ietf(); </Output> <Route eventlog_to_rsyslog> Path eventlog => rsyslog </Route> In the above configuration make sure to replace RSYSLOG_HOST variable by your Rsyslog server IP. Restart the NXLog service through the Services tool as Administrator or use Powershell command line: Restart-Service nxlog Windows Event Forwarder to Windows Event Collector to Rsyslog Most of the following commands are to be run as Administrator in a Powershell interpretor. Windows Event Collector (WEC) setup The Windows Event Collector, also known as WEC is a Microsoft service that can be activate and configured to aggregate logs from the Windows Event Forwarders (WEF). Get the WEC FQDN Log in the Windows Event Collector, and execute the following command: ( [System.Net.Dns] :: GetHostByName (( $env:computerName ))). Hostname Note it, you will need it on the section WEF: \"Deploying the GPO\" for FQDN_WEC_SERVER field to replace Configure the subscription file Get the SDDL information by executing the following command: wevtutil gl security Note the channelAccess information. On the WEC server, create an XML file, named DC_SUBSCRIPTION.xml with the following information: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Subscription xmlns= \"http://schemas.microsoft.com/2006/03/windows/events/subscription\" > <!-- Name of subscription --> <SubscriptionId> DC_SUBSCRIPTION </SubscriptionId> <!-- Push mode (DC to WEC) --> <SubscriptionType> SourceInitiated </SubscriptionType> <Description> Source Initiated Subscription from DC_SUBSCRIPTION </Description> <!-- Subscription is active --> <Enabled> true </Enabled> <Uri> http://schemas.microsoft.com/wbem/wsman/1/windows/EventLog </Uri> <!-- This mode ensures that events are delivered with minimal delay --> <!-- It is an appropriate choice if you are collecting alerts or critical events --> <!-- It uses push delivery mode and sets a batch timeout of 30 seconds --> <ConfigurationMode> MinLatency </ConfigurationMode> <!-- Event log to retrieved --> <Query> <![CDATA[ <QueryList> <Query Id=\"0\"> <Select Path=\"Application\">*</Select> <Select Path=\"Security\">*</Select> <Select Path=\"System\">*</Select> </Query> </QueryList> ]]> </Query> <!-- Collect events generated since the subscription (not oldest) --> <ReadExistingEvents> false </ReadExistingEvents> <!-- Protocol and port used (DC to WEC) --> <TransportName> http </TransportName> <!-- Mandatory value (https://www-01.ibm.com/support/docview.wss?crawler=1&uid=swg1IV71375) --> <ContentFormat> RenderedText </ContentFormat> <Locale Language= \"en-US\" /> <!-- Target Event log on WEC --> <LogFile> ForwardedEvents </LogFile> <!-- Define which domain computers are allowed or not to initiate subscriptions --> <!-- This exemple grants members of the Domain Computers domain group, as well as the local Network Service group (for local forwarder) --> <AllowedSourceDomainComputers> O:NSG:NSD:(A;;GA;;;DC)(A;;GA;;;NS) </AllowedSourceDomainComputers> </Subscription> Warning: replace Domain Computers domain group \"(A;;GA;;;DC)\" by \"(A;;GA;;;S-1-5-....)\" previously collected in the channelAccess More information of the SDDL format (https://msdn.microsoft.com/en-us/library/windows/desktop/aa379567(v=vs.85).aspx) Ensure the file is correctly saved, then close it. Configure the Windows Remote Management On the WEC server, open a command interpretor as Administrator, and run the following command: winrm qc -q Activate the \"Event Collector\" service As Administrator, enter the following command: wecutil qc / q Activate the subscription to a zone As Administrator, enter the following command: Warning: please change the character FILE_PATH wecutil cs \"<FILE_PATH>\\DC_SUBSCRIPTION.xml\" Display the state of the subscription to this zone As Administrator, enter the following command: wecutil gr DC_SUBSCRIPTION Windows Event Forwarder (WEF) setup The Windows Event Forwarder, also known as WEF is a Microsoft service that can be activated for log forwarding towards a Windows Event Collector (WEC). Configure the Event Log Reader Configure the collector URI(s). Start the WinRM service (run as Administrator) Add the Network Service account to the built-in Event Log Readers security group. As show hereafter: Configure the Windows Remote Management On the AD admin console, open a command interpretor as Administrator, and run the following command: winrm qc -q Deploying the GPO In Computer Configuration > Administrative Templates > Windows Components > Event Forwarding -> Configure target Subscription Manager, execute the following command: Warning: Please replace the FQDN_WEC_SERVER by the previously collected information Server=http://FQDN_WEC_SERVER:5985/wsman/SubscriptionManager/WEC,Refresh=60 Apply the GPO On the AD admin console, open a command interpretor as Administrator, and run the following command: gpupdate / force More documentation available here on Microsoft documentation Windows Event Collector to Rsyslog Ensure the logs are correctly reveived on the WEC server, using the Windows Event Viewer and selecting Windows Logs > Forwarded Events Setup the Nxlog agent to forward the logs to the RSYSLOG by using this configuration : define ROOT C:\\Program Files (x86)\\nxlog Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog1> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Application\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog2> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"System\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog3> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Security\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog4> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"ForwardedEvents\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Output rsyslog> Module om_tcp Host RSYSLOG_HOST Port 514 OutputType Syslog_TLS Exec to_syslog_ietf(); </Output> <Route eventlog_to_rsyslog> Path eventlog1, eventlog2, eventlog3, eventlog4 => rsyslog </Route> In the above configuration make sure to replace RSYSLOG_HOST variable by your Rsyslog server IP. Restart the NXLog service through the Services tool as Administrator or use Powershell command line: Restart-Service nxlog Transport to SEKOIA.IO Configure the forwarder the direct way Download the certificate In order to allow the connection of your events forwarder to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate. In a PowerShell console run as administrator, retrieve the certificate with the following command: Invoke-WebRequest -Uri https :// app . sekoia . io / assets / files / SEKOIA-IO-intake . pem -OutFile 'C:\\Program Files (x86)\\nxlog\\cert\\SEKOIA-IO-intake.pem' NXLog setup on Windows This section describes how to configure NXLog to forward your Windows events by means of a syslog transport channel. First of all, download NXLog at the following link : https://nxlog.co/products/all/download. Then, open the NXLog configuration file at C:\\Program Files (x86)\\nxlog\\conf\\nxlog.conf and update it with the following instructions: In the this template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. ## This is a sample configuration file. See the nxlog reference manual about the ## configuration options. It should be installed locally and is also available ## online at http://nxlog.org/nxlog-docs/en/nxlog-reference-manual.html ## Please set the ROOT to the folder your nxlog was installed into, ## otherwise it will not start. #define ROOT C:\\Program Files\\nxlog define ROOT C:\\Program Files (x86)\\nxlog define CERTDIR %ROOT%\\cert Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog> # Use 'im_mseventlog' for Windows XP, 2000 and 2003 Module im_msvistalog Exec $Message = to_json(); </Input> <Output sekoia_intake> Module om_ssl Host intake.sekoia.io Port 10514 CAFile %CERTDIR%\\SEKOIA-IO-intake.pem AllowUntrusted FALSE Exec to_syslog_ietf(); Exec $raw_event = replace($raw_event, '[NXLOG@', '[SEKOIA@53288 intake_key=\"YOUR_INTAKE_KEY\"][NXLOG@', 1); OutputType Syslog_TLS </Output> <Route eventlog_to_sekoia_intake> Path eventlog => sekoia_intake </Route> Restart the NXLog service through the Services tool as Administrator or use PowerShell command line: Restart-Service nxlog Configure the Rsyslog to forward to SEKOIA.IO Rsyslog prerequisites In order to allow the Rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget Download the certificate In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server Open or create a new configuration file for Rsyslog: sudo vim /etc/rsyslog.d/15-windows.conf Paste the following Rsyslog configuration to trigger the emission of Windows logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOWindowsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOWindowsTemplate template if ( $syslogtag contains 'Microsoft-Windows' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOWindowsTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes Restart Rsyslog $ sudo systemctl restart rsyslog.service Sysmon usage NXLog setup with Sysmon If your are using the NXLog community edition, there's a limitation in terms of number of different eventlog entries that could be processed. In order to monitor the common ones (Application, System, Security) and Sysmon, you could make the following changes: ... <Input eventlog1> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Application\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog2> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"System\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog3> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Security\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog4> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Microsoft-Windows-Sysmon/Operational\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> ... <Route eventlog_to_rsyslog> Path eventlog1, eventlog2, eventlog3, eventlog4 => rsyslog </Route> ... Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Readings NXLog Community Edition Reference Manual","title":"Windows"},{"location":"integrations/windows/#overview","text":"Microsoft Windows is a popular operating system developed by Microsoft since 1985. In this documenation we will explain 2 ways to collect and send Windows logs to SEKOIA.IO. From the Windows machine directly to SEKOIA.IO using the NXLog agent From the Windows machine to an internal log concentrator (Rsyslog), then forwarded to SEKOIA.IO In addition, this documentation will explain how to collect and send Windows logs from sensitive assets such as Domain Controllers, if you don't want to install a third party agent.","title":"Overview"},{"location":"integrations/windows/#windows-event-logs","text":"On Microsoft Windows workstations and servers, most of the important hardward and software activities that are relevant for security detection and analysis, are logged into three files. Application: for Windows components such as drivers and built-in interface elements System: records the events related to programs installed on a system Security: records the events related to security, such as logon attempts and ressource access Those logs a readable locally in the Windows Event Viewer, in the section Windows Logs. If you want to improve detection and analysis, you may want to enable Sysmon. Warning: The installation of this tool will generate more logs, so it will consume more CPU ressources. Install it on equipements that are correctly dimensioned, or try it on low risk assets at first. Sysmon is a Microsoft tool you can download on their website . A common installation instruction and configuration file is available on SwiftOnSecurity's Github . You will find dedicated NXLog configuration file for Sysmon usage at the end of this page. This setup guide will show you how to forward events produced by a Windows system either directly to SEKOIA.IO, or through your local rsyslog server. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Windows Event logs"},{"location":"integrations/windows/#transport-to-the-concentrator","text":"","title":"Transport to the concentrator"},{"location":"integrations/windows/#nxlog-agent-to-the-rsyslog","text":"","title":"NXLog agent to the Rsyslog"},{"location":"integrations/windows/#nxlog-setup-on-windows","text":"This section describes how to configure NXLog to forward your Windows events by means of a syslog transport channel. First of all, download NXLog at the following link : https://nxlog.co/products/all/download. Then, open the NXLog configuration file at C:\\Program Files (x86)\\nxlog\\conf\\nxlog.conf and update it with the following instructions: ## This is a sample configuration file. See the nxlog reference manual about the ## configuration options. It should be installed locally and is also available ## online at http://nxlog.org/nxlog-docs/en/nxlog-reference-manual.html ## Please set the ROOT to the folder your nxlog was installed into, ## otherwise it will not start. #define ROOT C:\\Program Files\\nxlog define ROOT C:\\Program Files (x86)\\nxlog define CERTDIR %ROOT%\\cert Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog> # Use 'im_mseventlog' for Windows XP, 2000 and 2003 Module im_msvistalog Exec $Message = to_json(); </Input> <Output rsyslog> Module om_tcp Host RSYSLOG_HOST Port 514 OutputType Syslog_TLS Exec to_syslog_ietf(); </Output> <Route eventlog_to_rsyslog> Path eventlog => rsyslog </Route> In the above configuration make sure to replace RSYSLOG_HOST variable by your Rsyslog server IP. Restart the NXLog service through the Services tool as Administrator or use Powershell command line: Restart-Service nxlog","title":"NXLog setup on Windows"},{"location":"integrations/windows/#windows-event-forwarder-to-windows-event-collector-to-rsyslog","text":"Most of the following commands are to be run as Administrator in a Powershell interpretor.","title":"Windows Event Forwarder to Windows Event Collector to Rsyslog"},{"location":"integrations/windows/#windows-event-collector-wec-setup","text":"The Windows Event Collector, also known as WEC is a Microsoft service that can be activate and configured to aggregate logs from the Windows Event Forwarders (WEF).","title":"Windows Event Collector (WEC) setup"},{"location":"integrations/windows/#get-the-wec-fqdn","text":"Log in the Windows Event Collector, and execute the following command: ( [System.Net.Dns] :: GetHostByName (( $env:computerName ))). Hostname Note it, you will need it on the section WEF: \"Deploying the GPO\" for FQDN_WEC_SERVER field to replace","title":"Get the WEC FQDN"},{"location":"integrations/windows/#configure-the-subscription-file","text":"Get the SDDL information by executing the following command: wevtutil gl security Note the channelAccess information. On the WEC server, create an XML file, named DC_SUBSCRIPTION.xml with the following information: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Subscription xmlns= \"http://schemas.microsoft.com/2006/03/windows/events/subscription\" > <!-- Name of subscription --> <SubscriptionId> DC_SUBSCRIPTION </SubscriptionId> <!-- Push mode (DC to WEC) --> <SubscriptionType> SourceInitiated </SubscriptionType> <Description> Source Initiated Subscription from DC_SUBSCRIPTION </Description> <!-- Subscription is active --> <Enabled> true </Enabled> <Uri> http://schemas.microsoft.com/wbem/wsman/1/windows/EventLog </Uri> <!-- This mode ensures that events are delivered with minimal delay --> <!-- It is an appropriate choice if you are collecting alerts or critical events --> <!-- It uses push delivery mode and sets a batch timeout of 30 seconds --> <ConfigurationMode> MinLatency </ConfigurationMode> <!-- Event log to retrieved --> <Query> <![CDATA[ <QueryList> <Query Id=\"0\"> <Select Path=\"Application\">*</Select> <Select Path=\"Security\">*</Select> <Select Path=\"System\">*</Select> </Query> </QueryList> ]]> </Query> <!-- Collect events generated since the subscription (not oldest) --> <ReadExistingEvents> false </ReadExistingEvents> <!-- Protocol and port used (DC to WEC) --> <TransportName> http </TransportName> <!-- Mandatory value (https://www-01.ibm.com/support/docview.wss?crawler=1&uid=swg1IV71375) --> <ContentFormat> RenderedText </ContentFormat> <Locale Language= \"en-US\" /> <!-- Target Event log on WEC --> <LogFile> ForwardedEvents </LogFile> <!-- Define which domain computers are allowed or not to initiate subscriptions --> <!-- This exemple grants members of the Domain Computers domain group, as well as the local Network Service group (for local forwarder) --> <AllowedSourceDomainComputers> O:NSG:NSD:(A;;GA;;;DC)(A;;GA;;;NS) </AllowedSourceDomainComputers> </Subscription> Warning: replace Domain Computers domain group \"(A;;GA;;;DC)\" by \"(A;;GA;;;S-1-5-....)\" previously collected in the channelAccess More information of the SDDL format (https://msdn.microsoft.com/en-us/library/windows/desktop/aa379567(v=vs.85).aspx) Ensure the file is correctly saved, then close it.","title":"Configure the subscription file"},{"location":"integrations/windows/#configure-the-windows-remote-management","text":"On the WEC server, open a command interpretor as Administrator, and run the following command: winrm qc -q","title":"Configure the Windows Remote Management"},{"location":"integrations/windows/#activate-the-event-collector-service","text":"As Administrator, enter the following command: wecutil qc / q","title":"Activate the \"Event Collector\" service"},{"location":"integrations/windows/#activate-the-subscription-to-a-zone","text":"As Administrator, enter the following command: Warning: please change the character FILE_PATH wecutil cs \"<FILE_PATH>\\DC_SUBSCRIPTION.xml\"","title":"Activate the subscription to a zone"},{"location":"integrations/windows/#display-the-state-of-the-subscription-to-this-zone","text":"As Administrator, enter the following command: wecutil gr DC_SUBSCRIPTION","title":"Display the state of the subscription to this zone"},{"location":"integrations/windows/#windows-event-forwarder-wef-setup","text":"The Windows Event Forwarder, also known as WEF is a Microsoft service that can be activated for log forwarding towards a Windows Event Collector (WEC).","title":"Windows Event Forwarder (WEF) setup"},{"location":"integrations/windows/#configure-the-event-log-reader","text":"Configure the collector URI(s). Start the WinRM service (run as Administrator) Add the Network Service account to the built-in Event Log Readers security group. As show hereafter:","title":"Configure the Event Log Reader"},{"location":"integrations/windows/#configure-the-windows-remote-management_1","text":"On the AD admin console, open a command interpretor as Administrator, and run the following command: winrm qc -q","title":"Configure the Windows Remote Management"},{"location":"integrations/windows/#deploying-the-gpo","text":"In Computer Configuration > Administrative Templates > Windows Components > Event Forwarding -> Configure target Subscription Manager, execute the following command: Warning: Please replace the FQDN_WEC_SERVER by the previously collected information Server=http://FQDN_WEC_SERVER:5985/wsman/SubscriptionManager/WEC,Refresh=60","title":"Deploying the GPO"},{"location":"integrations/windows/#apply-the-gpo","text":"On the AD admin console, open a command interpretor as Administrator, and run the following command: gpupdate / force More documentation available here on Microsoft documentation","title":"Apply the GPO"},{"location":"integrations/windows/#windows-event-collector-to-rsyslog","text":"Ensure the logs are correctly reveived on the WEC server, using the Windows Event Viewer and selecting Windows Logs > Forwarded Events Setup the Nxlog agent to forward the logs to the RSYSLOG by using this configuration : define ROOT C:\\Program Files (x86)\\nxlog Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog1> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Application\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog2> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"System\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog3> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Security\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog4> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"ForwardedEvents\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Output rsyslog> Module om_tcp Host RSYSLOG_HOST Port 514 OutputType Syslog_TLS Exec to_syslog_ietf(); </Output> <Route eventlog_to_rsyslog> Path eventlog1, eventlog2, eventlog3, eventlog4 => rsyslog </Route> In the above configuration make sure to replace RSYSLOG_HOST variable by your Rsyslog server IP. Restart the NXLog service through the Services tool as Administrator or use Powershell command line: Restart-Service nxlog","title":"Windows Event Collector to Rsyslog"},{"location":"integrations/windows/#transport-to-sekoiaio","text":"","title":"Transport to SEKOIA.IO"},{"location":"integrations/windows/#configure-the-forwarder-the-direct-way","text":"","title":"Configure the forwarder the direct way"},{"location":"integrations/windows/#download-the-certificate","text":"In order to allow the connection of your events forwarder to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate. In a PowerShell console run as administrator, retrieve the certificate with the following command: Invoke-WebRequest -Uri https :// app . sekoia . io / assets / files / SEKOIA-IO-intake . pem -OutFile 'C:\\Program Files (x86)\\nxlog\\cert\\SEKOIA-IO-intake.pem'","title":"Download the certificate"},{"location":"integrations/windows/#nxlog-setup-on-windows_1","text":"This section describes how to configure NXLog to forward your Windows events by means of a syslog transport channel. First of all, download NXLog at the following link : https://nxlog.co/products/all/download. Then, open the NXLog configuration file at C:\\Program Files (x86)\\nxlog\\conf\\nxlog.conf and update it with the following instructions: In the this template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. ## This is a sample configuration file. See the nxlog reference manual about the ## configuration options. It should be installed locally and is also available ## online at http://nxlog.org/nxlog-docs/en/nxlog-reference-manual.html ## Please set the ROOT to the folder your nxlog was installed into, ## otherwise it will not start. #define ROOT C:\\Program Files\\nxlog define ROOT C:\\Program Files (x86)\\nxlog define CERTDIR %ROOT%\\cert Moduledir %ROOT%\\modules CacheDir %ROOT%\\data Pidfile %ROOT%\\data\\nxlog.pid SpoolDir %ROOT%\\data LogFile %ROOT%\\data\\nxlog.log <Extension _syslog> Module xm_syslog </Extension> <Extension _json> Module xm_json </Extension> <Input eventlog> # Use 'im_mseventlog' for Windows XP, 2000 and 2003 Module im_msvistalog Exec $Message = to_json(); </Input> <Output sekoia_intake> Module om_ssl Host intake.sekoia.io Port 10514 CAFile %CERTDIR%\\SEKOIA-IO-intake.pem AllowUntrusted FALSE Exec to_syslog_ietf(); Exec $raw_event = replace($raw_event, '[NXLOG@', '[SEKOIA@53288 intake_key=\"YOUR_INTAKE_KEY\"][NXLOG@', 1); OutputType Syslog_TLS </Output> <Route eventlog_to_sekoia_intake> Path eventlog => sekoia_intake </Route> Restart the NXLog service through the Services tool as Administrator or use PowerShell command line: Restart-Service nxlog","title":"NXLog setup on Windows"},{"location":"integrations/windows/#configure-the-rsyslog-to-forward-to-sekoiaio","text":"","title":"Configure the Rsyslog to forward to SEKOIA.IO"},{"location":"integrations/windows/#rsyslog-prerequisites","text":"In order to allow the Rsyslog to work properly, please ensure the following packages are installed: sudo apt install rsyslog rsyslog-gnutls wget","title":"Rsyslog prerequisites"},{"location":"integrations/windows/#download-the-certificate_1","text":"In order to allow the connection of your Rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/windows/#configure-the-rsyslog-server","text":"Open or create a new configuration file for Rsyslog: sudo vim /etc/rsyslog.d/15-windows.conf Paste the following Rsyslog configuration to trigger the emission of Windows logs by your Rsyslog server to SEKOIA.IO: # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOWindowsTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOWindowsTemplate template if ( $syslogtag contains 'Microsoft-Windows' ) then { action ( type = \"omfwd\" protocol = \"tcp\" target = \"intake.sekoia.io\" port = \"10514\" TCP_Framing = \"octet-counted\" StreamDriver = \"gtls\" StreamDriverMode = \"1\" StreamDriverAuthMode = \"x509/name\" StreamDriverPermittedPeers = \"intake.sekoia.io\" Template = \"SEKOIAIOWindowsTemplate\" ) } In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key you can find in the Operation Center > Configure > Intakes","title":"Configure the Rsyslog server"},{"location":"integrations/windows/#restart-rsyslog","text":"$ sudo systemctl restart rsyslog.service","title":"Restart Rsyslog"},{"location":"integrations/windows/#sysmon-usage","text":"","title":"Sysmon usage"},{"location":"integrations/windows/#nxlog-setup-with-sysmon","text":"If your are using the NXLog community edition, there's a limitation in terms of number of different eventlog entries that could be processed. In order to monitor the common ones (Application, System, Security) and Sysmon, you could make the following changes: ... <Input eventlog1> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Application\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog2> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"System\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog3> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Security\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> <Input eventlog4> Module im_msvistalog Query <QueryList><Query Id=\"0\"><Select Path=\"Microsoft-Windows-Sysmon/Operational\">*</Select></Query></QueryList> Exec $Message = to_json(); </Input> ... <Route eventlog_to_rsyslog> Path eventlog1, eventlog2, eventlog3, eventlog4 => rsyslog </Route> ...","title":"NXLog setup with Sysmon"},{"location":"integrations/windows/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/windows/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/windows/#further-readings","text":"NXLog Community Edition Reference Manual","title":"Further Readings"},{"location":"integrations/zeek/","text":"Overview Zeek is a free and open-source software network analysis framework; it was originally developed in 1994 by Vern Paxson and was named in reference to George Orwell's Big Brother from his novel Nineteen Eighty-Four. It can be used as a network intrusion detection system (NIDS) but with additional live analysis of network events.It is released under the BSD license. Setup This setup guide will show you how to forward dns, http and conn logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls. Download the certificate In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem Configure the Rsyslog server We can configure rsyslog to parse the conn.log http.log and dns.log and report its entries to SEKOIA.IO. Open or create a new Zeek configuration file for rsyslog: sudo vim /etc/rsyslog.d/66-zeek.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor zeek log files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # conn log $InputFileName /var/log/zeek/conn.log $InputFileTag zeek: $InputFileStateFile stat-zeek-conn $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # http log $InputFileName /var/log/zeek/http.log $InputFileTag zeek: $InputFileStateFile stat-zeek-http $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # dns log $InputFileName /var/log/zeek/dns.log $InputFileTag zeek: $InputFileStateFile stat-zeek-dns $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOZeekTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOZeekTemplate template if $programname startswith 'zeek' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOZeekTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key. Restart rsyslog $ sudo service rsyslog restart Enjoy your events Go to the events page to watch your incoming events. Related files SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b) Further Reading Zeek documentation Rsyslog IMFile module","title":"Zeek"},{"location":"integrations/zeek/#overview","text":"Zeek is a free and open-source software network analysis framework; it was originally developed in 1994 by Vern Paxson and was named in reference to George Orwell's Big Brother from his novel Nineteen Eighty-Four. It can be used as a network intrusion detection system (NIDS) but with additional live analysis of network events.It is released under the BSD license.","title":"Overview"},{"location":"integrations/zeek/#setup","text":"This setup guide will show you how to forward dns, http and conn logs to SEKOIA.IO by means of an rsyslog transport channel. On most linux servers, two packages need to be installed: rsyslog and rsyslog-gnutls.","title":"Setup"},{"location":"integrations/zeek/#download-the-certificate","text":"In order to allow the connection of your rsyslog server to the SEKOIA.IO intake, please download the SEKOIA.IO intake certificate: $ wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem","title":"Download the certificate"},{"location":"integrations/zeek/#configure-the-rsyslog-server","text":"We can configure rsyslog to parse the conn.log http.log and dns.log and report its entries to SEKOIA.IO. Open or create a new Zeek configuration file for rsyslog: sudo vim /etc/rsyslog.d/66-zeek.conf At the beginning of the configuration file, paste the following instruction to order the rsyslog server to load the module imfile : $ModLoad imfile Then paste the following configuration to leverage this module to monitor zeek log files (please note that the path to the log file may change depending on the OS and your configuration): # Define the SEKIOA-IO intake certificate $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem # Configure up the network ssl connection $ActionSendStreamDriver gtls # use gtls netstream driver $ActionSendStreamDriverMode 1 # require TLS for the connection $ActionSendStreamDriverAuthMode x509/name # server is authenticated # conn log $InputFileName /var/log/zeek/conn.log $InputFileTag zeek: $InputFileStateFile stat-zeek-conn $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # http log $InputFileName /var/log/zeek/http.log $InputFileTag zeek: $InputFileStateFile stat-zeek-http $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # dns log $InputFileName /var/log/zeek/dns.log $InputFileTag zeek: $InputFileStateFile stat-zeek-dns $InputFileSeverity notice $InputFileFacility local5 $InputFilePollInterval 1 $InputRunFileMonitor # Template definition [RFC5424](https://tools.ietf.org/html/rfc5424#section-7.2.2) # IMPORTANT: don't forget to set your intake key in the template template ( name = \"SEKOIAIOZeekTemplate\" type = \"string\" string = \"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\" ) # Send your events to SEKOIA.IO intake servers under SEKOIAIOZeekTemplate template if $programname startswith 'zeek' then @@ ( o ) intake.sekoia.io:10514 ; SEKOIAIOZeekTemplate In the above template instruction, please replace YOUR_INTAKE_KEY variable with your intake key.","title":"Configure the Rsyslog server"},{"location":"integrations/zeek/#restart-rsyslog","text":"$ sudo service rsyslog restart","title":"Restart rsyslog"},{"location":"integrations/zeek/#enjoy-your-events","text":"Go to the events page to watch your incoming events.","title":"Enjoy your events"},{"location":"integrations/zeek/#related-files","text":"SEKOIA-IO-intake.pem : SEKOIA.IO TLS Server Certificate (1674b)","title":"Related files"},{"location":"integrations/zeek/#further-reading","text":"Zeek documentation Rsyslog IMFile module","title":"Further Reading"},{"location":"integrations/transport/","text":"SEKOIA.IO Log Collector Transports SEKOIA.IO supports multiple log collectors: Syslog over TLS ( intake.sekoia.io:10514 ): you can forward your events by using the Syslog protocol specified in RFC 5424. RELP over TLS ( intake.sekoia.io:11514 ): your can forward your events by using Rsyslog\u2019s reliable protocol called RELP. HTTPS ( https://intake.sekoia.io ): you can POST your JSON events. This makes SEKOIA.IO compatible with a large number of product and solutions to collect your logs. Even if Rsyslog is the most supported solution to push your logs to SEKOIA.IO, you can use other solutions, such as Logstash, syslog-ng, Graylog, etc. Syslog integration We are providing documentation and example configurations on how to configure your log system for Rsyslog and syslog-ng , but it should be easy to configure other log collectors to forward their events to SEKOIA.IO. HTTPS integration To push your events through our HTTPS log collector, you have to POST your logs in the JSON format. More information and code example are available on this page . Log Aggregator Configuration Rsyslog Logstash syslog-ng Graylog","title":"Overview"},{"location":"integrations/transport/#sekoiaio-log-collector-transports","text":"SEKOIA.IO supports multiple log collectors: Syslog over TLS ( intake.sekoia.io:10514 ): you can forward your events by using the Syslog protocol specified in RFC 5424. RELP over TLS ( intake.sekoia.io:11514 ): your can forward your events by using Rsyslog\u2019s reliable protocol called RELP. HTTPS ( https://intake.sekoia.io ): you can POST your JSON events. This makes SEKOIA.IO compatible with a large number of product and solutions to collect your logs. Even if Rsyslog is the most supported solution to push your logs to SEKOIA.IO, you can use other solutions, such as Logstash, syslog-ng, Graylog, etc.","title":"SEKOIA.IO Log Collector Transports"},{"location":"integrations/transport/#syslog-integration","text":"We are providing documentation and example configurations on how to configure your log system for Rsyslog and syslog-ng , but it should be easy to configure other log collectors to forward their events to SEKOIA.IO.","title":"Syslog integration"},{"location":"integrations/transport/#https-integration","text":"To push your events through our HTTPS log collector, you have to POST your logs in the JSON format. More information and code example are available on this page .","title":"HTTPS integration"},{"location":"integrations/transport/#log-aggregator-configuration","text":"Rsyslog Logstash syslog-ng Graylog","title":"Log Aggregator Configuration"},{"location":"integrations/transport/graylog/","text":"Forward Logs to SEKOIA.IO From Graylog If you are using Graylog as a log collector, you can configure it to push your logs to SEKOIA.IO. This operation is done by using the \u201c HttpOutput Plugin for Graylog \u201d. Prerequisites: Access to SEKOIA.IO Intake key(s) created Priviledge account on the Graylog server Installation procedure Download the plugin Clone this repository Run mvn package to build a JAR file. Copy generated JAR file in target directory to your Graylog plugin directory. Restart graylog-server and you are done. The plugin directory is the plugins/ folder relative from your graylog-server directory by default and can be configured in your graylog.conf file. Configuration of the log forwarding Pipeline creation Add the intake key to a dedicated stream of logs towards SEKOIA.IO. rule \"add-key\" when true then set_field(\"intake_key\", \"INSERT_HERE_YOUR_INTAKE_KEY\"); end Change the value INSERT_HERE_YOUR_INTAKE_KEY by the relevant intake key. Activation of the HTTP output Declare the output mode associated to the SEKOIA.IO stream with the type com.plugin.HttpOutput with the unique parameter https://intake.sekoia.io","title":"Graylog"},{"location":"integrations/transport/graylog/#forward-logs-to-sekoiaio-from-graylog","text":"If you are using Graylog as a log collector, you can configure it to push your logs to SEKOIA.IO. This operation is done by using the \u201c HttpOutput Plugin for Graylog \u201d. Prerequisites: Access to SEKOIA.IO Intake key(s) created Priviledge account on the Graylog server","title":"Forward Logs to SEKOIA.IO From Graylog"},{"location":"integrations/transport/graylog/#installation-procedure","text":"Download the plugin Clone this repository Run mvn package to build a JAR file. Copy generated JAR file in target directory to your Graylog plugin directory. Restart graylog-server and you are done. The plugin directory is the plugins/ folder relative from your graylog-server directory by default and can be configured in your graylog.conf file.","title":"Installation procedure"},{"location":"integrations/transport/graylog/#configuration-of-the-log-forwarding","text":"","title":"Configuration of the log forwarding"},{"location":"integrations/transport/graylog/#pipeline-creation","text":"Add the intake key to a dedicated stream of logs towards SEKOIA.IO. rule \"add-key\" when true then set_field(\"intake_key\", \"INSERT_HERE_YOUR_INTAKE_KEY\"); end Change the value INSERT_HERE_YOUR_INTAKE_KEY by the relevant intake key.","title":"Pipeline creation"},{"location":"integrations/transport/graylog/#activation-of-the-http-output","text":"Declare the output mode associated to the SEKOIA.IO stream with the type com.plugin.HttpOutput with the unique parameter https://intake.sekoia.io","title":"Activation of the HTTP output"},{"location":"integrations/transport/https/","text":"Forward Logs to SEKOIA.IO via HTTPS To push your events through our HTTPS log collector, you have to POST your logs in the JSON format. To send us events, you should set Content-Type HTTP header to application/json . The following fields are currently handled by SEKOIA.IO\u2019S HTTPS log collector: Field Mandatory? Type Description intakey_key Yes String Intake to which you would like to push events to json Yes String The actual log payload. If you want to push structured JSON logs, please send them as quoted JSON here @timestamp No Datetime Event date if you want to push your own date (fallback is to use the reception\u2019s date) Push Events to SEKOIA.IO Using Python To push text events, one can just POST content to https://intake.sekoia.io : import requests content = { \"intake_key\" : \"YOUR_INTAKE_KEY\" , \"json\" : \"[764008:0] info: 198.51.100.10 example.org. A IN\" } requests . post ( \"https://intake.sekoia.io\" , json = content ) To push structured JSON data to SEKOIA.IO, you can push your payload as quoted JSON in the POST ed payload: import requests import json structured_log = { \"key\" : \"value\" } content = { \"intake_key\" : \"YOUR_INTAKE_KEY\" , \"json\" : json . dumps ( structured_log )} requests . post ( \"https://intake.sekoia.io\" , json = content )","title":"HTTPS"},{"location":"integrations/transport/https/#forward-logs-to-sekoiaio-via-https","text":"To push your events through our HTTPS log collector, you have to POST your logs in the JSON format. To send us events, you should set Content-Type HTTP header to application/json . The following fields are currently handled by SEKOIA.IO\u2019S HTTPS log collector: Field Mandatory? Type Description intakey_key Yes String Intake to which you would like to push events to json Yes String The actual log payload. If you want to push structured JSON logs, please send them as quoted JSON here @timestamp No Datetime Event date if you want to push your own date (fallback is to use the reception\u2019s date)","title":"Forward Logs to SEKOIA.IO via HTTPS"},{"location":"integrations/transport/https/#push-events-to-sekoiaio-using-python","text":"To push text events, one can just POST content to https://intake.sekoia.io : import requests content = { \"intake_key\" : \"YOUR_INTAKE_KEY\" , \"json\" : \"[764008:0] info: 198.51.100.10 example.org. A IN\" } requests . post ( \"https://intake.sekoia.io\" , json = content ) To push structured JSON data to SEKOIA.IO, you can push your payload as quoted JSON in the POST ed payload: import requests import json structured_log = { \"key\" : \"value\" } content = { \"intake_key\" : \"YOUR_INTAKE_KEY\" , \"json\" : json . dumps ( structured_log )} requests . post ( \"https://intake.sekoia.io\" , json = content )","title":"Push Events to SEKOIA.IO Using Python"},{"location":"integrations/transport/logstash/","text":"Forward Logs to SEKOIA.IO From Logstash If you are using Logstash as a log collector and/or parser, you can configure it to push your logs to SEKOIA.IO. This operation is done by using the \u201c Http output plugin \u201d, which is bundled into the default version of Logstash. To push logs, you have to configure some filters in Logstash that will add the proper \u201cintake key\u201d considering your logs. In the following example, we are pushing Apache HTTP Server and NGINX logs to SEKOIA.IO. To filter in events, we are relying on Logstash tags. Make sure you update your intake key value by changing CHANGE_ME_INTAKE_KEY below. You can add as many \u201cfilters\u201c you want in the filter section. filter { # Edit this filter to adapt to your own needs. if \"apache2\" in [tags] { mutate { add_field => { \"[@metadata][intake_key]\" => \"CHANGE_ME_INTAKE_KEY\" } } } else if \"nginx\" in [tags] { mutate { add_field => { \"[@metadata][intake_key]\" => \"CHANGE_ME_INTAKE_KEY\" } } } } output { if \"\" in [@metadata][intake_key] { http { format => \"json\" http_method => \"post\" url => \"https://intake.sekoia.io\" mapping => { \"@timestamp\" => \"%{@timestamp}\" \"json\" => \"%{message}\" \"intake_key\" => \"%{[@metadata][intake_key]}\" } } } }","title":"Logstash"},{"location":"integrations/transport/logstash/#forward-logs-to-sekoiaio-from-logstash","text":"If you are using Logstash as a log collector and/or parser, you can configure it to push your logs to SEKOIA.IO. This operation is done by using the \u201c Http output plugin \u201d, which is bundled into the default version of Logstash. To push logs, you have to configure some filters in Logstash that will add the proper \u201cintake key\u201d considering your logs. In the following example, we are pushing Apache HTTP Server and NGINX logs to SEKOIA.IO. To filter in events, we are relying on Logstash tags. Make sure you update your intake key value by changing CHANGE_ME_INTAKE_KEY below. You can add as many \u201cfilters\u201c you want in the filter section. filter { # Edit this filter to adapt to your own needs. if \"apache2\" in [tags] { mutate { add_field => { \"[@metadata][intake_key]\" => \"CHANGE_ME_INTAKE_KEY\" } } } else if \"nginx\" in [tags] { mutate { add_field => { \"[@metadata][intake_key]\" => \"CHANGE_ME_INTAKE_KEY\" } } } } output { if \"\" in [@metadata][intake_key] { http { format => \"json\" http_method => \"post\" url => \"https://intake.sekoia.io\" mapping => { \"@timestamp\" => \"%{@timestamp}\" \"json\" => \"%{message}\" \"intake_key\" => \"%{[@metadata][intake_key]}\" } } } }","title":"Forward Logs to SEKOIA.IO From Logstash"},{"location":"integrations/transport/rsyslog/","text":"Forward Logs to SEKOIA.IO Using Rsyslog Overview Many technologies or agents allows the forwarding of their logs using the syslog protocol (RFC 5426). We recommand to centralise them on a dedicated server: Rsyslog. Prerequisites The following prerequisites are needed in order to setup efficient Rsyslog: Have administrator privileges of the Debian server: root . Inbound traffic from the equipments to the Rsyslog must be open on TCP 514 . Outbound traffic from the Rsyslog to the SEKOIA.IO platform must be open on TCP 10514 (IP for intake.sekoia.io is 145.239.192.38 ). General procedure After receiving the IDs to connect to the Linux server, the main activities are to be followed. Connect to the Rsyslog node using SSH Install the relevant packages Modify the /etc/rsyslog.conf main configuration file Create configuration files for each technology you want to forward to SEKOIA.IO Edit these files with the related Intake Keys Download the SEKOIA.IO certificate Start the Rsyslog service and ensure it is correclty set-up Example of auto-setup configuration In order to help users setting up this concentrator we suggest the following bash script working for Ubuntu or Debian server. It will automatically configure you Rsyslog server to collect and forward Windows Event logs. sudo must be installed and set-up for the current user. Before, connect to SEKOIA.IO Operation Center, add a Windows Intake to the relevant Entity, and note the Intake Key #!/bin/bash ##### This file is used to automate the Rsyslog setup # Install service and dependances sudo apt update sudo apt install -y rsyslog rsyslog-gnutls wget ### Create a dedicated Rsyslog configuration file RsyslogConfFile = \"/etc/rsyslog.conf\" sudo /bin/cat <<\\EOM >$RsyslogConfFile # /etc/rsyslog.conf configuration file for Rsyslog module(load=\"imuxsock\") # provides support for local system logging module(load=\"imklog\") # provides kernel logging support # provides UDP syslog reception module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # provides TCP syslog reception module(load=\"imtcp\") input(type=\"imtcp\" port=\"514\") # Use traditional timestamp format. $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # Set the default permissions for all log files. $FileOwner root $FileGroup adm $FileCreateMode 0640 $DirCreateMode 0755 $Umask 0022 $ActionQueueType LinkedList # create a queue stored in the RAM $ActionQueueFileName sek_fwd # set up the prefix for writting $ActionQueueMaxDiskSpace 5g # allow 5 giga of storage for the buffer $ActionQueueSaveOnShutdown on # write on disk is the rsyslog is whut down $ActionResumeRetryCount -1 # prevent the rsyslog from droping the logs if the connexion is interrupted # Where to place spool and state files $WorkDirectory /var/spool/rsyslog $IncludeConfig /etc/rsyslog.d/*.conf # Rules *.*;auth,authpriv.none -/var/log/syslog EOM ### Create a dedicated Windows configuration file WindowsFile = \"/etc/rsyslog.d/15-windows.conf\" sudo /bin/cat <<\\EOM >$WindowsFile $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem template(name=\"SEKOIAIOWindowsTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\") if ($syslogtag contains 'Microsoft-Windows') then { action( type=\"omfwd\" protocol=\"tcp\" target=\"intake.sekoia.io\" port=\"10514\" TCP_Framing=\"octet-counted\" StreamDriver=\"gtls\" StreamDriverMode=\"1\" StreamDriverAuthMode=\"x509/name\" StreamDriverPermittedPeers=\"intake.sekoia.io\" Template=\"SEKOIAIOWindowsTemplate\" ) } EOM # Collect the Sekoia Key for encryption between Rsyslog and Sekoia.io sudo wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem It is possible to copy-paste this configuration locally then upload it with SCP command, or simple copy-paste it from the web to your remote server. Once the file created on the Rsyslog, dont forget to make it executable with the command chmod +x <filename.sh> Then run it: ./<filename.sh> Change the intake key value in the /etc/rsyslog.d/15-windows.conf file Restart the Rsyslog service: sudo systemctl restart rsyslog.service Forward Logs Using RELP Protocol Rsyslog is able to push logs via a reliable protocol, called RELP. By using this prococol, SEKOIA.IO\u2019s collection point will acknowledge logs when receiving it. This will let the client Rsyslog be able to resend events if an error occured. SEKOIA.IO\u2019s RELP endpoint is available at relp.intake.sekoia.io ( 145.239.192.124 ) on port 11514 . The most noticeable change using RELP in Rsyslog, is the output module used ( omrelp ). The first step is to install rsyslog-relp and rsyslog-openssl packages to be able to push logs. Most distributions are providing these packages natively. Then, you have to edit your main Rsyslog configuration to load the omrelp module: module(load=\"omrelp\" tls.tlslib=\"openssl\") Finally, you have to configure the output action to push your events to SEKOIA.IO via the RELP protocol. In this example, we are pushing Unbound events. template(name=\"SEKOIAIOUnboundTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\") if ($programname startswith 'unbound') then { action( type=\"omrelp\" target=\"relp.intake.sekoia.io\" port=\"11514\" tls=\"on\" tls.caCert=\"/etc/rsyslog.d/SEKOIA-IO-intake.pem\" tls.authmode=\"name\" tls.permittedPeer=[\"relp.intake.sekoia.io\"] template=\"SEKOIAIOUnboundTemplate\" ) } Troubleshooting After setting up your Rsyslog, you may face issues due to contextual environment or error during copy pasting. Rsyslog daemon Ensure the Rsyslog service is currrently running on the server ps -A | grep rsyslog You should see a line with rsyslogd daemon. If not, try to restart the service: sudo systemctl restart rsyslog.service Also insure the following lines are not commented in the configuration file /etc/rsyslog.conf : # provide TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" ) Local messages Ensure the logs are received on the Rsyslog server, meaning: - Configurations are correctly undertaken on the remote equipements - Internal network flows are open on TCP or UDP 514 To check it, run the following command: tail -n 15 /var/log/syslog Forwarded messages to SEKOIA.IO Ensure the connection is ESTABLISHED between the Rsyslog server and SEKOIA.IO. To do so, please run the following command: sudo ss -ltp | grep syslog If the connection is not established, and the previous statuses are operational, it is possible that a file xx-<technology>.conf has a mistyping. By experience, most of the time there is a misconfiguration in the IF condition. For example, if you receive the Fortigate logs in /var/log/syslog but not in SEKOIA.IO, you can follow those steps: First try to log the raw events to a local file, by appending the following line in the configuration file: *.* then /tmp/fortigate.log ; SEKOIAIOFortigateTemplate Restart the Rsyslog service sudo systemctl restart rsyslog.service Check if the fortigate.log file is created and populated sudo tail /tmp/fortigate.log Then edit your configuration file by adding the if condition step by step","title":"Rsyslog"},{"location":"integrations/transport/rsyslog/#forward-logs-to-sekoiaio-using-rsyslog","text":"","title":"Forward Logs to SEKOIA.IO Using Rsyslog"},{"location":"integrations/transport/rsyslog/#overview","text":"Many technologies or agents allows the forwarding of their logs using the syslog protocol (RFC 5426). We recommand to centralise them on a dedicated server: Rsyslog.","title":"Overview"},{"location":"integrations/transport/rsyslog/#prerequisites","text":"The following prerequisites are needed in order to setup efficient Rsyslog: Have administrator privileges of the Debian server: root . Inbound traffic from the equipments to the Rsyslog must be open on TCP 514 . Outbound traffic from the Rsyslog to the SEKOIA.IO platform must be open on TCP 10514 (IP for intake.sekoia.io is 145.239.192.38 ).","title":"Prerequisites"},{"location":"integrations/transport/rsyslog/#general-procedure","text":"After receiving the IDs to connect to the Linux server, the main activities are to be followed. Connect to the Rsyslog node using SSH Install the relevant packages Modify the /etc/rsyslog.conf main configuration file Create configuration files for each technology you want to forward to SEKOIA.IO Edit these files with the related Intake Keys Download the SEKOIA.IO certificate Start the Rsyslog service and ensure it is correclty set-up","title":"General procedure"},{"location":"integrations/transport/rsyslog/#example-of-auto-setup-configuration","text":"In order to help users setting up this concentrator we suggest the following bash script working for Ubuntu or Debian server. It will automatically configure you Rsyslog server to collect and forward Windows Event logs. sudo must be installed and set-up for the current user. Before, connect to SEKOIA.IO Operation Center, add a Windows Intake to the relevant Entity, and note the Intake Key #!/bin/bash ##### This file is used to automate the Rsyslog setup # Install service and dependances sudo apt update sudo apt install -y rsyslog rsyslog-gnutls wget ### Create a dedicated Rsyslog configuration file RsyslogConfFile = \"/etc/rsyslog.conf\" sudo /bin/cat <<\\EOM >$RsyslogConfFile # /etc/rsyslog.conf configuration file for Rsyslog module(load=\"imuxsock\") # provides support for local system logging module(load=\"imklog\") # provides kernel logging support # provides UDP syslog reception module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # provides TCP syslog reception module(load=\"imtcp\") input(type=\"imtcp\" port=\"514\") # Use traditional timestamp format. $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # Set the default permissions for all log files. $FileOwner root $FileGroup adm $FileCreateMode 0640 $DirCreateMode 0755 $Umask 0022 $ActionQueueType LinkedList # create a queue stored in the RAM $ActionQueueFileName sek_fwd # set up the prefix for writting $ActionQueueMaxDiskSpace 5g # allow 5 giga of storage for the buffer $ActionQueueSaveOnShutdown on # write on disk is the rsyslog is whut down $ActionResumeRetryCount -1 # prevent the rsyslog from droping the logs if the connexion is interrupted # Where to place spool and state files $WorkDirectory /var/spool/rsyslog $IncludeConfig /etc/rsyslog.d/*.conf # Rules *.*;auth,authpriv.none -/var/log/syslog EOM ### Create a dedicated Windows configuration file WindowsFile = \"/etc/rsyslog.d/15-windows.conf\" sudo /bin/cat <<\\EOM >$WindowsFile $DefaultNetstreamDriverCAFile /etc/rsyslog.d/SEKOIA-IO-intake.pem template(name=\"SEKOIAIOWindowsTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\") if ($syslogtag contains 'Microsoft-Windows') then { action( type=\"omfwd\" protocol=\"tcp\" target=\"intake.sekoia.io\" port=\"10514\" TCP_Framing=\"octet-counted\" StreamDriver=\"gtls\" StreamDriverMode=\"1\" StreamDriverAuthMode=\"x509/name\" StreamDriverPermittedPeers=\"intake.sekoia.io\" Template=\"SEKOIAIOWindowsTemplate\" ) } EOM # Collect the Sekoia Key for encryption between Rsyslog and Sekoia.io sudo wget -O /etc/rsyslog.d/SEKOIA-IO-intake.pem https://app.sekoia.io/assets/files/SEKOIA-IO-intake.pem It is possible to copy-paste this configuration locally then upload it with SCP command, or simple copy-paste it from the web to your remote server. Once the file created on the Rsyslog, dont forget to make it executable with the command chmod +x <filename.sh> Then run it: ./<filename.sh> Change the intake key value in the /etc/rsyslog.d/15-windows.conf file Restart the Rsyslog service: sudo systemctl restart rsyslog.service","title":"Example of auto-setup configuration"},{"location":"integrations/transport/rsyslog/#forward-logs-using-relp-protocol","text":"Rsyslog is able to push logs via a reliable protocol, called RELP. By using this prococol, SEKOIA.IO\u2019s collection point will acknowledge logs when receiving it. This will let the client Rsyslog be able to resend events if an error occured. SEKOIA.IO\u2019s RELP endpoint is available at relp.intake.sekoia.io ( 145.239.192.124 ) on port 11514 . The most noticeable change using RELP in Rsyslog, is the output module used ( omrelp ). The first step is to install rsyslog-relp and rsyslog-openssl packages to be able to push logs. Most distributions are providing these packages natively. Then, you have to edit your main Rsyslog configuration to load the omrelp module: module(load=\"omrelp\" tls.tlslib=\"openssl\") Finally, you have to configure the output action to push your events to SEKOIA.IO via the RELP protocol. In this example, we are pushing Unbound events. template(name=\"SEKOIAIOUnboundTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"YOUR_INTAKE_KEY\\\"] %msg%\\n\") if ($programname startswith 'unbound') then { action( type=\"omrelp\" target=\"relp.intake.sekoia.io\" port=\"11514\" tls=\"on\" tls.caCert=\"/etc/rsyslog.d/SEKOIA-IO-intake.pem\" tls.authmode=\"name\" tls.permittedPeer=[\"relp.intake.sekoia.io\"] template=\"SEKOIAIOUnboundTemplate\" ) }","title":"Forward Logs Using RELP Protocol"},{"location":"integrations/transport/rsyslog/#troubleshooting","text":"After setting up your Rsyslog, you may face issues due to contextual environment or error during copy pasting.","title":"Troubleshooting"},{"location":"integrations/transport/rsyslog/#rsyslog-daemon","text":"Ensure the Rsyslog service is currrently running on the server ps -A | grep rsyslog You should see a line with rsyslogd daemon. If not, try to restart the service: sudo systemctl restart rsyslog.service Also insure the following lines are not commented in the configuration file /etc/rsyslog.conf : # provide TCP syslog reception module ( load = \"imtcp\" ) input ( type = \"imtcp\" port = \"514\" ) # provides UDP syslog reception module ( load = \"imudp\" ) input ( type = \"imudp\" port = \"514\" )","title":"Rsyslog daemon"},{"location":"integrations/transport/rsyslog/#local-messages","text":"Ensure the logs are received on the Rsyslog server, meaning: - Configurations are correctly undertaken on the remote equipements - Internal network flows are open on TCP or UDP 514 To check it, run the following command: tail -n 15 /var/log/syslog","title":"Local messages"},{"location":"integrations/transport/rsyslog/#forwarded-messages-to-sekoiaio","text":"Ensure the connection is ESTABLISHED between the Rsyslog server and SEKOIA.IO. To do so, please run the following command: sudo ss -ltp | grep syslog If the connection is not established, and the previous statuses are operational, it is possible that a file xx-<technology>.conf has a mistyping. By experience, most of the time there is a misconfiguration in the IF condition. For example, if you receive the Fortigate logs in /var/log/syslog but not in SEKOIA.IO, you can follow those steps: First try to log the raw events to a local file, by appending the following line in the configuration file: *.* then /tmp/fortigate.log ; SEKOIAIOFortigateTemplate Restart the Rsyslog service sudo systemctl restart rsyslog.service Check if the fortigate.log file is created and populated sudo tail /tmp/fortigate.log Then edit your configuration file by adding the if condition step by step","title":"Forwarded messages to SEKOIA.IO"},{"location":"integrations/transport/syslog-ng/","text":"Forward Logs to SEKOIA.IO From syslog-ng If you are using syslog-ng as a log collector, you can configure it to push your logs to SEKOIA.IO. This operation is done by using Syslog protocol. To push logs, you have to configure some filters and rewrite rules in syslog-ng that will add the proper \u201cintake key\u201d considering your logs. Destination Configuration The first thing to do is to add certification authority (CA) files in the /etc/syslog-ng/ca.d directory. To proceed, you can execute the following commands in your favorite shell: Click here to display commands # cat > /etc/syslog-ng/ca.d/2e5ac55d.0 << EOF -----BEGIN CERTIFICATE----- MIIDSjCCAjKgAwIBAgIQRK+wgNajJ7qJMDmGLvhAazANBgkqhkiG9w0BAQUFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTAwMDkzMDIxMTIxOVoXDTIxMDkzMDE0MDExNVow PzEkMCIGA1UEChMbRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QgQ28uMRcwFQYDVQQD Ew5EU1QgUm9vdCBDQSBYMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB AN+v6ZdQCINXtMxiZfaQguzH0yxrMMpb7NnDfcdAwRgUi+DoM3ZJKuM/IUmTrE4O rz5Iy2Xu/NMhD2XSKtkyj4zl93ewEnu1lcCJo6m67XMuegwGMoOifooUMM0RoOEq OLl5CjH9UL2AZd+3UWODyOKIYepLYYHsUmu5ouJLGiifSKOeDNoJjj4XLh7dIN9b xiqKqy69cK3FCxolkHRyxXtqqzTWMIn/5WgTe1QLyNau7Fqckh49ZLOMxt+/yUFw 7BZy1SbsOFU5Q9D8/RhcQPGX69Wam40dutolucbY38EVAjqr2m7xPi71XAicPNaD aeQQmxkqtilX4+U9m5/wAl0CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNV HQ8BAf8EBAMCAQYwHQYDVR0OBBYEFMSnsaR7LHH62+FLkHX/xBVghYkQMA0GCSqG SIb3DQEBBQUAA4IBAQCjGiybFwBcqR7uKGY3Or+Dxz9LwwmglSBd49lZRNI+DT69 ikugdB/OEIKcdBodfpga3csTS7MgROSR6cz8faXbauX+5v3gTt23ADq1cEmv8uXr AvHRAosZy5Q6XkjEGB5YGV8eAlrwDPGxrancWYaLbumR9YbK+rlmM6pZW87ipxZz R8srzJmwN0jP41ZL9c8PDHIyh8bwRLtTcm1D9SZImlJnt1ir/md2cXjbDaJWFBM5 JDGFoqgCWjBH4d1QB7wCCZAA62RjYJsWvIjJEubSfZGL+T0yjWW06XyxV3bqxbYo Ob8VZRzI9neWagqNdwvYkQsEjgfbKbYK7p2CNTUQ -----END CERTIFICATE----- EOF # cat > /etc/syslog-ng/ca.d/4f06f81d.0 << EOF -----BEGIN CERTIFICATE----- MIIEkjCCA3qgAwIBAgIQCgFBQgAAAVOFc2oLheynCDANBgkqhkiG9w0BAQsFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTE2MDMxNzE2NDA0NloXDTIxMDMxNzE2NDA0Nlow SjELMAkGA1UEBhMCVVMxFjAUBgNVBAoTDUxldCdzIEVuY3J5cHQxIzAhBgNVBAMT GkxldCdzIEVuY3J5cHQgQXV0aG9yaXR5IFgzMIIBIjANBgkqhkiG9w0BAQEFAAOC AQ8AMIIBCgKCAQEAnNMM8FrlLke3cl03g7NoYzDq1zUmGSXhvb418XCSL7e4S0EF q6meNQhY7LEqxGiHC6PjdeTm86dicbp5gWAf15Gan/PQeGdxyGkOlZHP/uaZ6WA8 SMx+yk13EiSdRxta67nsHjcAHJyse6cF6s5K671B5TaYucv9bTyWaN8jKkKQDIZ0 Z8h/pZq4UmEUEz9l6YKHy9v6Dlb2honzhT+Xhq+w3Brvaw2VFn3EK6BlspkENnWA a6xK8xuQSXgvopZPKiAlKQTGdMDQMc2PMTiVFrqoM7hD8bEfwzB/onkxEz0tNvjj /PIzark5McWvxI0NHWQWM6r6hCm21AvA2H3DkwIDAQABo4IBfTCCAXkwEgYDVR0T AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAYYwfwYIKwYBBQUHAQEEczBxMDIG CCsGAQUFBzABhiZodHRwOi8vaXNyZy50cnVzdGlkLm9jc3AuaWRlbnRydXN0LmNv bTA7BggrBgEFBQcwAoYvaHR0cDovL2FwcHMuaWRlbnRydXN0LmNvbS9yb290cy9k c3Ryb290Y2F4My5wN2MwHwYDVR0jBBgwFoAUxKexpHsscfrb4UuQdf/EFWCFiRAw VAYDVR0gBE0wSzAIBgZngQwBAgEwPwYLKwYBBAGC3xMBAQEwMDAuBggrBgEFBQcC ARYiaHR0cDovL2Nwcy5yb290LXgxLmxldHNlbmNyeXB0Lm9yZzA8BgNVHR8ENTAz MDGgL6AthitodHRwOi8vY3JsLmlkZW50cnVzdC5jb20vRFNUUk9PVENBWDNDUkwu Y3JsMB0GA1UdDgQWBBSoSmpjBH3duubRObemRWXv86jsoTANBgkqhkiG9w0BAQsF AAOCAQEA3TPXEfNjWDjdGBX7CVW+dla5cEilaUcne8IkCJLxWh9KEik3JHRRHGJo uM2VcGfl96S8TihRzZvoroed6ti6WqEBmtzw3Wodatg+VyOeph4EYpr/1wXKtx8/ wApIvJSwtmVi4MFU5aMqrSDE6ea73Mj2tcMyo5jMd6jmeWUHK8so/joWUoHOUgwu X4Po1QYz+3dszkDqMp4fklxBwXRsW10KXzPMTZ+sOPAveyxindmjkW8lGy+QsRlG PfZ+G6Z6h7mjem0Y+iWlkYcV4PIWL1iwBi8saCbGS5jN2p8M+X+Q7UNKEkROb3N6 KOqkqm57TH2H3eDJAkSnh6/DNFu0Qg == -----END CERTIFICATE----- EOF Then, you have to define syslog-ng\u2019s destination module to push Syslog events to SEKOIA.IO in /etc/syslog-ng/syslog-ng.conf : destination d_sekoia_io { syslog(\"intake.sekoia.io\" port(10514) transport(\"tls\") frac_digits(6) ts_format(iso) tls(peer-verify(required-trusted) ca-dir(\"/etc/syslog-ng/ca.d\")) ); }; Dispatching Events In the following example, we are pushing Apache HTTP Server and NGINX logs to SEKOIA.IO. To filter in events, we are relying on process names. Make sure you update your intake key value by changing CHANGE_ME_INTAKE_KEY below. You have to add the following configuration snippet in your main syslog-ng configuration file, /etc/syslog-ng/syslog-ng.conf : filter f_apache2 { program(apache2); }; rewrite r_apache2 { set(\"CHANGE_ME_INTAKE_KEY\" value(\".SDATA.SEKOIA@53288.intake_key\")); }; log { source(s_network); filter(f_apache2); rewrite(r_apache2); destination(d_sekoia_io); }; filter f_nginx { program(nginx); }; rewrite r_nginx { set(\"CHANGE_ME_INTAKE_KEY\" value(\".SDATA.SEKOIA@53288.intake_key\")); }; log { source(s_network); filter(f_nginx); rewrite(r_nginx); destination(d_sekoia_io); };","title":"syslog-ng"},{"location":"integrations/transport/syslog-ng/#forward-logs-to-sekoiaio-from-syslog-ng","text":"If you are using syslog-ng as a log collector, you can configure it to push your logs to SEKOIA.IO. This operation is done by using Syslog protocol. To push logs, you have to configure some filters and rewrite rules in syslog-ng that will add the proper \u201cintake key\u201d considering your logs.","title":"Forward Logs to SEKOIA.IO From syslog-ng"},{"location":"integrations/transport/syslog-ng/#destination-configuration","text":"The first thing to do is to add certification authority (CA) files in the /etc/syslog-ng/ca.d directory. To proceed, you can execute the following commands in your favorite shell: Click here to display commands # cat > /etc/syslog-ng/ca.d/2e5ac55d.0 << EOF -----BEGIN CERTIFICATE----- MIIDSjCCAjKgAwIBAgIQRK+wgNajJ7qJMDmGLvhAazANBgkqhkiG9w0BAQUFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTAwMDkzMDIxMTIxOVoXDTIxMDkzMDE0MDExNVow PzEkMCIGA1UEChMbRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QgQ28uMRcwFQYDVQQD Ew5EU1QgUm9vdCBDQSBYMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB AN+v6ZdQCINXtMxiZfaQguzH0yxrMMpb7NnDfcdAwRgUi+DoM3ZJKuM/IUmTrE4O rz5Iy2Xu/NMhD2XSKtkyj4zl93ewEnu1lcCJo6m67XMuegwGMoOifooUMM0RoOEq OLl5CjH9UL2AZd+3UWODyOKIYepLYYHsUmu5ouJLGiifSKOeDNoJjj4XLh7dIN9b xiqKqy69cK3FCxolkHRyxXtqqzTWMIn/5WgTe1QLyNau7Fqckh49ZLOMxt+/yUFw 7BZy1SbsOFU5Q9D8/RhcQPGX69Wam40dutolucbY38EVAjqr2m7xPi71XAicPNaD aeQQmxkqtilX4+U9m5/wAl0CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNV HQ8BAf8EBAMCAQYwHQYDVR0OBBYEFMSnsaR7LHH62+FLkHX/xBVghYkQMA0GCSqG SIb3DQEBBQUAA4IBAQCjGiybFwBcqR7uKGY3Or+Dxz9LwwmglSBd49lZRNI+DT69 ikugdB/OEIKcdBodfpga3csTS7MgROSR6cz8faXbauX+5v3gTt23ADq1cEmv8uXr AvHRAosZy5Q6XkjEGB5YGV8eAlrwDPGxrancWYaLbumR9YbK+rlmM6pZW87ipxZz R8srzJmwN0jP41ZL9c8PDHIyh8bwRLtTcm1D9SZImlJnt1ir/md2cXjbDaJWFBM5 JDGFoqgCWjBH4d1QB7wCCZAA62RjYJsWvIjJEubSfZGL+T0yjWW06XyxV3bqxbYo Ob8VZRzI9neWagqNdwvYkQsEjgfbKbYK7p2CNTUQ -----END CERTIFICATE----- EOF # cat > /etc/syslog-ng/ca.d/4f06f81d.0 << EOF -----BEGIN CERTIFICATE----- MIIEkjCCA3qgAwIBAgIQCgFBQgAAAVOFc2oLheynCDANBgkqhkiG9w0BAQsFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTE2MDMxNzE2NDA0NloXDTIxMDMxNzE2NDA0Nlow SjELMAkGA1UEBhMCVVMxFjAUBgNVBAoTDUxldCdzIEVuY3J5cHQxIzAhBgNVBAMT GkxldCdzIEVuY3J5cHQgQXV0aG9yaXR5IFgzMIIBIjANBgkqhkiG9w0BAQEFAAOC AQ8AMIIBCgKCAQEAnNMM8FrlLke3cl03g7NoYzDq1zUmGSXhvb418XCSL7e4S0EF q6meNQhY7LEqxGiHC6PjdeTm86dicbp5gWAf15Gan/PQeGdxyGkOlZHP/uaZ6WA8 SMx+yk13EiSdRxta67nsHjcAHJyse6cF6s5K671B5TaYucv9bTyWaN8jKkKQDIZ0 Z8h/pZq4UmEUEz9l6YKHy9v6Dlb2honzhT+Xhq+w3Brvaw2VFn3EK6BlspkENnWA a6xK8xuQSXgvopZPKiAlKQTGdMDQMc2PMTiVFrqoM7hD8bEfwzB/onkxEz0tNvjj /PIzark5McWvxI0NHWQWM6r6hCm21AvA2H3DkwIDAQABo4IBfTCCAXkwEgYDVR0T AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAYYwfwYIKwYBBQUHAQEEczBxMDIG CCsGAQUFBzABhiZodHRwOi8vaXNyZy50cnVzdGlkLm9jc3AuaWRlbnRydXN0LmNv bTA7BggrBgEFBQcwAoYvaHR0cDovL2FwcHMuaWRlbnRydXN0LmNvbS9yb290cy9k c3Ryb290Y2F4My5wN2MwHwYDVR0jBBgwFoAUxKexpHsscfrb4UuQdf/EFWCFiRAw VAYDVR0gBE0wSzAIBgZngQwBAgEwPwYLKwYBBAGC3xMBAQEwMDAuBggrBgEFBQcC ARYiaHR0cDovL2Nwcy5yb290LXgxLmxldHNlbmNyeXB0Lm9yZzA8BgNVHR8ENTAz MDGgL6AthitodHRwOi8vY3JsLmlkZW50cnVzdC5jb20vRFNUUk9PVENBWDNDUkwu Y3JsMB0GA1UdDgQWBBSoSmpjBH3duubRObemRWXv86jsoTANBgkqhkiG9w0BAQsF AAOCAQEA3TPXEfNjWDjdGBX7CVW+dla5cEilaUcne8IkCJLxWh9KEik3JHRRHGJo uM2VcGfl96S8TihRzZvoroed6ti6WqEBmtzw3Wodatg+VyOeph4EYpr/1wXKtx8/ wApIvJSwtmVi4MFU5aMqrSDE6ea73Mj2tcMyo5jMd6jmeWUHK8so/joWUoHOUgwu X4Po1QYz+3dszkDqMp4fklxBwXRsW10KXzPMTZ+sOPAveyxindmjkW8lGy+QsRlG PfZ+G6Z6h7mjem0Y+iWlkYcV4PIWL1iwBi8saCbGS5jN2p8M+X+Q7UNKEkROb3N6 KOqkqm57TH2H3eDJAkSnh6/DNFu0Qg == -----END CERTIFICATE----- EOF Then, you have to define syslog-ng\u2019s destination module to push Syslog events to SEKOIA.IO in /etc/syslog-ng/syslog-ng.conf : destination d_sekoia_io { syslog(\"intake.sekoia.io\" port(10514) transport(\"tls\") frac_digits(6) ts_format(iso) tls(peer-verify(required-trusted) ca-dir(\"/etc/syslog-ng/ca.d\")) ); };","title":"Destination Configuration"},{"location":"integrations/transport/syslog-ng/#dispatching-events","text":"In the following example, we are pushing Apache HTTP Server and NGINX logs to SEKOIA.IO. To filter in events, we are relying on process names. Make sure you update your intake key value by changing CHANGE_ME_INTAKE_KEY below. You have to add the following configuration snippet in your main syslog-ng configuration file, /etc/syslog-ng/syslog-ng.conf : filter f_apache2 { program(apache2); }; rewrite r_apache2 { set(\"CHANGE_ME_INTAKE_KEY\" value(\".SDATA.SEKOIA@53288.intake_key\")); }; log { source(s_network); filter(f_apache2); rewrite(r_apache2); destination(d_sekoia_io); }; filter f_nginx { program(nginx); }; rewrite r_nginx { set(\"CHANGE_ME_INTAKE_KEY\" value(\".SDATA.SEKOIA@53288.intake_key\")); }; log { source(s_network); filter(f_nginx); rewrite(r_nginx); destination(d_sekoia_io); };","title":"Dispatching Events"},{"location":"intelligence_center/","text":"Intelligence Center The Intelligence Center is a Threat Intelligence knowledge base that is being constantly updated by SEKOIA analysts. It is meant to store all levels of Cyber Threat Intelligence (CTI), from strategic (targets, motivations) to technical (indicator of compromises). We recommend you learn about our data model before learning how you can leverage the API .","title":"Overview"},{"location":"intelligence_center/#intelligence-center","text":"The Intelligence Center is a Threat Intelligence knowledge base that is being constantly updated by SEKOIA analysts. It is meant to store all levels of Cyber Threat Intelligence (CTI), from strategic (targets, motivations) to technical (indicator of compromises). We recommend you learn about our data model before learning how you can leverage the API .","title":"Intelligence Center"},{"location":"intelligence_center/api/","text":"Intelligence Center API The API is reachable at https://api.sekoia.io . All Intelligence Center endpoints start with https://api.sekoia.io/v2/inthreat/ . Authentication Authentication is required for all API endpoints. Authentication is done with the Authorization header: Authorization: Bearer <APIKEY> Feeds A feed allows filtering the CTI Objects based on some filters. The available filters are: Types TLPs Targeted sectors of activity Targeted locations Sources A feed can be consumed by all the users belonging to the community that created it. Default Feed A default feed with no filters is available in all communities without having to create it. The special feed ID to use is d6092c37-d8d7-45c3-8aff-c4dc26030608 . Feed Creation The easiest way to create feed configurations is to use the Intelligence Center interface, by clicking on Feeds in the left menu. The dropdown next to the feed's name will allow you to copy the ID or the consumption URLs of the feed. If you would prefer creating the feed with the API, you can use the feeds endpoint. The result should contain the feed id that may be used to consume the feed. Feed Consumption To consume the objects from a feed the following endpoint may be used: GET v2/inthreat/collections/{feed_id}/objects . i.e. GET v2/inthreat/collections/d6092c37-d8d7-45c3-8aff-c4dc26030608/objects/ Be aware that only the users belonging to the feed's community will be able to access it when using a custom feed. Pagination The objects endpoint returns only 100 objects by default but this value can be increased up to 2000 objects per request with the limit parameter. i.e. GET v2/inthreat/collections/{feed_id}/objects?limit=2000 The response contains the STIX Objects in items and a pagination cursor in next_cursor . You can pass this cursor using the cursor parameter to get the next page of content. i.e. GET v2/inthreat/collections/{feed_id}/objects?limit=2000&cursor={next_cursor} You can safely stop iterating when items is empty or less than the requested limit . Incremental Updates It is recommended to only fetch the full feed content the first time and then to use incremental updates. To get only the records that were created and/or modified since your last request, simply use the last next_cursor that was returned in your request. Filtering by Object Type It is possible to filter returned objects by type with the match[type] parameter. Its value should be a comma-separated list of types. i.e. GET v2/inthreat/collections/{feed_id}/objects?match[type]=indicator Note that if the type is not available in the feed it will not be returned even if specified in the query. Example Script Here is a sample Python script to fetch STIX objects from the Intelligence Center: import dbm import requests from posixpath import join as urljoin APIKEY = \"APIKEY\" DEFAULT_FEED = \"d6092c37-d8d7-45c3-8aff-c4dc26030608\" BASE_URL = \"https://api.sekoia.io/v2/inthreat/\" CURSOR_FILE = \"cursor.db\" def get_new_objects ( feed_id = DEFAULT_FEED , limit = 2000 ): url = urljoin ( BASE_URL , \"collections\" , feed_id , f \"objects?limit= { limit } \" ) paginated_url = url # Use the builtin dbm module to save the cursors to disk with dbm . open ( CURSOR_FILE , \"c\" ) as cursors : while True : # If a cursor exists for this feed, add it to the URL if feed_id in cursors : paginated_url = f \" { url } &cursor= { cursors [ feed_id ] . decode ( 'ascii' ) } \" # Request the next batch of objects from the API, authenticate with the APIKEY response = requests . get ( paginated_url , headers = { \"Authorization\" : f \"Bearer { APIKEY } \" } ) response . raise_for_status () data = response . json () # Yield individual STIX Objects (SDO & SRO) for item in data [ \"items\" ]: yield item # Update cursor to have incremental updates cursors [ feed_id ] = data [ \"next_cursor\" ] # Stop current iteration if we reached the last updated object if not data [ \"items\" ] or len ( data [ \"items\" ]) < limit : break Getting an indicator's context When using indicators for detection, you might want to fetch all indicators by using a filter on the indicator type and then only fetch additional context when an indicator matched. The v2/inthreat/objects/{indicator_id}/context returns a STIX bundle containing: The indicator The first level relationships and related objects Potential relevant Course-Of-Action objects Referenced Sources and Object Markings Getting an object by its ID It is possible to get a specific object by its ID by using the GET v2/inthreat/objects/{object_id} endpoint. For relationships, use the GET v2/inthreat/relationships/{relationship_id} endpoint instead. Looking for an IOC It is possible to look for a specific indicator of compromise in the Intelligence Center and get its context with the GET v2/inthreat/indicators/context endpoint (see documentation ). import json import requests from posixpath import join as urljoin APIKEY = \"APIKEY\" BASE_URL = \"https://api.sekoia.io/v2/inthreat/\" HEADERS = { \"Authorization\" : f \"Bearer { APIKEY } \" } def get_indicator_context ( observable_type , observable_value ): response = requests . get ( urljoin ( BASE_URL , \"indicators/context\" ), params = { \"type\" : observable_type , \"value\" : observable_value }, headers = HEADERS ) response . raise_for_status () return response . json () TAXII The Intelligence Center also exposes a TAXII 2.1 server that conforms to the specification . The discovery endpoint of the TAXII server is located at https://app.sekoia.io/api/v2/inthreat/taxii-server/taxii2 . The API root of the TAXII server is located at https://app.sekoia.io/api/v2/inthreat/taxii-server . TAXII relies on HTTP Basic Authentication. To authenticate to the TAXII server, you should use your Intelligence Center API key as the Basic Authentication password (you can use any username). Here is an example of a script using the TAXII endpoints to iterate over objects: import dbm import requests from basicauth import encode as basicauth_encode # Replace with Intelligence Center API Key APIKEY = \"APIKEY\" # Replace with the TAXII URL copied from the \"feeds\" page FEED_URL = \"https://app.sekoia.io/api/v2/inthreat/taxii-server/collections/d6092c37-d8d7-45c3-8aff-c4dc26030608/objects\" CURSOR_FILE = \"cursor.db\" def get_new_objects ( feed_url = FEED_URL , limit = 2000 , version = \"2.1\" ): has_more = True headers = { \"Accept\" : \"application/taxii+json\" , \"Authorization\" : basicauth_encode ( \"api\" , APIKEY ), } parameters = { \"match[spec_version]\" : version , \"limit\" : limit } # Use the builtin dbm module to save the cursors to disk with dbm . open ( CURSOR_FILE , \"c\" ) as cursors : while has_more : # If a cursor exists for this feed, add it to the parameters if feed_url in cursors : parameters [ \"next\" ] = cursors [ feed_url ] . decode ( \"ascii\" ) # Request the next batch of objects from the API, authenticate with the APIKEY response = requests . get ( feed_url , headers = headers , params = parameters ) response . raise_for_status () data = response . json () has_more = data [ \"more\" ] cursors [ feed_url ] = data [ \"next\" ] yield from data [ \"objects\" ]","title":"API"},{"location":"intelligence_center/api/#intelligence-center-api","text":"The API is reachable at https://api.sekoia.io . All Intelligence Center endpoints start with https://api.sekoia.io/v2/inthreat/ .","title":"Intelligence Center API"},{"location":"intelligence_center/api/#authentication","text":"Authentication is required for all API endpoints. Authentication is done with the Authorization header: Authorization: Bearer <APIKEY>","title":"Authentication"},{"location":"intelligence_center/api/#feeds","text":"A feed allows filtering the CTI Objects based on some filters. The available filters are: Types TLPs Targeted sectors of activity Targeted locations Sources A feed can be consumed by all the users belonging to the community that created it.","title":"Feeds"},{"location":"intelligence_center/api/#default-feed","text":"A default feed with no filters is available in all communities without having to create it. The special feed ID to use is d6092c37-d8d7-45c3-8aff-c4dc26030608 .","title":"Default Feed"},{"location":"intelligence_center/api/#feed-creation","text":"The easiest way to create feed configurations is to use the Intelligence Center interface, by clicking on Feeds in the left menu. The dropdown next to the feed's name will allow you to copy the ID or the consumption URLs of the feed. If you would prefer creating the feed with the API, you can use the feeds endpoint. The result should contain the feed id that may be used to consume the feed.","title":"Feed Creation"},{"location":"intelligence_center/api/#feed-consumption","text":"To consume the objects from a feed the following endpoint may be used: GET v2/inthreat/collections/{feed_id}/objects . i.e. GET v2/inthreat/collections/d6092c37-d8d7-45c3-8aff-c4dc26030608/objects/ Be aware that only the users belonging to the feed's community will be able to access it when using a custom feed.","title":"Feed Consumption"},{"location":"intelligence_center/api/#pagination","text":"The objects endpoint returns only 100 objects by default but this value can be increased up to 2000 objects per request with the limit parameter. i.e. GET v2/inthreat/collections/{feed_id}/objects?limit=2000 The response contains the STIX Objects in items and a pagination cursor in next_cursor . You can pass this cursor using the cursor parameter to get the next page of content. i.e. GET v2/inthreat/collections/{feed_id}/objects?limit=2000&cursor={next_cursor} You can safely stop iterating when items is empty or less than the requested limit .","title":"Pagination"},{"location":"intelligence_center/api/#incremental-updates","text":"It is recommended to only fetch the full feed content the first time and then to use incremental updates. To get only the records that were created and/or modified since your last request, simply use the last next_cursor that was returned in your request.","title":"Incremental Updates"},{"location":"intelligence_center/api/#filtering-by-object-type","text":"It is possible to filter returned objects by type with the match[type] parameter. Its value should be a comma-separated list of types. i.e. GET v2/inthreat/collections/{feed_id}/objects?match[type]=indicator Note that if the type is not available in the feed it will not be returned even if specified in the query.","title":"Filtering by Object Type"},{"location":"intelligence_center/api/#example-script","text":"Here is a sample Python script to fetch STIX objects from the Intelligence Center: import dbm import requests from posixpath import join as urljoin APIKEY = \"APIKEY\" DEFAULT_FEED = \"d6092c37-d8d7-45c3-8aff-c4dc26030608\" BASE_URL = \"https://api.sekoia.io/v2/inthreat/\" CURSOR_FILE = \"cursor.db\" def get_new_objects ( feed_id = DEFAULT_FEED , limit = 2000 ): url = urljoin ( BASE_URL , \"collections\" , feed_id , f \"objects?limit= { limit } \" ) paginated_url = url # Use the builtin dbm module to save the cursors to disk with dbm . open ( CURSOR_FILE , \"c\" ) as cursors : while True : # If a cursor exists for this feed, add it to the URL if feed_id in cursors : paginated_url = f \" { url } &cursor= { cursors [ feed_id ] . decode ( 'ascii' ) } \" # Request the next batch of objects from the API, authenticate with the APIKEY response = requests . get ( paginated_url , headers = { \"Authorization\" : f \"Bearer { APIKEY } \" } ) response . raise_for_status () data = response . json () # Yield individual STIX Objects (SDO & SRO) for item in data [ \"items\" ]: yield item # Update cursor to have incremental updates cursors [ feed_id ] = data [ \"next_cursor\" ] # Stop current iteration if we reached the last updated object if not data [ \"items\" ] or len ( data [ \"items\" ]) < limit : break","title":"Example Script"},{"location":"intelligence_center/api/#getting-an-indicators-context","text":"When using indicators for detection, you might want to fetch all indicators by using a filter on the indicator type and then only fetch additional context when an indicator matched. The v2/inthreat/objects/{indicator_id}/context returns a STIX bundle containing: The indicator The first level relationships and related objects Potential relevant Course-Of-Action objects Referenced Sources and Object Markings","title":"Getting an indicator's context"},{"location":"intelligence_center/api/#getting-an-object-by-its-id","text":"It is possible to get a specific object by its ID by using the GET v2/inthreat/objects/{object_id} endpoint. For relationships, use the GET v2/inthreat/relationships/{relationship_id} endpoint instead.","title":"Getting an object by its ID"},{"location":"intelligence_center/api/#looking-for-an-ioc","text":"It is possible to look for a specific indicator of compromise in the Intelligence Center and get its context with the GET v2/inthreat/indicators/context endpoint (see documentation ). import json import requests from posixpath import join as urljoin APIKEY = \"APIKEY\" BASE_URL = \"https://api.sekoia.io/v2/inthreat/\" HEADERS = { \"Authorization\" : f \"Bearer { APIKEY } \" } def get_indicator_context ( observable_type , observable_value ): response = requests . get ( urljoin ( BASE_URL , \"indicators/context\" ), params = { \"type\" : observable_type , \"value\" : observable_value }, headers = HEADERS ) response . raise_for_status () return response . json ()","title":"Looking for an IOC"},{"location":"intelligence_center/api/#taxii","text":"The Intelligence Center also exposes a TAXII 2.1 server that conforms to the specification . The discovery endpoint of the TAXII server is located at https://app.sekoia.io/api/v2/inthreat/taxii-server/taxii2 . The API root of the TAXII server is located at https://app.sekoia.io/api/v2/inthreat/taxii-server . TAXII relies on HTTP Basic Authentication. To authenticate to the TAXII server, you should use your Intelligence Center API key as the Basic Authentication password (you can use any username). Here is an example of a script using the TAXII endpoints to iterate over objects: import dbm import requests from basicauth import encode as basicauth_encode # Replace with Intelligence Center API Key APIKEY = \"APIKEY\" # Replace with the TAXII URL copied from the \"feeds\" page FEED_URL = \"https://app.sekoia.io/api/v2/inthreat/taxii-server/collections/d6092c37-d8d7-45c3-8aff-c4dc26030608/objects\" CURSOR_FILE = \"cursor.db\" def get_new_objects ( feed_url = FEED_URL , limit = 2000 , version = \"2.1\" ): has_more = True headers = { \"Accept\" : \"application/taxii+json\" , \"Authorization\" : basicauth_encode ( \"api\" , APIKEY ), } parameters = { \"match[spec_version]\" : version , \"limit\" : limit } # Use the builtin dbm module to save the cursors to disk with dbm . open ( CURSOR_FILE , \"c\" ) as cursors : while has_more : # If a cursor exists for this feed, add it to the parameters if feed_url in cursors : parameters [ \"next\" ] = cursors [ feed_url ] . decode ( \"ascii\" ) # Request the next batch of objects from the API, authenticate with the APIKEY response = requests . get ( feed_url , headers = headers , params = parameters ) response . raise_for_status () data = response . json () has_more = data [ \"more\" ] cursors [ feed_url ] = data [ \"next\" ] yield from data [ \"objects\" ]","title":"TAXII"},{"location":"intelligence_center/dashboard/","text":"Dashboards Default Dashboard When connecting to the Intelligence Center, and if the permissions granted allow it, you arrive on the \"Dashboard\" page. This allows you to have a quick view of the content of the Intelligence Center as well as the most recently added information. This functionality is available to all customers who subscribe to the Intelligence Center. Create and edit dashboards The Default Dashboard cannot be modified. However, each user has the possibility of creating other dashboards, in order to obtain filtered views of data relevant to their use. You also have the possibility to duplicate an existing dashboard and to edit the resulting new one. It is thus possible to create dashboards by sector of activity or by geographical area and the number of dashboards is not limited. Note Each dashboard you create will be available to all members of your community.","title":"Dashboards"},{"location":"intelligence_center/dashboard/#dashboards","text":"","title":"Dashboards"},{"location":"intelligence_center/dashboard/#default-dashboard","text":"When connecting to the Intelligence Center, and if the permissions granted allow it, you arrive on the \"Dashboard\" page. This allows you to have a quick view of the content of the Intelligence Center as well as the most recently added information. This functionality is available to all customers who subscribe to the Intelligence Center.","title":"Default Dashboard"},{"location":"intelligence_center/dashboard/#create-and-edit-dashboards","text":"The Default Dashboard cannot be modified. However, each user has the possibility of creating other dashboards, in order to obtain filtered views of data relevant to their use. You also have the possibility to duplicate an existing dashboard and to edit the resulting new one. It is thus possible to create dashboards by sector of activity or by geographical area and the number of dashboards is not limited. Note Each dashboard you create will be available to all members of your community.","title":"Create and edit dashboards"},{"location":"intelligence_center/data_export/","text":"Data Export When on an Object's page or when looking at a Report, you can export related items directly from the application and download them in the CSV or JSON format. The export modal lets you to select: If you want to export all relationships or select a specific type The format of the created file (CSV or JSON) The CSV format will convert some values (such as the TLP, sources and kill chain phases) to a human-readable format instead of IDs.","title":"Data Export"},{"location":"intelligence_center/data_export/#data-export","text":"When on an Object's page or when looking at a Report, you can export related items directly from the application and download them in the CSV or JSON format. The export modal lets you to select: If you want to export all relationships or select a specific type The format of the created file (CSV or JSON) The CSV format will convert some values (such as the TLP, sources and kill chain phases) to a human-readable format instead of IDs.","title":"Data Export"},{"location":"intelligence_center/data_model/","text":"Data Model The Intelligence Center uses the industry standard STIX to represent information, in its upcoming 2.1 version . STIX uses JSON objects with pre-defined schemas to represent Cyber Threat Intelligence data. The knowledge graph is based on nodes (STIX Domain Objects or SDO) and relationships (STIX Relationship Objects or SRO). The Intelligence Center supports the following STIX Domain Objects: Attack Pattern Campaign Course of Action Identity Indicator Intrusion Set Location Malware Report Threat Actor Tool Vulnerability External Sources One of the founding principle of the Intelligence Center is the consolidation of information coming from several sources. Sources are represented in STIX by Identity objects. Our consolidation strategy means that the created_by_ref field of the STIX objects will always be set to the SEKOIA identity. The sources that contributed to one of our STIX object are available, as references, in the x_inthreat_sources_refs custom field. As an exemple, here are parts of the Spearphishing Link object presented in the screenshot: { \"type\": \"attack-pattern\", \"name\": \"Spearphishing Link\", \"id\": \"attack-pattern--6cd1a813-ccdf-4ba0-9b54-cb808f1059cc\", \"created_by_ref\": \"identity--357447d7-9229-4ce1-b7fa-f1b83587048e\", # SEKOIA \"x_inthreat_sources_refs\": [ \"identity--357447d7-9229-4ce1-b7fa-f1b83587048e\", # SEKOIA \"identity--c78cb6e5-0c4b-4611-8297-d1b8b55e40b5\" # The MITRE Corporation ], [...] } Confidence STIX 2.1 adds an optional confidence field for an object creator to express how confident (s)he is about the information. The Intelligence Center uses the confidence field in two ways: On objects, the confidence score may be specified to express a specific confidence level on an object. When specified, this confidence level should be read with the Admiralty Credibility scale. Score Label Explanation 1 Confirmed by other sources Confirmed by other independent sources; logical in itself; Consistent with other information on the subject 2 Probably True Not confirmed; logical in itself; consistent with other information on the subject 3 Possibly True Not confirmed; reasonably logical in itself; agrees with some other information on the subject 4 Doubtful Not confirmed; possible but not logical; no other information on the subject 5 Improbable Not confirmed; not logical in itself; contradicted by other information on the subject 6 Truth cannot be judged No basis exists for evaluating the validity of the information On source objects (of type Identity ), the confidence score may be specified to express the source's reliability. When specified, this confidence level should be read with the Admiralty Reliability scale. Score Label Explanation A Completely reliable No doubt of authenticity, trustworthiness, or competency; has a history of complete reliability B Usually reliable Minor doubt about authenticity, trustworthiness, or competency; has a history of valid information most of the time C Fairly reliable Doubt of authenticity, trustworthiness, or competency but has provided valid information in the past D Not usually reliable Significant doubt about authenticity, trustworthiness, or competency but has provided valid information in the past E Unreliable Lacking in authenticity, trustworthiness, and competency; history of invalid information F Reliability cannot be judged No basis exists for evaluating the reliability of the source","title":"Data Model"},{"location":"intelligence_center/data_model/#data-model","text":"The Intelligence Center uses the industry standard STIX to represent information, in its upcoming 2.1 version . STIX uses JSON objects with pre-defined schemas to represent Cyber Threat Intelligence data. The knowledge graph is based on nodes (STIX Domain Objects or SDO) and relationships (STIX Relationship Objects or SRO). The Intelligence Center supports the following STIX Domain Objects: Attack Pattern Campaign Course of Action Identity Indicator Intrusion Set Location Malware Report Threat Actor Tool Vulnerability","title":"Data Model"},{"location":"intelligence_center/data_model/#external-sources","text":"One of the founding principle of the Intelligence Center is the consolidation of information coming from several sources. Sources are represented in STIX by Identity objects. Our consolidation strategy means that the created_by_ref field of the STIX objects will always be set to the SEKOIA identity. The sources that contributed to one of our STIX object are available, as references, in the x_inthreat_sources_refs custom field. As an exemple, here are parts of the Spearphishing Link object presented in the screenshot: { \"type\": \"attack-pattern\", \"name\": \"Spearphishing Link\", \"id\": \"attack-pattern--6cd1a813-ccdf-4ba0-9b54-cb808f1059cc\", \"created_by_ref\": \"identity--357447d7-9229-4ce1-b7fa-f1b83587048e\", # SEKOIA \"x_inthreat_sources_refs\": [ \"identity--357447d7-9229-4ce1-b7fa-f1b83587048e\", # SEKOIA \"identity--c78cb6e5-0c4b-4611-8297-d1b8b55e40b5\" # The MITRE Corporation ], [...] }","title":"External Sources"},{"location":"intelligence_center/data_model/#confidence","text":"STIX 2.1 adds an optional confidence field for an object creator to express how confident (s)he is about the information. The Intelligence Center uses the confidence field in two ways: On objects, the confidence score may be specified to express a specific confidence level on an object. When specified, this confidence level should be read with the Admiralty Credibility scale. Score Label Explanation 1 Confirmed by other sources Confirmed by other independent sources; logical in itself; Consistent with other information on the subject 2 Probably True Not confirmed; logical in itself; consistent with other information on the subject 3 Possibly True Not confirmed; reasonably logical in itself; agrees with some other information on the subject 4 Doubtful Not confirmed; possible but not logical; no other information on the subject 5 Improbable Not confirmed; not logical in itself; contradicted by other information on the subject 6 Truth cannot be judged No basis exists for evaluating the validity of the information On source objects (of type Identity ), the confidence score may be specified to express the source's reliability. When specified, this confidence level should be read with the Admiralty Reliability scale. Score Label Explanation A Completely reliable No doubt of authenticity, trustworthiness, or competency; has a history of complete reliability B Usually reliable Minor doubt about authenticity, trustworthiness, or competency; has a history of valid information most of the time C Fairly reliable Doubt of authenticity, trustworthiness, or competency but has provided valid information in the past D Not usually reliable Significant doubt about authenticity, trustworthiness, or competency but has provided valid information in the past E Unreliable Lacking in authenticity, trustworthiness, and competency; history of invalid information F Reliability cannot be judged No basis exists for evaluating the reliability of the source","title":"Confidence"},{"location":"intelligence_center/graph_explorations/","text":"Graph Explorations Graph Explorations enables you to select objects and relationships from the knowledge base and add them to a graphical representation that you can then share with your colleagues. Start an Exploration You can start a Graph Exploration from any object in the Intelligence Center, by clicking on the \"New Graph Exploration\" button at the top right of the page. You can also start an exploration from a Report graph. Add Relationships Clicking on any object on the graph displays related information in the side panel. You can also list relationships with the \"Relationships\" tab. From this list, you can add any relationship to the graph by clicking the hover button. Remove Items You can remove relationships from the graph by clicking on the hover button from the relationship list. Objects can be removed by clicking the red trash icon next to their name. Removing a node automatically removes associated relationships. Save Graph Exploration Whenever you would like to save a newly created graph exploration, or the changes you made to an existing one, click on the Save button in the toolbar. If this is a creation, you will be prompted for a name. List saved Explorations At any time, it is possible to list saved explorations from your community by clicking on the \"Graph Explorations\" item of the menu. From there, you will be able to dive into saved explorations by clicking on any of them.","title":"Graph Explorations"},{"location":"intelligence_center/graph_explorations/#graph-explorations","text":"Graph Explorations enables you to select objects and relationships from the knowledge base and add them to a graphical representation that you can then share with your colleagues.","title":"Graph Explorations"},{"location":"intelligence_center/graph_explorations/#start-an-exploration","text":"You can start a Graph Exploration from any object in the Intelligence Center, by clicking on the \"New Graph Exploration\" button at the top right of the page. You can also start an exploration from a Report graph.","title":"Start an Exploration"},{"location":"intelligence_center/graph_explorations/#add-relationships","text":"Clicking on any object on the graph displays related information in the side panel. You can also list relationships with the \"Relationships\" tab. From this list, you can add any relationship to the graph by clicking the hover button.","title":"Add Relationships"},{"location":"intelligence_center/graph_explorations/#remove-items","text":"You can remove relationships from the graph by clicking on the hover button from the relationship list. Objects can be removed by clicking the red trash icon next to their name. Removing a node automatically removes associated relationships.","title":"Remove Items"},{"location":"intelligence_center/graph_explorations/#save-graph-exploration","text":"Whenever you would like to save a newly created graph exploration, or the changes you made to an existing one, click on the Save button in the toolbar. If this is a creation, you will be prompted for a name.","title":"Save Graph Exploration"},{"location":"intelligence_center/graph_explorations/#list-saved-explorations","text":"At any time, it is possible to list saved explorations from your community by clicking on the \"Graph Explorations\" item of the menu. From there, you will be able to dive into saved explorations by clicking on any of them.","title":"List saved Explorations"},{"location":"intelligence_center/integrations/","text":"Intelligence Center external integrations Intelligence Center data can be consumed using several third party integrations. MISP Feed The default feed is available as a MISP feed. It can be added to an existing MISP instance by following MISP's documentation . The following field values are required for the feed to work properly: Input Source: Network URL: https://api.sekoia.io/api/v2/inthreat/misp Source Format: MISP Feed Headers: Authorization: Bearer <APIKEY> Enabled: True You then need to make sure you have a scheduled task in place to regularly fetch the feed's content. OpenCTI connector An OpenCTI connector is available to consumme a feed. All the instruction to run it are available at the connector GitHub repository: https://github.com/OpenCTI-Platform/connectors/tree/master/sekoia. Cortex Analyser SEKOIA is also providing a Cortex analyzer to enrich data in TheHive ecosystem. To setup the analyzer please follow this guide . In a nutshell: Get the SEKOIA.IO API Key Install the Analyzer refering to this section of the TheHive documentation Connect into Cortex with orgadmin role Select your organization on the top right corner Move to Analyser Config and search sekoia Select SEKOIAIntelligenceCenter Provide simple configurations Enable the Analyzer you would like to use, by clicking on the right side If wanted, tailor made your Analyzer with additional details SEKOIA.IO App for Splunk An App for Splunk is available to detect threats in your logs based on our feed. You can find the download links and additional information on the dedicated GitHub repository .","title":"External Integrations"},{"location":"intelligence_center/integrations/#intelligence-center-external-integrations","text":"Intelligence Center data can be consumed using several third party integrations.","title":"Intelligence Center external integrations"},{"location":"intelligence_center/integrations/#misp-feed","text":"The default feed is available as a MISP feed. It can be added to an existing MISP instance by following MISP's documentation . The following field values are required for the feed to work properly: Input Source: Network URL: https://api.sekoia.io/api/v2/inthreat/misp Source Format: MISP Feed Headers: Authorization: Bearer <APIKEY> Enabled: True You then need to make sure you have a scheduled task in place to regularly fetch the feed's content.","title":"MISP Feed"},{"location":"intelligence_center/integrations/#opencti-connector","text":"An OpenCTI connector is available to consumme a feed. All the instruction to run it are available at the connector GitHub repository: https://github.com/OpenCTI-Platform/connectors/tree/master/sekoia.","title":"OpenCTI connector"},{"location":"intelligence_center/integrations/#cortex-analyser","text":"SEKOIA is also providing a Cortex analyzer to enrich data in TheHive ecosystem. To setup the analyzer please follow this guide . In a nutshell: Get the SEKOIA.IO API Key Install the Analyzer refering to this section of the TheHive documentation Connect into Cortex with orgadmin role Select your organization on the top right corner Move to Analyser Config and search sekoia Select SEKOIAIntelligenceCenter Provide simple configurations Enable the Analyzer you would like to use, by clicking on the right side If wanted, tailor made your Analyzer with additional details","title":"Cortex Analyser"},{"location":"intelligence_center/integrations/#sekoiaio-app-for-splunk","text":"An App for Splunk is available to detect threats in your logs based on our feed. You can find the download links and additional information on the dedicated GitHub repository .","title":"SEKOIA.IO App for Splunk"},{"location":"intelligence_center/observables/","text":"Observables What is interesting about the Observables page? This page provides a quick and efficient search engine for technical information available within the Intelligence Center. Use Case A classic use case is when you are looking for any information regarding an IP address, domain name, url or file hash. It will give you an answer with the potential related threat or/and associated tag. Tag Tag information has a time validity and provides some technical enrichment depending of the type of the observable: geolocation, internet provider, reputation (e.g. scanner) How are the observables produced? Technical information is automatically extracted from various sources: public, subscriptions, partners, SEKOIA internal analysis. Depending of the source, a tag name could be associated with a valid_from and valid_until timestamp, providing an up-to-date technical information directly integrated to the Intelligence Center database: an IP address could be enriched with the tag scanner once, then have the tag expired if that IP address scanning activity is no longer observed. An observable also have relationships with other observables: an IP address could belongs-to an subnet object, and have a url hosted-on . It could become an indicator of compromise with its associated threat (e.g. malware, campaign). The main features of the observables page Type (examples) Autonomous system x509 Certificate Directory Domain name Email addr File Filename Ipv4 addr Ipv6 addr Mac addr Mutex Organization Text URL Windows registry key Source (examples) SEKOIA SEKOIA C2 Tracker SEKOIA Malware Watcher Tria.ge URLHaus PhishTank MalwareBazaar Tag (examples) amazon_aws (Amazon AWS IP Ranges) cloudflare (Cloudflare IPv4) country:* (Country) crl or ocsp (CRL and OCSP Domain Names) cryptomining (Domain Names related to Cryptomining activity) disposable_email (List of domain names providing disposable email services) domains_top_1_000_000 (Top 1M domain names) domains_top_100_000 (Top 100k domain names) domains_top_10_000 (Top 10k domain names) dynamic-dns (Top 5000 dynamic malicious domains) google (Domain Names used by Google products) googlebot (IP Addresses used by Google Bot) iplookup (IP Lookup Services) multicast (RFC 5771 multicast CIDR blocks) office365 (Office 365 IP Ranges and Domains) ovh_webhosting (Addresses IP OVH Web Hosting - Shared) rfc1918 (RFC1918 - Private Addresses) rfc5735 (RFC5735 - Special Use Addresses) rfc6598 (RFC6598 - Shared Address Space) rfc6761 (RFC6761 - Special Use Domain Names) scanner:* (Hosts involved in mass scanning and/or exploitation attempts) security_vendor (Security Vendor Blogs) sinkhole (Brakmic Sinkholes) tor (Tor Exit Nodes) university (University Domain Names and Websites) url_shortener (URL Shorteners) Searches You could proceed with bulk research, one observable on each line. The result page will give you two tabs, one for the known the other for the unknown with the potential associated tags, threats related. When clicking on an Observable, a dedicated page will display information, raw object and sometimes relationships as shown bellow: Example Use Case You found some domain names during an investigation and you want to know if those observables are known in the Intelligence Center and if there is more context to it. Simply paste the domain names into the search fields and hit enter. In the Known tab you will find observables known in the Intelligence Center and some context over it if there is some. The Unknown tab will contain the observables never seen in the Intelligence Center. Observable vs IoC (Indicator of Compromise) Observable are derived from all data contained in the Intelligence Center, they are not always contextualized. On the other hand, IoCs represent malicious activity link to a Campaign, an Intrusion Set, a Malware, a Tool, a TTP or an infrastructure of attack.","title":"Observables"},{"location":"intelligence_center/observables/#observables","text":"","title":"Observables"},{"location":"intelligence_center/observables/#what-is-interesting-about-the-observables-page","text":"This page provides a quick and efficient search engine for technical information available within the Intelligence Center.","title":"What is interesting about the Observables page?"},{"location":"intelligence_center/observables/#use-case","text":"A classic use case is when you are looking for any information regarding an IP address, domain name, url or file hash. It will give you an answer with the potential related threat or/and associated tag.","title":"Use Case"},{"location":"intelligence_center/observables/#tag","text":"Tag information has a time validity and provides some technical enrichment depending of the type of the observable: geolocation, internet provider, reputation (e.g. scanner)","title":"Tag"},{"location":"intelligence_center/observables/#how-are-the-observables-produced","text":"Technical information is automatically extracted from various sources: public, subscriptions, partners, SEKOIA internal analysis. Depending of the source, a tag name could be associated with a valid_from and valid_until timestamp, providing an up-to-date technical information directly integrated to the Intelligence Center database: an IP address could be enriched with the tag scanner once, then have the tag expired if that IP address scanning activity is no longer observed. An observable also have relationships with other observables: an IP address could belongs-to an subnet object, and have a url hosted-on . It could become an indicator of compromise with its associated threat (e.g. malware, campaign).","title":"How are the observables produced?"},{"location":"intelligence_center/observables/#the-main-features-of-the-observables-page","text":"","title":"The main features of the observables page"},{"location":"intelligence_center/observables/#type-examples","text":"Autonomous system x509 Certificate Directory Domain name Email addr File Filename Ipv4 addr Ipv6 addr Mac addr Mutex Organization Text URL Windows registry key","title":"Type (examples)"},{"location":"intelligence_center/observables/#source-examples","text":"SEKOIA SEKOIA C2 Tracker SEKOIA Malware Watcher Tria.ge URLHaus PhishTank MalwareBazaar","title":"Source (examples)"},{"location":"intelligence_center/observables/#tag-examples","text":"amazon_aws (Amazon AWS IP Ranges) cloudflare (Cloudflare IPv4) country:* (Country) crl or ocsp (CRL and OCSP Domain Names) cryptomining (Domain Names related to Cryptomining activity) disposable_email (List of domain names providing disposable email services) domains_top_1_000_000 (Top 1M domain names) domains_top_100_000 (Top 100k domain names) domains_top_10_000 (Top 10k domain names) dynamic-dns (Top 5000 dynamic malicious domains) google (Domain Names used by Google products) googlebot (IP Addresses used by Google Bot) iplookup (IP Lookup Services) multicast (RFC 5771 multicast CIDR blocks) office365 (Office 365 IP Ranges and Domains) ovh_webhosting (Addresses IP OVH Web Hosting - Shared) rfc1918 (RFC1918 - Private Addresses) rfc5735 (RFC5735 - Special Use Addresses) rfc6598 (RFC6598 - Shared Address Space) rfc6761 (RFC6761 - Special Use Domain Names) scanner:* (Hosts involved in mass scanning and/or exploitation attempts) security_vendor (Security Vendor Blogs) sinkhole (Brakmic Sinkholes) tor (Tor Exit Nodes) university (University Domain Names and Websites) url_shortener (URL Shorteners)","title":"Tag (examples)"},{"location":"intelligence_center/observables/#searches","text":"You could proceed with bulk research, one observable on each line. The result page will give you two tabs, one for the known the other for the unknown with the potential associated tags, threats related. When clicking on an Observable, a dedicated page will display information, raw object and sometimes relationships as shown bellow:","title":"Searches"},{"location":"intelligence_center/observables/#example-use-case","text":"You found some domain names during an investigation and you want to know if those observables are known in the Intelligence Center and if there is more context to it. Simply paste the domain names into the search fields and hit enter. In the Known tab you will find observables known in the Intelligence Center and some context over it if there is some. The Unknown tab will contain the observables never seen in the Intelligence Center.","title":"Example Use Case"},{"location":"intelligence_center/observables/#observable-vs-ioc-indicator-of-compromise","text":"Observable are derived from all data contained in the Intelligence Center, they are not always contextualized. On the other hand, IoCs represent malicious activity link to a Campaign, an Intrusion Set, a Malware, a Tool, a TTP or an infrastructure of attack.","title":"Observable vs IoC (Indicator of Compromise)"},{"location":"operation_center/","text":"Operation Center The Operation Center allows you to integrate and analyze in real time the events produced by your systems. The detection strategy is based on the enhancement of SEKOIA's threat intelligence knowledge base, as well as on the modeling of specific threat scenarios.","title":"Overview"},{"location":"operation_center/#operation-center","text":"The Operation Center allows you to integrate and analyze in real time the events produced by your systems. The detection strategy is based on the enhancement of SEKOIA's threat intelligence knowledge base, as well as on the modeling of specific threat scenarios.","title":"Operation Center"},{"location":"operation_center/actions/","text":"Actions Types of Actions Once the user has selected and configured a Trigger , most of the time an Operator will follow, then it is time to insert a first Action . The Actions execute specific activities depending on the needs. You will find hereafter the main Actions that a user will be able to use. Sekoia suggestions Getters Get an Alert or Get a Case Note: Copy-past in the research section: get_an_alert and get_a_case to retreive the related Blocks . Allows a user to collect information on an Alert or a Case (incident) such as, but not limited to, the: Created (at, by) and update (at, by) Rule that raised the alert (uuid, name, type, pattern, severity, description) Related entity Source and Target Status (uuid, name, description) Urgency (value, severity, criticity, current_value) Comments (uuid, date, author, content) Countermeasures (comments, actions steps) Is_incident status Get the list of comments of a case or the comment of a case Note: Copy-past in the research section: get_the_list_of_comments_of_a_case and get_the_comment_of_a_case to retreive the related Blocks . In order to exploit the comment posted on a case, it is first needed to collect all the meta information of the needed comment(s): The only mandatory information needed is the case_uuid that you can collect by using the Get a Case module. Then the user can exploit: the number of comments, uuid, created_at, created_by, created_by_type, content of the comment. Read JSON File Note: Copy-past in the research section: Read JSON File to retreive the related Block . With this module, a user can collect the output of the Action > Get an alert when the parameter STIX is activated. Here is an example of use: Setters On the playbook GUI, it is possible to set Blocks such as Create, Update, Patch, Add, Post, Delete activities. Create Blocks With the Create Blocks a user will be able to: Create a case: creates_a_new_case Create an alert: create_an_alert Create asset: creates_a_new_asset Add owners to an asset: adds_owners_to_an_asset Add key to the asset: adds_an_key_to_the_asset Add attribute to the asset: : adds_an_attribute_to_the_asset Update Blocks With the Update Blocks a user will be able to: Update a case: updates_an_case Patch an alert: patch_an_alert Delete Blocks With the Delete Blocks a user will be able to: Delete a comment on an alert: delete_a_comment_from_an_alert Delete an asset: deletes_an_asset Delete an owner from an asset: deletes_an_owner_from_an_asset Delete a key from an asset: deletes_an_key_from_an_asset Delete an attribute form an asset: deletes_an_attribute_from_an_asset Trigger an action on the event workflow Is is possible to trigger an action on an alert as explained in the webhook section. For instance the following image shows how to properly close an alert afer analysing it. As of today, the available action_uuid are the following : Action : \"Acknowledge\" , Description : \"Acknowledge the alert\" , action_uuid : \"937bdabf-6a08-434b-b6d3-d7447e4e452a\" Action : \"Validate\" , Description : \"Validate the alert\" , action_uuid : \"c39a0a95-aa2c-4d0d-8d2e-d3decf426eea\" Action : \"Reject\" , Description : \"Reject the alert\" , action_uuid : \"ade85d7b-7507-4026-bfc6-cc006d10ddac\" Action : \"Close\" , Description : \"Close the alert\" , action_uuid : \"1390be4e-ced8-4dd6-9bed-573471b235ab\" Other Sekoia Blocks are available, such as: Associate new rule on a case Associate new alert on a case Activate or deny a countermeasure Fortigate Blocks With the Fortigate Blocks a user will be able to: - Add an IP in the Address list of a Fortigate FW: Post Fortigate IP Address - Add a FQDN in the Address list of a Fortigate FW: Post Fortigate FQDN Address - Create a Group with an Address member: Post Fortigate Address Group External Data Enrichers Most of the time an analyst uses external tools in order to better understand the severity of the alert. If you have subscribed to one or more of the following online tools, you should be able to get an API key, and simply use it on your playbooks. IKnowWhatYouDownload The module allows to request information such as: IP history IP Check IP List Onyphe The module allows to request information such as: Scan (SYN, Onion...) Lookups (Md5, reverse DNS) Get IP additional information (Threat list, geolocalisation...) Scan Domain Name CTLs RiskIQ The module allows to request information such as: Whois Reverse Whois (organisation, nameserver, name, email, address) Passive DNS (Name, IP, Hex) SSL host, certificate (SHA-1, Serial Number) / DNS Shodan The module allows to request information such as: Search for Host information Get DNS reverse, resolve, domain Virustotal The module allows to request information such as: Scan (URL, IP, Hash, File, Domain) Notifications PagerDuty This module allows the analysts to receive notification in PagerDuty when a new alert is raised. MatterMost Sekoia alert: this module allows the analysts to receive notification in MatterMost when a new alert is raised. Post a message: additional information can be rooted on a specific channel with detailed context Mandrill Email notification is available, the following example shows an automatic email sent when a new alert is raised. The mandatory fields are: The recipient email address, name The sender email address, name The subject The HTML mail content, here is a simple example: < h1 > A new alert is declared: {{ node.2.rule.name }}. </ h1 > < h2 > Description: {{ node.2.rule.description }}. </ h2 > < p > More details: </ p > < ul >< li > Urgency: {{ node.2.urgency.value }} </ li > < li > Entity: {{ node.2.entity.name }} </ li > < li > Kill chain: {{ node.2.kill_chain_short_id }} </ li > < li > Source: {{ node.2.source }} </ li > < li > Target: {{ node.2.target }} </ li > </ ul > The dynamic content is written in JINJA. For more information on this language, please follow this documentation .","title":"Actions"},{"location":"operation_center/actions/#actions","text":"","title":"Actions"},{"location":"operation_center/actions/#types-of-actions","text":"Once the user has selected and configured a Trigger , most of the time an Operator will follow, then it is time to insert a first Action . The Actions execute specific activities depending on the needs. You will find hereafter the main Actions that a user will be able to use.","title":"Types of Actions"},{"location":"operation_center/actions/#sekoia-suggestions","text":"","title":"Sekoia suggestions"},{"location":"operation_center/actions/#getters","text":"","title":"Getters"},{"location":"operation_center/actions/#get-an-alert-or-get-a-case","text":"Note: Copy-past in the research section: get_an_alert and get_a_case to retreive the related Blocks . Allows a user to collect information on an Alert or a Case (incident) such as, but not limited to, the: Created (at, by) and update (at, by) Rule that raised the alert (uuid, name, type, pattern, severity, description) Related entity Source and Target Status (uuid, name, description) Urgency (value, severity, criticity, current_value) Comments (uuid, date, author, content) Countermeasures (comments, actions steps) Is_incident status","title":"Get an Alert or Get a Case"},{"location":"operation_center/actions/#get-the-list-of-comments-of-a-case-or-the-comment-of-a-case","text":"Note: Copy-past in the research section: get_the_list_of_comments_of_a_case and get_the_comment_of_a_case to retreive the related Blocks . In order to exploit the comment posted on a case, it is first needed to collect all the meta information of the needed comment(s): The only mandatory information needed is the case_uuid that you can collect by using the Get a Case module. Then the user can exploit: the number of comments, uuid, created_at, created_by, created_by_type, content of the comment.","title":"Get the list of comments of a case or the comment of a case"},{"location":"operation_center/actions/#read-json-file","text":"Note: Copy-past in the research section: Read JSON File to retreive the related Block . With this module, a user can collect the output of the Action > Get an alert when the parameter STIX is activated. Here is an example of use:","title":"Read JSON File"},{"location":"operation_center/actions/#setters","text":"On the playbook GUI, it is possible to set Blocks such as Create, Update, Patch, Add, Post, Delete activities.","title":"Setters"},{"location":"operation_center/actions/#create-blocks","text":"With the Create Blocks a user will be able to: Create a case: creates_a_new_case Create an alert: create_an_alert Create asset: creates_a_new_asset Add owners to an asset: adds_owners_to_an_asset Add key to the asset: adds_an_key_to_the_asset Add attribute to the asset: : adds_an_attribute_to_the_asset","title":"Create Blocks"},{"location":"operation_center/actions/#update-blocks","text":"With the Update Blocks a user will be able to: Update a case: updates_an_case Patch an alert: patch_an_alert","title":"Update Blocks"},{"location":"operation_center/actions/#delete-blocks","text":"With the Delete Blocks a user will be able to: Delete a comment on an alert: delete_a_comment_from_an_alert Delete an asset: deletes_an_asset Delete an owner from an asset: deletes_an_owner_from_an_asset Delete a key from an asset: deletes_an_key_from_an_asset Delete an attribute form an asset: deletes_an_attribute_from_an_asset","title":"Delete Blocks"},{"location":"operation_center/actions/#trigger-an-action-on-the-event-workflow","text":"Is is possible to trigger an action on an alert as explained in the webhook section. For instance the following image shows how to properly close an alert afer analysing it. As of today, the available action_uuid are the following : Action : \"Acknowledge\" , Description : \"Acknowledge the alert\" , action_uuid : \"937bdabf-6a08-434b-b6d3-d7447e4e452a\" Action : \"Validate\" , Description : \"Validate the alert\" , action_uuid : \"c39a0a95-aa2c-4d0d-8d2e-d3decf426eea\" Action : \"Reject\" , Description : \"Reject the alert\" , action_uuid : \"ade85d7b-7507-4026-bfc6-cc006d10ddac\" Action : \"Close\" , Description : \"Close the alert\" , action_uuid : \"1390be4e-ced8-4dd6-9bed-573471b235ab\" Other Sekoia Blocks are available, such as: Associate new rule on a case Associate new alert on a case Activate or deny a countermeasure","title":"Trigger an action on the event workflow"},{"location":"operation_center/actions/#fortigate-blocks","text":"With the Fortigate Blocks a user will be able to: - Add an IP in the Address list of a Fortigate FW: Post Fortigate IP Address - Add a FQDN in the Address list of a Fortigate FW: Post Fortigate FQDN Address - Create a Group with an Address member: Post Fortigate Address Group","title":"Fortigate Blocks"},{"location":"operation_center/actions/#external-data-enrichers","text":"Most of the time an analyst uses external tools in order to better understand the severity of the alert. If you have subscribed to one or more of the following online tools, you should be able to get an API key, and simply use it on your playbooks.","title":"External Data Enrichers"},{"location":"operation_center/actions/#iknowwhatyoudownload","text":"The module allows to request information such as: IP history IP Check IP List","title":"IKnowWhatYouDownload"},{"location":"operation_center/actions/#onyphe","text":"The module allows to request information such as: Scan (SYN, Onion...) Lookups (Md5, reverse DNS) Get IP additional information (Threat list, geolocalisation...) Scan Domain Name CTLs","title":"Onyphe"},{"location":"operation_center/actions/#riskiq","text":"The module allows to request information such as: Whois Reverse Whois (organisation, nameserver, name, email, address) Passive DNS (Name, IP, Hex) SSL host, certificate (SHA-1, Serial Number) / DNS","title":"RiskIQ"},{"location":"operation_center/actions/#shodan","text":"The module allows to request information such as: Search for Host information Get DNS reverse, resolve, domain","title":"Shodan"},{"location":"operation_center/actions/#virustotal","text":"The module allows to request information such as: Scan (URL, IP, Hash, File, Domain)","title":"Virustotal"},{"location":"operation_center/actions/#notifications","text":"","title":"Notifications"},{"location":"operation_center/actions/#pagerduty","text":"This module allows the analysts to receive notification in PagerDuty when a new alert is raised.","title":"PagerDuty"},{"location":"operation_center/actions/#mattermost","text":"Sekoia alert: this module allows the analysts to receive notification in MatterMost when a new alert is raised. Post a message: additional information can be rooted on a specific channel with detailed context","title":"MatterMost"},{"location":"operation_center/actions/#mandrill","text":"Email notification is available, the following example shows an automatic email sent when a new alert is raised. The mandatory fields are: The recipient email address, name The sender email address, name The subject The HTML mail content, here is a simple example: < h1 > A new alert is declared: {{ node.2.rule.name }}. </ h1 > < h2 > Description: {{ node.2.rule.description }}. </ h2 > < p > More details: </ p > < ul >< li > Urgency: {{ node.2.urgency.value }} </ li > < li > Entity: {{ node.2.entity.name }} </ li > < li > Kill chain: {{ node.2.kill_chain_short_id }} </ li > < li > Source: {{ node.2.source }} </ li > < li > Target: {{ node.2.target }} </ li > </ ul > The dynamic content is written in JINJA. For more information on this language, please follow this documentation .","title":"Mandrill"},{"location":"operation_center/alerts/","text":"Alerts Management Alerts are created by the Operation Center when any threat is detected (when an event matches a rule). Concepts Alert Status and lifecycle There is five possible statuses for an alert: Pending : As soon as an alert is triggered, it is placed in 'Pending' status. If the generation mode for this alert is 'Automatic', this status changes automatically to 'Ongoing'. In other cases, the following actions are accepted: Acknowledge, Reject, Validate. Acknowledged : This status is used when an analysis is ongoing. If the analyst can decide if an alert is a true or a false positive quickly, this status can be optional, time to acknowledge used in statistics will be set to time to change to Ongoing or Rejected status.The following actions are accepted: Validate, Reject. Ongoing : Alert is considered as true positive and countermeasures are not yet been applied. This status is the first one seen in case of automatic mode. The following actions are accepted: Close (countermeasures have been applied, no more alert), Reject (after more analysis, this alert was a false positive). Closed : Every necessary actions have been applied for the alert. This status is a final status, no action accepted. Rejected : The alert was a false positive. This status is a final status, no action accepted. Alert Urgency The Urgency is a number used to give a score to the risk associated with a specific alert. It is calculated from the severity of a rule and the criticality of assets related to the alert. This gives a value between 1 (very low risk) and 100 (very high risk). The urgency is provided under two different representations on alert detail: a numerical and a textual representation. Display Value Low [0-20[ Moderate [20-40[ High [40-60[ Major [60-80[ Urgent [80-100] Alert Types and Categories The Alert Type is associated with the rule, but can be changed with the value associated to specific indicators in case of CTI rules. The Alert Type is defined according to a custom set of values derived from theReference Incident Classification Taxonomy of ENISA: abusive-usage bandwidth-download bandwidth-upload p2p high-drop abusive-content spam harmfull-speech violence malicious-code virus worm trojan spyware dialer rootkit malware botnet-drone ransomware malware-configuration c&c information-gathering scanner sniffing social-engineering portscan sweepscan appscan intrusion-attempts ids-alert brute-force exploit intrusions privileged-account-compromise unprivileged-account-compromise application-compromise bot defacement compromised backdoor availability dos ddos sabotage outage information-content-security Unauthorised-information-access Unauthorised-information-modification fraud unauthorized-use-of-resources copyright masquerade phishing vulnerable vulnerable-service other blacklist unknown other test test Alert List When you first connect to SEKOIA.IO, the alerts list will diplay the last 10 alerts raised on your community ordered by Date. Here is an example of this page populated with various alerts: On this screen, you can observe the following information: From the left to the right, 7 features are availables on the top screen: Refresh the page, with new alerts generated (a figure will increment on the button) Select All existing alerts to be listed Select only the Today 's alerts Change the column display fields such as hidding the Entity, or Adding the Source or Target Display the alerts by Most Frequent , Recently Updated , Recently Created or Most Urgent Display the alerts by Status Use Advanced filter capacities, as shown in the picture bellow: On the main alert listing, it is also possible to: - Select multiple alerts in the same time, then choose and apply a massive change of status - Filter in all rules with the same Rule Name - Get more information on the Threats by clicking on it, so you should be redirected on the Intelligence Center focused on this specific Threat By default, the alert listing displays the following information ( more information on the meaning of these fields is provided after the Alert Details section ): - A Selector so you can perform an action on multiple alerts on the same time - A Similarity counter, showing the alert was raised multiple time for the same reason, rather than simply adding a new line on the alert feed - Alert creation Date - Alert Status . When hovering over the icon, a frame indicates the status modification date and who modified it - The Entity to which the alert corresponds - Urgency of the alert, between 0 and 100, 100 being the highest - The Type of alert - The Name of the rule which triggered the alert - The Threats related to the alert in termes of malicious activities, related tools, Campaign... Your custom configuration will be saved in order to allow you to keep your customer filters when you will come back to this page. For Partners, an additional filter is available in order to display all or a subset of alerts related to its managed communities. The alert listing also displays the Communities related to the alerts. Alert Details The Alert Details can be reached by clicking on the line of any alert in the Alert List. This will provide you with a new view for quick to deep investigations, as shown bellow: The Alert Details header contains the Urgency, the Name and the Alert's Short ID as well as the following actions: Alert Status : can be used to move the Alert through its lifecycle Add to Case Playbooks : display the list of on-demand playbooks and be able to trigger them The different sections of the page are then separated into 4 tabs: Details , Tasks , Events and Graph Investigation . Details The Details tab contains the information needed to understand what the alert is about and why it was raised. It is split into two columns. The first column contains the following items: The detailed alert urgency The alert type The impacted assets The impacted entity Related cases (cases in which this alert appears) The Kill Chain phase Details about the rule that was triggered Details from the Intelligence Center about all threats linked to this alert The urgency and kill chain phase can be edited on hover. The second column is the timeline. It contains all items that constitute the history of the alert: Alert status changes Comments, that can be added with the button at the top right Events represented by their Smart Descriptions Completed Tasks Rejected Tasks It is possible to filter the timeline to display only items of a certain type. Tasks The Tasks tabs lists tasks and subtasks that are associated with the alert. Buttons are available to complete or reject tasks. When completing a task, you are informing the Operation Center that you have undertaken the corresponding action on your perimeter. The urgency of the alert will thereby decrease, as well as the risk indicator. Subtasks can have an OpenC2 specification which can be displayed by clicking on the automation icon. Events The Events tab lists the events that raised the alert in a display similar to the Events page . When interracting with individual values, it is possible to: Filter for: only applies to the events that raised the alert Filter out: only applies to the events that raised the alert Search events with this value The \"Value Selection\" mode can be toggled with the button at the top right of the event list in order to select multiple values in displayed events. The selected values can then be used to: Create and Alert Filter Search events with these values Create an Alert Filter Alert Filters can be used to prevent known false positives from raising the same alert in the future. You can create an Alert Filter for the Rule that triggered the alert easily by selecting multiple values and clicking on the Alert Filter button. The filter's pattern is automatically created from selected values. By default, \"Reject the Alert\" is selected to automatically reject the alert after creating the Alert Filter. Search Events with this value The \"Search Events with this value\" feature can be used to perform a search into all events that occured during the alert's timeframe (+- 1 hour). The search query is automatically created from selected values. A side panel opens with the search results, allowing to investigate an alert without leaving its page. Graph Investigation The Graph Tab is presenting the analyst with a graphical visualisation of the Alert. The following items appear on the graph: Observables: these are automatically extracted from events (IP addresses, Domain Names, URLs, User Account, etc.). Observable Relationships: relationships between observables are represented by arrows linking them on the graph. Relationships are extracted from events using the Smart Description definitions. CTI Objects: STIX objects from the Intelligence Center that provide additional context. STIX relationships between Threat Objects, as minded bellow: Threat Intelligence You can access Threat Intelligence by clicking on any CTI object on the graph. The left panel will display its description and let you list its known relationships. Related objects can then be added on the graph to pivot into the Threat Intelligence database. Observables You can access Observable Details by clicking on any Observable on the graph. The left panel will display all events inside the alert related to this observable, with their Smart Description . Full Events can be accessed into the right side panel by clicking on \"Full Events\". It is also possible to Search events with this value by clicking on the button next to the observable's name.","title":"Alerts"},{"location":"operation_center/alerts/#alerts-management","text":"Alerts are created by the Operation Center when any threat is detected (when an event matches a rule).","title":"Alerts Management"},{"location":"operation_center/alerts/#concepts","text":"","title":"Concepts"},{"location":"operation_center/alerts/#alert-status-and-lifecycle","text":"There is five possible statuses for an alert: Pending : As soon as an alert is triggered, it is placed in 'Pending' status. If the generation mode for this alert is 'Automatic', this status changes automatically to 'Ongoing'. In other cases, the following actions are accepted: Acknowledge, Reject, Validate. Acknowledged : This status is used when an analysis is ongoing. If the analyst can decide if an alert is a true or a false positive quickly, this status can be optional, time to acknowledge used in statistics will be set to time to change to Ongoing or Rejected status.The following actions are accepted: Validate, Reject. Ongoing : Alert is considered as true positive and countermeasures are not yet been applied. This status is the first one seen in case of automatic mode. The following actions are accepted: Close (countermeasures have been applied, no more alert), Reject (after more analysis, this alert was a false positive). Closed : Every necessary actions have been applied for the alert. This status is a final status, no action accepted. Rejected : The alert was a false positive. This status is a final status, no action accepted.","title":"Alert Status and lifecycle"},{"location":"operation_center/alerts/#alert-urgency","text":"The Urgency is a number used to give a score to the risk associated with a specific alert. It is calculated from the severity of a rule and the criticality of assets related to the alert. This gives a value between 1 (very low risk) and 100 (very high risk). The urgency is provided under two different representations on alert detail: a numerical and a textual representation. Display Value Low [0-20[ Moderate [20-40[ High [40-60[ Major [60-80[ Urgent [80-100]","title":"Alert Urgency"},{"location":"operation_center/alerts/#alert-types-and-categories","text":"The Alert Type is associated with the rule, but can be changed with the value associated to specific indicators in case of CTI rules. The Alert Type is defined according to a custom set of values derived from theReference Incident Classification Taxonomy of ENISA: abusive-usage bandwidth-download bandwidth-upload p2p high-drop abusive-content spam harmfull-speech violence malicious-code virus worm trojan spyware dialer rootkit malware botnet-drone ransomware malware-configuration c&c information-gathering scanner sniffing social-engineering portscan sweepscan appscan intrusion-attempts ids-alert brute-force exploit intrusions privileged-account-compromise unprivileged-account-compromise application-compromise bot defacement compromised backdoor availability dos ddos sabotage outage information-content-security Unauthorised-information-access Unauthorised-information-modification fraud unauthorized-use-of-resources copyright masquerade phishing vulnerable vulnerable-service other blacklist unknown other test test","title":"Alert Types and Categories"},{"location":"operation_center/alerts/#alert-list","text":"When you first connect to SEKOIA.IO, the alerts list will diplay the last 10 alerts raised on your community ordered by Date. Here is an example of this page populated with various alerts: On this screen, you can observe the following information: From the left to the right, 7 features are availables on the top screen: Refresh the page, with new alerts generated (a figure will increment on the button) Select All existing alerts to be listed Select only the Today 's alerts Change the column display fields such as hidding the Entity, or Adding the Source or Target Display the alerts by Most Frequent , Recently Updated , Recently Created or Most Urgent Display the alerts by Status Use Advanced filter capacities, as shown in the picture bellow: On the main alert listing, it is also possible to: - Select multiple alerts in the same time, then choose and apply a massive change of status - Filter in all rules with the same Rule Name - Get more information on the Threats by clicking on it, so you should be redirected on the Intelligence Center focused on this specific Threat By default, the alert listing displays the following information ( more information on the meaning of these fields is provided after the Alert Details section ): - A Selector so you can perform an action on multiple alerts on the same time - A Similarity counter, showing the alert was raised multiple time for the same reason, rather than simply adding a new line on the alert feed - Alert creation Date - Alert Status . When hovering over the icon, a frame indicates the status modification date and who modified it - The Entity to which the alert corresponds - Urgency of the alert, between 0 and 100, 100 being the highest - The Type of alert - The Name of the rule which triggered the alert - The Threats related to the alert in termes of malicious activities, related tools, Campaign... Your custom configuration will be saved in order to allow you to keep your customer filters when you will come back to this page. For Partners, an additional filter is available in order to display all or a subset of alerts related to its managed communities. The alert listing also displays the Communities related to the alerts.","title":"Alert List"},{"location":"operation_center/alerts/#alert-details","text":"The Alert Details can be reached by clicking on the line of any alert in the Alert List. This will provide you with a new view for quick to deep investigations, as shown bellow: The Alert Details header contains the Urgency, the Name and the Alert's Short ID as well as the following actions: Alert Status : can be used to move the Alert through its lifecycle Add to Case Playbooks : display the list of on-demand playbooks and be able to trigger them The different sections of the page are then separated into 4 tabs: Details , Tasks , Events and Graph Investigation .","title":"Alert Details"},{"location":"operation_center/alerts/#details","text":"The Details tab contains the information needed to understand what the alert is about and why it was raised. It is split into two columns. The first column contains the following items: The detailed alert urgency The alert type The impacted assets The impacted entity Related cases (cases in which this alert appears) The Kill Chain phase Details about the rule that was triggered Details from the Intelligence Center about all threats linked to this alert The urgency and kill chain phase can be edited on hover. The second column is the timeline. It contains all items that constitute the history of the alert: Alert status changes Comments, that can be added with the button at the top right Events represented by their Smart Descriptions Completed Tasks Rejected Tasks It is possible to filter the timeline to display only items of a certain type.","title":"Details"},{"location":"operation_center/alerts/#tasks","text":"The Tasks tabs lists tasks and subtasks that are associated with the alert. Buttons are available to complete or reject tasks. When completing a task, you are informing the Operation Center that you have undertaken the corresponding action on your perimeter. The urgency of the alert will thereby decrease, as well as the risk indicator. Subtasks can have an OpenC2 specification which can be displayed by clicking on the automation icon.","title":"Tasks"},{"location":"operation_center/alerts/#events","text":"The Events tab lists the events that raised the alert in a display similar to the Events page . When interracting with individual values, it is possible to: Filter for: only applies to the events that raised the alert Filter out: only applies to the events that raised the alert Search events with this value The \"Value Selection\" mode can be toggled with the button at the top right of the event list in order to select multiple values in displayed events. The selected values can then be used to: Create and Alert Filter Search events with these values","title":"Events"},{"location":"operation_center/alerts/#create-an-alert-filter","text":"Alert Filters can be used to prevent known false positives from raising the same alert in the future. You can create an Alert Filter for the Rule that triggered the alert easily by selecting multiple values and clicking on the Alert Filter button. The filter's pattern is automatically created from selected values. By default, \"Reject the Alert\" is selected to automatically reject the alert after creating the Alert Filter.","title":"Create an Alert Filter"},{"location":"operation_center/alerts/#search-events-with-this-value","text":"The \"Search Events with this value\" feature can be used to perform a search into all events that occured during the alert's timeframe (+- 1 hour). The search query is automatically created from selected values. A side panel opens with the search results, allowing to investigate an alert without leaving its page.","title":"Search Events with this value"},{"location":"operation_center/alerts/#graph-investigation","text":"The Graph Tab is presenting the analyst with a graphical visualisation of the Alert. The following items appear on the graph: Observables: these are automatically extracted from events (IP addresses, Domain Names, URLs, User Account, etc.). Observable Relationships: relationships between observables are represented by arrows linking them on the graph. Relationships are extracted from events using the Smart Description definitions. CTI Objects: STIX objects from the Intelligence Center that provide additional context. STIX relationships between Threat Objects, as minded bellow:","title":"Graph Investigation"},{"location":"operation_center/alerts/#threat-intelligence","text":"You can access Threat Intelligence by clicking on any CTI object on the graph. The left panel will display its description and let you list its known relationships. Related objects can then be added on the graph to pivot into the Threat Intelligence database.","title":"Threat Intelligence"},{"location":"operation_center/alerts/#observables","text":"You can access Observable Details by clicking on any Observable on the graph. The left panel will display all events inside the alert related to this observable, with their Smart Description . Full Events can be accessed into the right side panel by clicking on \"Full Events\". It is also possible to Search events with this value by clicking on the button next to the observable's name.","title":"Observables"},{"location":"operation_center/assets/","text":"Assets management Assets in SEKOIA.IO can be of different types. The most commonly used are: Computers, identified by an ip address or a hostname Networks, identified by cidr Users, identified by an email address ... The numbers of events and alerts associated with assets for the past 7 days, as well as the risk level, are displayed in the asset list. Assets participate to the security workflow of the community, when an incoming event matches with a defined asset, the event is enriched with assets information. Their criticality will be taken into account when calculating the urgency of an alert, together with the severity of the rule that triggered the alert. When more than one asset match with an event, the highest criticality is used for urgency calculation. Criticality of assets ranges from 0 to 100, 100 being the most critical. Assets also permit the selective application of detection rules: rules can be applied to one, several or all entities, and to one, several or all assets.","title":"Assets"},{"location":"operation_center/assets/#assets-management","text":"Assets in SEKOIA.IO can be of different types. The most commonly used are: Computers, identified by an ip address or a hostname Networks, identified by cidr Users, identified by an email address ... The numbers of events and alerts associated with assets for the past 7 days, as well as the risk level, are displayed in the asset list. Assets participate to the security workflow of the community, when an incoming event matches with a defined asset, the event is enriched with assets information. Their criticality will be taken into account when calculating the urgency of an alert, together with the severity of the rule that triggered the alert. When more than one asset match with an event, the highest criticality is used for urgency calculation. Criticality of assets ranges from 0 to 100, 100 being the most critical. Assets also permit the selective application of detection rules: rules can be applied to one, several or all entities, and to one, several or all assets.","title":"Assets management"},{"location":"operation_center/cases/","text":"Cases Management Cases allow information relating to security records to be shared whith members of your community. A case consists of a title, a description, a severity, and may be associated with alerts from the Operation Center. A case has a lifecycle (currently, open or closed) and can be assigned to one or more people. This feature allows, for example, a security supervision team to escalate alerts to another team for clarification or to group alerts that seem related to facilitate analysis.","title":"Cases"},{"location":"operation_center/cases/#cases-management","text":"Cases allow information relating to security records to be shared whith members of your community. A case consists of a title, a description, a severity, and may be associated with alerts from the Operation Center. A case has a lifecycle (currently, open or closed) and can be assigned to one or more people. This feature allows, for example, a security supervision team to escalate alerts to another team for clarification or to group alerts that seem related to facilitate analysis.","title":"Cases Management"},{"location":"operation_center/entities/","text":"Entities Entities allow a logical grouping of your data sources and the associated alerts. It can be a company site, network zone, or any other grouping that you think is relevant. All fields of entities are editable and mandatory. The Alert generation mode affects the alert processing workflow. There are two generation modes: 'automatic', for which the alerts proceed to the status 'Ongoing' immediately after their creation 'manual', for which the alerts remain in the status 'Pending' until a manual action. A default value for alert generation mode is defined for each entity, which can be override in each detection rule. Under the entity details, you will find the associated data sources. To activate a new intake for your entity, please refer to the intakes documentation.","title":"Entities"},{"location":"operation_center/entities/#entities","text":"Entities allow a logical grouping of your data sources and the associated alerts. It can be a company site, network zone, or any other grouping that you think is relevant. All fields of entities are editable and mandatory. The Alert generation mode affects the alert processing workflow. There are two generation modes: 'automatic', for which the alerts proceed to the status 'Ongoing' immediately after their creation 'manual', for which the alerts remain in the status 'Pending' until a manual action. A default value for alert generation mode is defined for each entity, which can be override in each detection rule. Under the entity details, you will find the associated data sources. To activate a new intake for your entity, please refer to the intakes documentation.","title":"Entities"},{"location":"operation_center/events/","text":"Events Page The events page enables investigation in the latest events received by SEKOIA.IO using the log list, search bar and filters. Log list: The events page displays a list of latest events received by SEKOIA.IO. The default columns are: Timestamp: Event date Event.dialect: Type of intake that sent the event. Description: Smart description with clickable links formatted by SEKOIA.IO to show the most important elements and make them easily accessible, such as IP Address, Type or Name. It is possible to add or remove other columns using the \"Show fields\" tab by: Selecting the desired field in the \"Available field\" section using the \"+\" button. Selecting the field to remove in \"Selected fields\" section using the \"-\" button. Typing the desired field name in search bar at the top of \"Selected fields\". Log lines: Each line of log can be unrolled to show: Details: Detailed information retrieved from the log about event once parsed and elements related to the intake and community. These information can be used in the search bar. STIX: Event as a STIX bundle that will be used by detection engines. Raw event: Event as received by SEKOIA.IO. Search bar: You can search among the list of events by using Dork query langage, for which you can find more information here . Filters: It is possible to use and combine filters in the search bar by using: Smart description: By clicking on the \"+\" button. Details: The query is made on the existing fields in the \"details\" section by clicking on the \"+\" button for one of the items. To go back to the list of logs shown, you need to clear filters and select \"Current events\" Save filters: It is possible to save a query by clicking on the star icon. The period of time is not conserved with it. To check it out, you can click on \"Saved queries\" then select the wanted period of time and press enter to see the events found. Date: It is possible to select the period of time to be taken in consideration while using a filter: Presets: Recommended predefined dates. Dates relatives: Earliest and latest moments to select. Date ranges: Gap between 2 dates. Click on the blue lense to start a full search job. Search history: Each search result lasts 10 min by default and it is possible to configure the retention to reach 1 day (24 hours). When the result is expired, you still have the possibility of replaying it using the saved relative date or a new one. Sharing a search Search job have IDs that are available in the browser address bar. https://app.sekoia.io/sic/events?jobId=2b5ce17f-517e-4dd3-8253-1495a6ba538b You can share your researches with colleagues by sending them these jobIds, which are accessible within your community. Export the results of a search You can easily export the results of a search in CSV or JSON format and choose the fields you want to export. The export will be made to the default folder defined for downloads. Name of the file is optional, if not provided, the file will be named with the uuid of the job search. Note The 'description' field will not be exported.","title":"Events"},{"location":"operation_center/events/#events-page","text":"The events page enables investigation in the latest events received by SEKOIA.IO using the log list, search bar and filters.","title":"Events Page"},{"location":"operation_center/events/#log-list","text":"The events page displays a list of latest events received by SEKOIA.IO. The default columns are: Timestamp: Event date Event.dialect: Type of intake that sent the event. Description: Smart description with clickable links formatted by SEKOIA.IO to show the most important elements and make them easily accessible, such as IP Address, Type or Name. It is possible to add or remove other columns using the \"Show fields\" tab by: Selecting the desired field in the \"Available field\" section using the \"+\" button. Selecting the field to remove in \"Selected fields\" section using the \"-\" button. Typing the desired field name in search bar at the top of \"Selected fields\".","title":"Log list:"},{"location":"operation_center/events/#log-lines","text":"Each line of log can be unrolled to show: Details: Detailed information retrieved from the log about event once parsed and elements related to the intake and community. These information can be used in the search bar. STIX: Event as a STIX bundle that will be used by detection engines. Raw event: Event as received by SEKOIA.IO.","title":"Log lines:"},{"location":"operation_center/events/#search-bar","text":"You can search among the list of events by using Dork query langage, for which you can find more information here .","title":"Search bar:"},{"location":"operation_center/events/#filters","text":"It is possible to use and combine filters in the search bar by using: Smart description: By clicking on the \"+\" button. Details: The query is made on the existing fields in the \"details\" section by clicking on the \"+\" button for one of the items. To go back to the list of logs shown, you need to clear filters and select \"Current events\"","title":"Filters:"},{"location":"operation_center/events/#save-filters","text":"It is possible to save a query by clicking on the star icon. The period of time is not conserved with it. To check it out, you can click on \"Saved queries\" then select the wanted period of time and press enter to see the events found.","title":"Save filters:"},{"location":"operation_center/events/#date","text":"It is possible to select the period of time to be taken in consideration while using a filter: Presets: Recommended predefined dates. Dates relatives: Earliest and latest moments to select. Date ranges: Gap between 2 dates. Click on the blue lense to start a full search job.","title":"Date:"},{"location":"operation_center/events/#search-history","text":"Each search result lasts 10 min by default and it is possible to configure the retention to reach 1 day (24 hours). When the result is expired, you still have the possibility of replaying it using the saved relative date or a new one.","title":"Search history:"},{"location":"operation_center/events/#sharing-a-search","text":"Search job have IDs that are available in the browser address bar. https://app.sekoia.io/sic/events?jobId=2b5ce17f-517e-4dd3-8253-1495a6ba538b You can share your researches with colleagues by sending them these jobIds, which are accessible within your community.","title":"Sharing a search"},{"location":"operation_center/events/#export-the-results-of-a-search","text":"You can easily export the results of a search in CSV or JSON format and choose the fields you want to export. The export will be made to the default folder defined for downloads. Name of the file is optional, if not provided, the file will be named with the uuid of the job search. Note The 'description' field will not be exported.","title":"Export the results of a search"},{"location":"operation_center/faq/","text":"Operation Center: Frequently Asked Questions Is the IP behind intake.sekoia.io static? IP for intake.sekoia.io is 145.239.192.38 . intake.sekoia.io is the domain name used to send your logs to SEKOIA.IO, either via Syslog or HTTP protocols. The IP address behind that service is static and stable. You can use that IP to configure your firewalls to allow connections from your forwarding systems to SEKOIA.IO. How to debug Rsyslog\u2019s forward configuration to SEKOIA.IO? If you use Rsyslog to forward your logs to SEKOIA.IO, you will probably have a section like this in your configuration files: template(name=\"SEKOIAIOUnboundTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"jOK5bMVXz5Iz7gfogQDbCcC7l7S2IrOs5\\\"] %msg%\\n\") if $programname startswith 'unbound' then @@(o)intake.sekoia.io:10514;SEKOIAIOUnboundTemplate If you want to retrieve the raw data that is forwarded to SEKOIA.IO, you can duplicate the last line and make Rsyslog dump logs to a local file: if $programname startswith 'unbound' then /tmp/nginx-output.log;SEKOIAIOUnboundTemplate This way, you will be able to exactly identify what data is sent to SEKOIA.IO. # tail -n 1 /tmp/nginx-output.log <30>1 2021-01-13T14:52:06.934860+01:00 ote unbound - LOG [SEKOIA@53288 intake_key=\"jOK5bMVXz5Iz7gfogQDbCcC7l7S2IrOs5\"] [596451:0] info: 127.0.0.1 intake.sekoia.io. A IN","title":"FAQ"},{"location":"operation_center/faq/#operation-center-frequently-asked-questions","text":"","title":"Operation Center: Frequently Asked Questions"},{"location":"operation_center/faq/#is-the-ip-behind-intakesekoiaio-static","text":"IP for intake.sekoia.io is 145.239.192.38 . intake.sekoia.io is the domain name used to send your logs to SEKOIA.IO, either via Syslog or HTTP protocols. The IP address behind that service is static and stable. You can use that IP to configure your firewalls to allow connections from your forwarding systems to SEKOIA.IO.","title":"Is the IP behind intake.sekoia.io static?"},{"location":"operation_center/faq/#how-to-debug-rsyslogs-forward-configuration-to-sekoiaio","text":"If you use Rsyslog to forward your logs to SEKOIA.IO, you will probably have a section like this in your configuration files: template(name=\"SEKOIAIOUnboundTemplate\" type=\"string\" string=\"<%pri%>1 %timestamp:::date-rfc3339% %hostname% %app-name% %procid% LOG [SEKOIA@53288 intake_key=\\\"jOK5bMVXz5Iz7gfogQDbCcC7l7S2IrOs5\\\"] %msg%\\n\") if $programname startswith 'unbound' then @@(o)intake.sekoia.io:10514;SEKOIAIOUnboundTemplate If you want to retrieve the raw data that is forwarded to SEKOIA.IO, you can duplicate the last line and make Rsyslog dump logs to a local file: if $programname startswith 'unbound' then /tmp/nginx-output.log;SEKOIAIOUnboundTemplate This way, you will be able to exactly identify what data is sent to SEKOIA.IO. # tail -n 1 /tmp/nginx-output.log <30>1 2021-01-13T14:52:06.934860+01:00 ote unbound - LOG [SEKOIA@53288 intake_key=\"jOK5bMVXz5Iz7gfogQDbCcC7l7S2IrOs5\"] [596451:0] info: 127.0.0.1 intake.sekoia.io. A IN","title":"How to debug Rsyslog\u2019s forward configuration to SEKOIA.IO?"},{"location":"operation_center/intakes/","text":"Intakes - Data Sources Management Intakes correspond to data sources sent to SEKOIA.IO. They are identified by a name, a log format and an 'intake key', which is used to configure the sending from your information system. On the intake list, the number of events sent to SEKOIA.IO for the past 7 days and the number of events in error (not integrated) are displayed, as well as the intake key, with a button to copy this value to the clipboard. To add a new intake, you will have to enter a name for it, choose the entity to which you want to associate the corresponding data and to choose the format of the events. All the documentation for integration of your data sources is also available in the integrations page. Do not hesitate to contact us at support@sekoia.io if the settings recommendations provided are not sufficient or not applicable to your system. We can then see with you how to transfer your events in the best conditions. Do neither hesitate to contact us if the format of the logs you want to send us is not in the list, we regularly add new formats, we can tell you when yours will be available.","title":"Intakes"},{"location":"operation_center/intakes/#intakes-data-sources-management","text":"Intakes correspond to data sources sent to SEKOIA.IO. They are identified by a name, a log format and an 'intake key', which is used to configure the sending from your information system. On the intake list, the number of events sent to SEKOIA.IO for the past 7 days and the number of events in error (not integrated) are displayed, as well as the intake key, with a button to copy this value to the clipboard. To add a new intake, you will have to enter a name for it, choose the entity to which you want to associate the corresponding data and to choose the format of the events. All the documentation for integration of your data sources is also available in the integrations page. Do not hesitate to contact us at support@sekoia.io if the settings recommendations provided are not sufficient or not applicable to your system. We can then see with you how to transfer your events in the best conditions. Do neither hesitate to contact us if the format of the logs you want to send us is not in the list, we regularly add new formats, we can tell you when yours will be available.","title":"Intakes - Data Sources Management"},{"location":"operation_center/operators/","text":"Operators Types of Operators Once the user has selected and configured a Trigger , it is time to choose from tree different Operators that can be retrieved by filtering the library on the top left. The Operators are used to filter, loop or store the data generated by a Trigger or an Action . Condition The Operator \"Condition\" is similar to the If...Else condition. It allows a user to test a variable. It can be followed by an Action or an Operator , even a \"Condition\". To edit a condition, it is needed to add a case, by clicking on the \"+\". For instance: Foreach The Operator \"Foreach\" is specialised in the browsing of a list of data, such as a JSON or STIX output. It can be followed by multiple Blocks such as the Action ones or the Operator ones, even a \"Foreach\" loop. Store The Operator \"Store\" allows a user to store data before using it in the same playbook. For instance the following video shows how to store a port number after a loop over a JSON formatted input, and use it to alter the status of an alert.","title":"Operators"},{"location":"operation_center/operators/#operators","text":"","title":"Operators"},{"location":"operation_center/operators/#types-of-operators","text":"Once the user has selected and configured a Trigger , it is time to choose from tree different Operators that can be retrieved by filtering the library on the top left. The Operators are used to filter, loop or store the data generated by a Trigger or an Action .","title":"Types of Operators"},{"location":"operation_center/operators/#condition","text":"The Operator \"Condition\" is similar to the If...Else condition. It allows a user to test a variable. It can be followed by an Action or an Operator , even a \"Condition\". To edit a condition, it is needed to add a case, by clicking on the \"+\". For instance:","title":"Condition"},{"location":"operation_center/operators/#foreach","text":"The Operator \"Foreach\" is specialised in the browsing of a list of data, such as a JSON or STIX output. It can be followed by multiple Blocks such as the Action ones or the Operator ones, even a \"Foreach\" loop.","title":"Foreach"},{"location":"operation_center/operators/#store","text":"The Operator \"Store\" allows a user to store data before using it in the same playbook. For instance the following video shows how to store a port number after a loop over a JSON formatted input, and use it to alter the status of an alert.","title":"Store"},{"location":"operation_center/playbook_overview/","text":"Automation Automating your security improves your cyber security operations by consolidating the use of your technologies, processes and policies. Automation combines security tools, processes and people, and speeds up the execution of your security responses while ensuring their repeatability and verifiability. Thus, SEKOIA.IO supervises and executes response actions to be in line with your business and operational objectives. Playbooks SEKOIA.IO comes with an in house GUI for creating automated investigation and orchestration actions: the playbooks. The playbooks are capable of automatically analysing and investigating all alerts coming from your SIEM. They also allow a quick and fully automated sorting after the analysis of multiple factors. Create your own playbooks to build your processes. Use the SEKOIA.IO interface to interconnect your tools and implement your scenarios. Configure the execution of your playbooks by triggering them manually on the alert/case page or automatically based on multiple factors. Getting started with a playbook A simple way to create your playbook: you only need to name it and add a description before validating it. Prerequisites First of all, create two roles in Settings > Communities > Your community > Roles > + ROLE : - SIC Operator by selecting the 36 roles that starts with SIC - Symphony Operator by selecting the 7 roles that starts with SYMPHONY Then, to be able to use a Block such as a Sekoia trigger, you should create an API key with the right level of accreditations: SIC Operation and Symphony Operator. To do it, go to Settings > Communities > Your community > API Keys > + API Key - Name and describe your API Key - Select the right roles for your key in order to be efficient Note: Save your new API Key in a vault. It will no longer be accessible! Now you will be able to setup a configuration for the Sekoia Blocks , built for you. Back in the Operation Center, section Playbooks: Select the trigger of your choice, for instance Security alerts Drag and drop it into the center of your screen On the right side, Create new configuration Paste your API key Fulfill the base_url field with https://api.sekoia.io Click SAVE button Build your playbooks with Blocks filtered into 3 types To create a playbook, you will basically need to use the following filters. More information by following these links: Triggers Operators Actions Playbook templates In order to ease the deployment of playbooks, we regularly produce Playbook templates that are ready to use for anyone with an API key. To use one: - Click on + NEW PLAYBOOK - Select Use a template - Filter on your needs alert , notification , webhook or use the Search bar - Select the template you are interested in - Click on CREATE A brand new playbook will be created. Then you just have to adjust the Module Configuration section of each block with your API key and URL. You can either create a new configuration or use an existing one that will appear on the list menu. Troubleshooting RUN feature If for some reason, a playbook is not working properly, it is possible to display its execution workflow on the section Playbook > <PlaybookName> Click on RUN In the following animation, you will see an issue on the IF Condition where the Green overlay ceased and the output of the if un Yellow. An example of correct playbook workflow should be in green from the begining to the end, as followed: CODE feature In addition of the RUN feature, the values of the playbook execution can be displayed in section, juste next to the RUN button . In order to set information parameters to a Block , the Copy-Past feature create small sections of content written in JINJA. For more information on this language, please follow this documentation . It is possible to test your JINJA code on a JSON file (Copy-Past the CODE of your playbook AND remove the nodes) with this small python script: import json from jinja2 import Template file_json = open ( \"file.json\" , \"r\" ) loading = json . load ( file_json ) # The JINJA partern to be tested jinja_patern = \"{{urgency.value}}\" tm = Template ( jinja_patern ) msg = tm . render ( urgency = loading [ \"urgency\" ]) print ( msg )","title":"Overview"},{"location":"operation_center/playbook_overview/#automation","text":"Automating your security improves your cyber security operations by consolidating the use of your technologies, processes and policies. Automation combines security tools, processes and people, and speeds up the execution of your security responses while ensuring their repeatability and verifiability. Thus, SEKOIA.IO supervises and executes response actions to be in line with your business and operational objectives.","title":"Automation"},{"location":"operation_center/playbook_overview/#playbooks","text":"SEKOIA.IO comes with an in house GUI for creating automated investigation and orchestration actions: the playbooks. The playbooks are capable of automatically analysing and investigating all alerts coming from your SIEM. They also allow a quick and fully automated sorting after the analysis of multiple factors. Create your own playbooks to build your processes. Use the SEKOIA.IO interface to interconnect your tools and implement your scenarios. Configure the execution of your playbooks by triggering them manually on the alert/case page or automatically based on multiple factors.","title":"Playbooks"},{"location":"operation_center/playbook_overview/#getting-started-with-a-playbook","text":"A simple way to create your playbook: you only need to name it and add a description before validating it.","title":"Getting started with a playbook"},{"location":"operation_center/playbook_overview/#prerequisites","text":"First of all, create two roles in Settings > Communities > Your community > Roles > + ROLE : - SIC Operator by selecting the 36 roles that starts with SIC - Symphony Operator by selecting the 7 roles that starts with SYMPHONY Then, to be able to use a Block such as a Sekoia trigger, you should create an API key with the right level of accreditations: SIC Operation and Symphony Operator. To do it, go to Settings > Communities > Your community > API Keys > + API Key - Name and describe your API Key - Select the right roles for your key in order to be efficient Note: Save your new API Key in a vault. It will no longer be accessible! Now you will be able to setup a configuration for the Sekoia Blocks , built for you. Back in the Operation Center, section Playbooks: Select the trigger of your choice, for instance Security alerts Drag and drop it into the center of your screen On the right side, Create new configuration Paste your API key Fulfill the base_url field with https://api.sekoia.io Click SAVE button","title":"Prerequisites"},{"location":"operation_center/playbook_overview/#build-your-playbooks-with-blocks-filtered-into-3-types","text":"To create a playbook, you will basically need to use the following filters. More information by following these links: Triggers Operators Actions","title":"Build your playbooks with Blocks filtered into 3 types"},{"location":"operation_center/playbook_overview/#playbook-templates","text":"In order to ease the deployment of playbooks, we regularly produce Playbook templates that are ready to use for anyone with an API key. To use one: - Click on + NEW PLAYBOOK - Select Use a template - Filter on your needs alert , notification , webhook or use the Search bar - Select the template you are interested in - Click on CREATE A brand new playbook will be created. Then you just have to adjust the Module Configuration section of each block with your API key and URL. You can either create a new configuration or use an existing one that will appear on the list menu.","title":"Playbook templates"},{"location":"operation_center/playbook_overview/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"operation_center/playbook_overview/#run-feature","text":"If for some reason, a playbook is not working properly, it is possible to display its execution workflow on the section Playbook > <PlaybookName> Click on RUN In the following animation, you will see an issue on the IF Condition where the Green overlay ceased and the output of the if un Yellow. An example of correct playbook workflow should be in green from the begining to the end, as followed:","title":"RUN feature"},{"location":"operation_center/playbook_overview/#code-feature","text":"In addition of the RUN feature, the values of the playbook execution can be displayed in section, juste next to the RUN button . In order to set information parameters to a Block , the Copy-Past feature create small sections of content written in JINJA. For more information on this language, please follow this documentation . It is possible to test your JINJA code on a JSON file (Copy-Past the CODE of your playbook AND remove the nodes) with this small python script: import json from jinja2 import Template file_json = open ( \"file.json\" , \"r\" ) loading = json . load ( file_json ) # The JINJA partern to be tested jinja_patern = \"{{urgency.value}}\" tm = Template ( jinja_patern ) msg = tm . render ( urgency = loading [ \"urgency\" ]) print ( msg )","title":"CODE feature"},{"location":"operation_center/rules/","text":"This page does not exist anymore. Rules have been migrated to Rules Catalog, for which documentation is available here: Rules Catalog","title":"Rules"},{"location":"operation_center/rules_catalog/","text":"Detection Rules Management - Coming soon Rules Catalog: Security profile MITRE ATT&CK The MITRE ATT&CK framework is a comprehensive matrix of tactics and techniques used by threat hunters and defenders to better classify attacks and assess an organization's risk. Every time you enable a rule, it appears on the Framework in blue in one or many cells. Each cell represents an attack technique. The cells are clickable and enable you to see or disable the rules activated in each one. You can see how many rules are enabled in a cell by hovering over it. The color changes depending on the number of rules activated in one cell. The blue gets darker when more rules are enabled and a white cell means that no rule is activated in it. Rules attributes Typology: Two types of rules are displayed on the Rules Catalog. Available rules : All custom rules created by you in addition to the available rules in the Rules Catalog. Verified rules : Rules created by SEKOIA.IO\u2019s analysts, related to the CTI and available in the Rules Catalog. Effort level: The effort level is increasing from Elementary to Master according to two criteria: Effort needed to enable a rule. Risk of false positives. Master rules are generic and raise a lot of alerts, which need to be contextualized in a more refined way. Whereas elementary rules require less effort and raise relatively fewer alerts. The blue square represents a clickable counter of activated rules, it is therefore possible to use the filters related to the effort level by clicking on one or more of them. Capabilities: The rules are related to different elements according to offensive and defensive capabilities. They can be related to threats, attacks and data sources. Each element is associated to a filter that can be used to display the rules related to the selected element. Manage existing rules Filters: There are different filters linked to all rules attributes above. They can be used either separately or in addition to other filters and allow you to Enable or Disable all rules displayed simultaneously. Rules details: On top of the details of a rule, you can see the name, when it was released and last updated. There are information about the effort level attributed to the rule, the severity, tags that ease search, related threats. It also explains the followed strategy and gives information about false positives, STIX patterning and data sources. To edit an existing rule, click on the gear icon to select entities and assets, or to add a filter alert. Add a custom rule In addition to the rules available in the catalog provided by SEKOIA.IO, you can add your own rules. General definition of the rule: The rule name is mandatory during the creation, it will be used to name the corresponding raised alerts by default. You can add an optional description bellow. Select the effort level required and the threats detected with this rule if any, by selecting it on from the MITRE ATT&CK or by using the search bar through key words or drop-down list. Entities & Assets: You can select entities and assets manually or use all of them as a basis to filter the analyzed events. Detection rules can be applied to all events, whatever the entity to which they are attached or the assets to which they can be linked, or applied only to some of them. If you choose 'Using all entities' and 'Using all assets', rules will be applied to all events even if new entites or new assets were added since last rule modification. For rules for which some entities or some assets were selected, if new entities or new assets are created, detection will not occur on corresponding events without rule modification. Detection patterns: The signature part corresponds to the rule itself, the detection pattern applied to events. There are two type of rules: CTI or correlation. For CTI rules, you just have to select the source of the indicators: SEKOIA Intelligence Feed is an IOCs feed managed by SEKOIA's Purple Team (indicators present in the SEKOIA.IO Intelligence Center). For correlation rules, the language used is STIX Patterning. More details about this langage are given below . Security alerts: In the Alert properties part, you should indicate the category and type of the alerts raised by the rule and the severity of the rule, which is used to calculate the urgency of the corresponding raised alerts in association with assets criticality for events matching assets. Note Modification of rules parameters will be applied for new alerts, raised after the compilation of the rule. STIX Patterning Correlation rules are written using STIX Patterning. In order to enhance detection of possibly malicious activity on networks and endpoints, a standard language is needed to describe what to look for in a cyber environment. STIX, abbreviation for Structured Threat Information eXpression, is a standardized language developed by MITRE and OASIS Cyber Threat Intelligence (CTI) Technical Committee to describe information about cyber-threats. It has been adopted as an international standard by various communities and organizations sharing information. STIX Patterns are composed of multiple building blocks, ranging from simple key-value comparisons to more complex, context-sensitive expressions. The most fundamental building block is the Comparison Expression, which is a comparison between a single property of a Cyber Observable Object and a given constant using a Comparison Operator. As a simple example, one might use the following Comparison Expression (contained within an Observation Expression) to match against an IPv4 address: [ipv4-addr:value = '127.0.0.1'] Observation Expressions are contained in square brackets [ ... ] and may consist of one or more Comparison Expressions joined by Boolean Operators. Observation Expressions may be followed by one or more Qualifiers, which allow for the expression of further restrictions on the set of data matching the pattern. The final, highest level building block of STIX Patterning combines two or more Object Expressions via Observation Operators, yielding a STIX Pattern capable of matching across multiple STIX Observed Data SDOs. Warning Currently, we set a limitation and only rules matching with single Observed Data, and so using only one Observation Expression contained in square brackets, are available for our customers. Note When matching an Observation against an Observation Expression, all Comparison Expressions contained within the Observation Expression MUST start matching against the same SCO in the Observation. That is, when resolving object paths of each Comparison Expression, the <object-type>:<property_name> MUST start from the same SCO. Different SCOs may ultimately be used in matching, but they MUST be referenced from the same, single SCO. An Observation Expression MAY contain Comparison Expressions with Object Paths that start with different object types, but such Comparison Expressions MUST be joined by OR . The Comparison Expressions of an Observation Expression that use AND MUST use the same base Object Path . Note Regarding the use of regular expressions ( MATCHES keyword) in STIX Patterning rules, it is necessary to escape the \u201c \\ \u201d. Thus, the STIX Patterning rule to identify countries other than France, you will need to use the following rule: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] For more information about STIX and STIX Patterning, please refers to the OASIS STIX Patterning specification . How to Validate STIX Patterning Rule\u2019s Syntax SEKOIA.IO checks the syntax of submitted rules and reports errors to the \u201crules\u201d interface. In order to validate your rule\u2019s syntax, you can also use an open source tool called stix2-patterns . This tool is part of the official tools developed by the OASIS Technical Committee. Here\u2019s an example on how to use that tool. These commands should be executed in a shell. $ python3 -m venv venv $ source venv/bin/activate $ pip install stix2-patterns $ validate-patterns Enter a pattern to validate: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] PASS: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] In this example, we are trying to validate a STIX Patterning rule that contains a regular expression ( MATCHES ). This rule is considered valid and can be submitted to SEKOIA.IO. How to Test STIX Patterning Rules? When one creates a rule in SEKOIA.IO, this one is automatically validated and applied to incoming traffic. In order to test your STIX Patterning rules, you can use an open source tool, called stix2-matcher . This tool is part of the official tools developed by the OASIS Technical Committee. First, you need to retrieve a STIX \u201cbundle\u201d from SEKOIA.IO. To do so, go to the \u201cevents\u201d page, find the event you want to work on, and export the STIX representation of that event: Export that STIX \u201cbundle\u201d in a file, called, for example, event.json . Here\u2019s an example on how to use that tool. These commands should be executed in a shell. We are also using jq a tool that helps to manipulate JSON files. $ python3 -m venv venv $ source venv/bin/activate $ pip install stix2-matcher $ cat event.json | jq '.objects[] | select( .type | contains(\"observed-data\") )' > observed-data.json $ cat << EOF >| patterns [ipv4-addr:value = '246.127.189.32'] [x-dns-traffic:extensions.'x-log'.hostname = 'hostname'] EOF $ stix2-matcher -f observed-data.json -p patterns MATCH: [ipv4-addr:value = '246.127.189.32'] MATCH: [x-dns-traffic:extensions.'x-log'.hostname = 'hostname'] Observed Data In order to trigger alerts, rule patterns must match with Observed Data. An \"Observed Data\" is the internal representation of any collected event in SEKOIA.IO. All events are normalized into a JSON object compliant with the STIX Observed Data specification. Detection rules are applied on events in STIX Observed Data format. Here is an example of an Observed Data that could be obtained from a squid event: { \"x_sic_entity_by_ref\" : \"identity--d6358bb4-d9bb-47aa-b074-c6d1aeb673e2\" , \"created\" : \"2019-09-20T16:17:42.971Z\" , \"objects\" : { \"0\" : { \"value\" : \"127.0.0.1\" , \"type\" : \"ipv4-addr\" }, \"1\" : { \"value\" : \"216.58.215.48\" , \"type\" : \"ipv4-addr\" }, \"2\" : { \"start\" : \"2019-09-20T16:17:40.935Z\" , \"type\" : \"network-traffic\" , \"end\" : \"2019-09-20T16:17:40.935Z\" , \"extensions\" : { \"http-request-ext\" : { \"request_header\" : { \"Content-Type\" : \"application/xml\" }, \"request_method\" : \"HEAD\" , \"request_value\" : \"http://216.58.215.48/\" } }, \"src_ref\" : \"0\" , \"dst_ref\" : \"1\" , \"protocols\" : [ \"ipv4\" ] }, \"3\" : { \"type\" : \"user-account\" , \"extensions\" : { \"x-log\" : { \"hostname\" : \"DESKTOP-UPU7IFP\" } } } }, \"type\" : \"observed-data\" , ... \"x_event_type\" : \"http\" } The JSON object has a type of observed-data and is composed by smaller objects following another STIX specification: Cyber Observables . The main observable is the third one, with a type of network-traffic. It has references to indicate the source of the packet ( src_ref - 127.0.0.1 ) and its destination ( dst_ref - 216.58.215.48 ). These information can be used to construct correlation rules. Below, a non-exhaustive list of information which can be used in your rules: ipv4-addr:value ipv6-addr:value domain-name:value url:value network-traffic:dst_port network-traffic:src_port network-traffic:dst_packets network-traffic:src_packets network-traffic:dst_ref.value (corresponding to IP value, in the above example, network-traffic:dst_ref refers to object 0, which has a value of '127.0.0.1') network-traffic:src_ref.value network-traffic:extensions.'http-request-ext'.request_value network-traffic:protocols[*] process:pid process:name process:command_line file:hashes.md5 user-account:account_login","title":"Rules Catalog"},{"location":"operation_center/rules_catalog/#detection-rules-management-coming-soon","text":"","title":"Detection Rules Management - Coming soon"},{"location":"operation_center/rules_catalog/#rules-catalog","text":"","title":"Rules Catalog:"},{"location":"operation_center/rules_catalog/#security-profile-mitre-attck","text":"The MITRE ATT&CK framework is a comprehensive matrix of tactics and techniques used by threat hunters and defenders to better classify attacks and assess an organization's risk. Every time you enable a rule, it appears on the Framework in blue in one or many cells. Each cell represents an attack technique. The cells are clickable and enable you to see or disable the rules activated in each one. You can see how many rules are enabled in a cell by hovering over it. The color changes depending on the number of rules activated in one cell. The blue gets darker when more rules are enabled and a white cell means that no rule is activated in it.","title":"Security profile MITRE ATT&amp;CK"},{"location":"operation_center/rules_catalog/#rules-attributes","text":"Typology: Two types of rules are displayed on the Rules Catalog. Available rules : All custom rules created by you in addition to the available rules in the Rules Catalog. Verified rules : Rules created by SEKOIA.IO\u2019s analysts, related to the CTI and available in the Rules Catalog. Effort level: The effort level is increasing from Elementary to Master according to two criteria: Effort needed to enable a rule. Risk of false positives. Master rules are generic and raise a lot of alerts, which need to be contextualized in a more refined way. Whereas elementary rules require less effort and raise relatively fewer alerts. The blue square represents a clickable counter of activated rules, it is therefore possible to use the filters related to the effort level by clicking on one or more of them. Capabilities: The rules are related to different elements according to offensive and defensive capabilities. They can be related to threats, attacks and data sources. Each element is associated to a filter that can be used to display the rules related to the selected element.","title":"Rules attributes"},{"location":"operation_center/rules_catalog/#manage-existing-rules","text":"Filters: There are different filters linked to all rules attributes above. They can be used either separately or in addition to other filters and allow you to Enable or Disable all rules displayed simultaneously. Rules details: On top of the details of a rule, you can see the name, when it was released and last updated. There are information about the effort level attributed to the rule, the severity, tags that ease search, related threats. It also explains the followed strategy and gives information about false positives, STIX patterning and data sources. To edit an existing rule, click on the gear icon to select entities and assets, or to add a filter alert.","title":"Manage existing rules"},{"location":"operation_center/rules_catalog/#add-a-custom-rule","text":"In addition to the rules available in the catalog provided by SEKOIA.IO, you can add your own rules. General definition of the rule: The rule name is mandatory during the creation, it will be used to name the corresponding raised alerts by default. You can add an optional description bellow. Select the effort level required and the threats detected with this rule if any, by selecting it on from the MITRE ATT&CK or by using the search bar through key words or drop-down list. Entities & Assets: You can select entities and assets manually or use all of them as a basis to filter the analyzed events. Detection rules can be applied to all events, whatever the entity to which they are attached or the assets to which they can be linked, or applied only to some of them. If you choose 'Using all entities' and 'Using all assets', rules will be applied to all events even if new entites or new assets were added since last rule modification. For rules for which some entities or some assets were selected, if new entities or new assets are created, detection will not occur on corresponding events without rule modification. Detection patterns: The signature part corresponds to the rule itself, the detection pattern applied to events. There are two type of rules: CTI or correlation. For CTI rules, you just have to select the source of the indicators: SEKOIA Intelligence Feed is an IOCs feed managed by SEKOIA's Purple Team (indicators present in the SEKOIA.IO Intelligence Center). For correlation rules, the language used is STIX Patterning. More details about this langage are given below . Security alerts: In the Alert properties part, you should indicate the category and type of the alerts raised by the rule and the severity of the rule, which is used to calculate the urgency of the corresponding raised alerts in association with assets criticality for events matching assets. Note Modification of rules parameters will be applied for new alerts, raised after the compilation of the rule.","title":"Add a custom rule"},{"location":"operation_center/rules_catalog/#stix-patterning","text":"Correlation rules are written using STIX Patterning. In order to enhance detection of possibly malicious activity on networks and endpoints, a standard language is needed to describe what to look for in a cyber environment. STIX, abbreviation for Structured Threat Information eXpression, is a standardized language developed by MITRE and OASIS Cyber Threat Intelligence (CTI) Technical Committee to describe information about cyber-threats. It has been adopted as an international standard by various communities and organizations sharing information. STIX Patterns are composed of multiple building blocks, ranging from simple key-value comparisons to more complex, context-sensitive expressions. The most fundamental building block is the Comparison Expression, which is a comparison between a single property of a Cyber Observable Object and a given constant using a Comparison Operator. As a simple example, one might use the following Comparison Expression (contained within an Observation Expression) to match against an IPv4 address: [ipv4-addr:value = '127.0.0.1'] Observation Expressions are contained in square brackets [ ... ] and may consist of one or more Comparison Expressions joined by Boolean Operators. Observation Expressions may be followed by one or more Qualifiers, which allow for the expression of further restrictions on the set of data matching the pattern. The final, highest level building block of STIX Patterning combines two or more Object Expressions via Observation Operators, yielding a STIX Pattern capable of matching across multiple STIX Observed Data SDOs. Warning Currently, we set a limitation and only rules matching with single Observed Data, and so using only one Observation Expression contained in square brackets, are available for our customers. Note When matching an Observation against an Observation Expression, all Comparison Expressions contained within the Observation Expression MUST start matching against the same SCO in the Observation. That is, when resolving object paths of each Comparison Expression, the <object-type>:<property_name> MUST start from the same SCO. Different SCOs may ultimately be used in matching, but they MUST be referenced from the same, single SCO. An Observation Expression MAY contain Comparison Expressions with Object Paths that start with different object types, but such Comparison Expressions MUST be joined by OR . The Comparison Expressions of an Observation Expression that use AND MUST use the same base Object Path . Note Regarding the use of regular expressions ( MATCHES keyword) in STIX Patterning rules, it is necessary to escape the \u201c \\ \u201d. Thus, the STIX Patterning rule to identify countries other than France, you will need to use the following rule: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] For more information about STIX and STIX Patterning, please refers to the OASIS STIX Patterning specification .","title":"STIX Patterning"},{"location":"operation_center/rules_catalog/#how-to-validate-stix-patterning-rules-syntax","text":"SEKOIA.IO checks the syntax of submitted rules and reports errors to the \u201crules\u201d interface. In order to validate your rule\u2019s syntax, you can also use an open source tool called stix2-patterns . This tool is part of the official tools developed by the OASIS Technical Committee. Here\u2019s an example on how to use that tool. These commands should be executed in a shell. $ python3 -m venv venv $ source venv/bin/activate $ pip install stix2-patterns $ validate-patterns Enter a pattern to validate: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] PASS: [ipv4-addr:x_tags[*].name MATCHES '^country:(?!FR)\\\\w+'] In this example, we are trying to validate a STIX Patterning rule that contains a regular expression ( MATCHES ). This rule is considered valid and can be submitted to SEKOIA.IO.","title":"How to Validate STIX Patterning Rule\u2019s Syntax"},{"location":"operation_center/rules_catalog/#how-to-test-stix-patterning-rules","text":"When one creates a rule in SEKOIA.IO, this one is automatically validated and applied to incoming traffic. In order to test your STIX Patterning rules, you can use an open source tool, called stix2-matcher . This tool is part of the official tools developed by the OASIS Technical Committee. First, you need to retrieve a STIX \u201cbundle\u201d from SEKOIA.IO. To do so, go to the \u201cevents\u201d page, find the event you want to work on, and export the STIX representation of that event: Export that STIX \u201cbundle\u201d in a file, called, for example, event.json . Here\u2019s an example on how to use that tool. These commands should be executed in a shell. We are also using jq a tool that helps to manipulate JSON files. $ python3 -m venv venv $ source venv/bin/activate $ pip install stix2-matcher $ cat event.json | jq '.objects[] | select( .type | contains(\"observed-data\") )' > observed-data.json $ cat << EOF >| patterns [ipv4-addr:value = '246.127.189.32'] [x-dns-traffic:extensions.'x-log'.hostname = 'hostname'] EOF $ stix2-matcher -f observed-data.json -p patterns MATCH: [ipv4-addr:value = '246.127.189.32'] MATCH: [x-dns-traffic:extensions.'x-log'.hostname = 'hostname']","title":"How to Test STIX Patterning Rules?"},{"location":"operation_center/rules_catalog/#observed-data","text":"In order to trigger alerts, rule patterns must match with Observed Data. An \"Observed Data\" is the internal representation of any collected event in SEKOIA.IO. All events are normalized into a JSON object compliant with the STIX Observed Data specification. Detection rules are applied on events in STIX Observed Data format. Here is an example of an Observed Data that could be obtained from a squid event: { \"x_sic_entity_by_ref\" : \"identity--d6358bb4-d9bb-47aa-b074-c6d1aeb673e2\" , \"created\" : \"2019-09-20T16:17:42.971Z\" , \"objects\" : { \"0\" : { \"value\" : \"127.0.0.1\" , \"type\" : \"ipv4-addr\" }, \"1\" : { \"value\" : \"216.58.215.48\" , \"type\" : \"ipv4-addr\" }, \"2\" : { \"start\" : \"2019-09-20T16:17:40.935Z\" , \"type\" : \"network-traffic\" , \"end\" : \"2019-09-20T16:17:40.935Z\" , \"extensions\" : { \"http-request-ext\" : { \"request_header\" : { \"Content-Type\" : \"application/xml\" }, \"request_method\" : \"HEAD\" , \"request_value\" : \"http://216.58.215.48/\" } }, \"src_ref\" : \"0\" , \"dst_ref\" : \"1\" , \"protocols\" : [ \"ipv4\" ] }, \"3\" : { \"type\" : \"user-account\" , \"extensions\" : { \"x-log\" : { \"hostname\" : \"DESKTOP-UPU7IFP\" } } } }, \"type\" : \"observed-data\" , ... \"x_event_type\" : \"http\" } The JSON object has a type of observed-data and is composed by smaller objects following another STIX specification: Cyber Observables . The main observable is the third one, with a type of network-traffic. It has references to indicate the source of the packet ( src_ref - 127.0.0.1 ) and its destination ( dst_ref - 216.58.215.48 ). These information can be used to construct correlation rules. Below, a non-exhaustive list of information which can be used in your rules: ipv4-addr:value ipv6-addr:value domain-name:value url:value network-traffic:dst_port network-traffic:src_port network-traffic:dst_packets network-traffic:src_packets network-traffic:dst_ref.value (corresponding to IP value, in the above example, network-traffic:dst_ref refers to object 0, which has a value of '127.0.0.1') network-traffic:src_ref.value network-traffic:extensions.'http-request-ext'.request_value network-traffic:protocols[*] process:pid process:name process:command_line file:hashes.md5 user-account:account_login","title":"Observed Data"},{"location":"operation_center/templates/","text":"This page does not exist anymore. Old Rules Template feature has been migrated to Rules Catalog feature, for which documentation is available here: Rules Catalog","title":"Templates"},{"location":"operation_center/threat_exposition/","text":"Dashboards SEKOIA.IO\u2019s Operation Center brings a dashboard mechanism, that is fully configurable and adaptable to all needs. Dashboards are composed of modular widgets that can be placed where you want. Widgets can be configured to specify the date range, applied filters, display, etc. Default Dashboard SEKOIA.IO comes with a pre-configured dashboard that gives a synthetic view of the current community activity, either from an operational security perspective (risk level, number of alerts, etc.) or from an activity perspective (list of last posted comments, last created alerts, etc.). Several of these widgets allow quick access to the relevant elements in the Operation Center. You can manage easily the time range you want to display and the refresh period of your dashboard in the top right of the screen. Create a New Dashboard The default dashboard offered by SEKOIA.IO cannot be modified. In addition to this, you have the possibility of creating your own dashboard, in order to match it as best as possible to your uses. Several widgets are available to you, with the possibility depending on the chosen widget to configure various filters. Note Dashboards you create will be available to all members of your community. You also have the possibility of creating a new dashboard by copying an existing dashboard, as well as the possibility of editing all the dashboards outside the \u201cDefault Dashboard\u201d. Provided Widgets SEKOIA.IO\u2019s Operation Center provides several widgets: Widget Name Description Screenshot Alerts List of alerts, optionally filtered by their status and sorted either by their urgency, their frequency, etc. Number of Alerts Count the number of alerts, optionally filtered by their status and by their associated entity Risk Level Global risk level (ARI) for the current community Cases List of cases, optionally filtered by their status and sorted either by their urgency or they last updated date. Number of Cases Count the number of cases, optionally filtered by their status Number of Events by Data Source Number of events collected by source of data displayed as a list, a doughnut or an histogram. Entities Overview List synthetic view of entities with for each one the risk level, number of alerts and the number of collected events. Last Comments List of comments posted on items such as alerts or cases. Top Observed Threats Show list threats (malware, tool or attack-pattern) observed in alerts.","title":"Dashboards"},{"location":"operation_center/threat_exposition/#dashboards","text":"SEKOIA.IO\u2019s Operation Center brings a dashboard mechanism, that is fully configurable and adaptable to all needs. Dashboards are composed of modular widgets that can be placed where you want. Widgets can be configured to specify the date range, applied filters, display, etc.","title":"Dashboards"},{"location":"operation_center/threat_exposition/#default-dashboard","text":"SEKOIA.IO comes with a pre-configured dashboard that gives a synthetic view of the current community activity, either from an operational security perspective (risk level, number of alerts, etc.) or from an activity perspective (list of last posted comments, last created alerts, etc.). Several of these widgets allow quick access to the relevant elements in the Operation Center. You can manage easily the time range you want to display and the refresh period of your dashboard in the top right of the screen.","title":"Default Dashboard"},{"location":"operation_center/threat_exposition/#create-a-new-dashboard","text":"The default dashboard offered by SEKOIA.IO cannot be modified. In addition to this, you have the possibility of creating your own dashboard, in order to match it as best as possible to your uses. Several widgets are available to you, with the possibility depending on the chosen widget to configure various filters. Note Dashboards you create will be available to all members of your community. You also have the possibility of creating a new dashboard by copying an existing dashboard, as well as the possibility of editing all the dashboards outside the \u201cDefault Dashboard\u201d.","title":"Create a New Dashboard"},{"location":"operation_center/threat_exposition/#provided-widgets","text":"SEKOIA.IO\u2019s Operation Center provides several widgets: Widget Name Description Screenshot Alerts List of alerts, optionally filtered by their status and sorted either by their urgency, their frequency, etc. Number of Alerts Count the number of alerts, optionally filtered by their status and by their associated entity Risk Level Global risk level (ARI) for the current community Cases List of cases, optionally filtered by their status and sorted either by their urgency or they last updated date. Number of Cases Count the number of cases, optionally filtered by their status Number of Events by Data Source Number of events collected by source of data displayed as a list, a doughnut or an histogram. Entities Overview List synthetic view of entities with for each one the risk level, number of alerts and the number of collected events. Last Comments List of comments posted on items such as alerts or cases. Top Observed Threats Show list threats (malware, tool or attack-pattern) observed in alerts.","title":"Provided Widgets"},{"location":"operation_center/triggers/","text":"Triggers To start creating a playbook, a Trigger is needed in order to launch the start of the playbook execution. Types of Triggers In the Operation Center, section Playbooks, once you have selected a created playbook, you will be able to filter the Blocks into one of the three categories: Triggers, Operators and Actions . Here we want to filter on Triggers only. The Triggers collects data that will be used by the Operators and Actions to answer a specific need. Alerts The Security alerts trigger allows a user to collect information of an alert, such as the alert_uuid , its date of creation ( created_at ), its category ( alert_type ), its urgency or the action that triggered THIS alert notification ( event_type ) for example \"alert-created\", \"alert-status-changed\", \"alert-comment-created\" etc. Webhook The Alert webhook trigger allows a user to automatically trigger actions once a human has raised and removed a doubt on an alert. This is a Block button, such as the one you will find hereunder, for the example of adding an standardised commentary on the alert commentary section. Cron The Cron trigger allows a user to periodically launch an automatic action, that should be defined and created by a user.","title":"Triggers"},{"location":"operation_center/triggers/#triggers","text":"To start creating a playbook, a Trigger is needed in order to launch the start of the playbook execution.","title":"Triggers"},{"location":"operation_center/triggers/#types-of-triggers","text":"In the Operation Center, section Playbooks, once you have selected a created playbook, you will be able to filter the Blocks into one of the three categories: Triggers, Operators and Actions . Here we want to filter on Triggers only. The Triggers collects data that will be used by the Operators and Actions to answer a specific need.","title":"Types of Triggers"},{"location":"operation_center/triggers/#alerts","text":"The Security alerts trigger allows a user to collect information of an alert, such as the alert_uuid , its date of creation ( created_at ), its category ( alert_type ), its urgency or the action that triggered THIS alert notification ( event_type ) for example \"alert-created\", \"alert-status-changed\", \"alert-comment-created\" etc.","title":"Alerts"},{"location":"operation_center/triggers/#webhook","text":"The Alert webhook trigger allows a user to automatically trigger actions once a human has raised and removed a doubt on an alert. This is a Block button, such as the one you will find hereunder, for the example of adding an standardised commentary on the alert commentary section.","title":"Webhook"},{"location":"operation_center/triggers/#cron","text":"The Cron trigger allows a user to periodically launch an automatic action, that should be defined and created by a user.","title":"Cron"},{"location":"releases/","text":"SEKOIA.IO Release Notes Latest Notes 2021-07-05: New visual identity Previous Release Notes 2021 2021-06-01: Graph Explorations and New Rules 2021-04-01: Rules catalog, events search optimization and Cobalt Strike Blogpost 2021-01-06: Operation Center\u2019s Configurable Dashboard System 2020 2020-09-10 : Case management, new events page & SEKOIA.IO Documentation. 2020-07-22 : Improved display of alerts\u2019s observed data, two-factor authentication. 2020-07-07 : Improved display of alerts\u2019s observed data. 2020-02-14 : Detection rules statistics, improved events page 2019 2019-06-18 2019-05-06 2019-01-25 2018 2018-12-03 2018-10-29 2018-10-15 2018-10-08 2018-10-01 2018-09-24","title":"SEKOIA.IO Release Notes"},{"location":"releases/#sekoiaio-release-notes","text":"","title":"SEKOIA.IO Release Notes"},{"location":"releases/#latest-notes","text":"2021-07-05: New visual identity","title":"Latest Notes"},{"location":"releases/#previous-release-notes","text":"","title":"Previous Release Notes"},{"location":"releases/#2021","text":"2021-06-01: Graph Explorations and New Rules 2021-04-01: Rules catalog, events search optimization and Cobalt Strike Blogpost 2021-01-06: Operation Center\u2019s Configurable Dashboard System","title":"2021"},{"location":"releases/#2020","text":"2020-09-10 : Case management, new events page & SEKOIA.IO Documentation. 2020-07-22 : Improved display of alerts\u2019s observed data, two-factor authentication. 2020-07-07 : Improved display of alerts\u2019s observed data. 2020-02-14 : Detection rules statistics, improved events page","title":"2020"},{"location":"releases/#2019","text":"2019-06-18 2019-05-06 2019-01-25","title":"2019"},{"location":"releases/#2018","text":"2018-12-03 2018-10-29 2018-10-15 2018-10-08 2018-10-01 2018-09-24","title":"2018"},{"location":"releases/2018-09-24/","text":"24/09/2018 This release focuses on the SIC application with a shiny new statistics API and a set of resiliency improvements. Features We followed the quote \u201cEverything is Data and Everyone needs it Analyzed\u201d while implementing out new statistical API for the SIC Application. This brandly new architecture leverages a time series database to expose various counters, risk indicators and datasets on both alerts and events production. This release brings various improvements on our OpenC2 language support in SIC. Among them, we improved the countermeasure descriptions through the use of fine grained-based action-steps. Fixes SIC customers interconnect their exporters to our detection engines by means of VPN connections. In order to reduce the impact of the various connection failures we encountered the last few days, we improved the overall resiliency of our VPN interconnections by means of a fast and automatic failover mechanism. We want to bring features and fixes to our customers as fast as possible while ensuring the quality of our products. One of our bottleneck in our release process was data migration on large sets of data. This release comes with our first data migration process which could be run as background tasks. Resiliencies Our APIs are the foundations of our collaborative applications. This release puts under constant monitoring all our HTTP endpoints to track number of requests and their execution times. A Service Level Objective (SLO) is the key element of a Service Level Agreement (SLA) as it defines the measurement process. This release introduces and actively monitors the first SLO of SEKOIA.IO: our HTTP APIs success rate. Belt and suspenders! Our new shiny alert management system, based on Prometheus, provides a faster outage notifications mechanism. Forecasts In SIC, we identified the need for a faster way to evaluate the operational impact of a detection rule. We designed a new page to expose statistics and aggregated information on all the alerts and incidents that are related to a selected rule. A key feature of SIC is its capacity of identifying the source and the target of a threat. To go even further, we offer a new page that consolidates all the information our detection engines collect on one source/target. For instance, this page will provide details on all the alerts, incidents and rules that are related to an IP address. SEKOIA.IO is built to support inter-application collaborations. As part of this, we designed an \u201caction card\u201d to ease user interactions between applications. More information in the next releases, stay tuned! If you have any concerns, feel free to contact us at support@sekoia.io .","title":"24/09/2018"},{"location":"releases/2018-09-24/#24092018","text":"This release focuses on the SIC application with a shiny new statistics API and a set of resiliency improvements.","title":"24/09/2018"},{"location":"releases/2018-09-24/#features","text":"We followed the quote \u201cEverything is Data and Everyone needs it Analyzed\u201d while implementing out new statistical API for the SIC Application. This brandly new architecture leverages a time series database to expose various counters, risk indicators and datasets on both alerts and events production. This release brings various improvements on our OpenC2 language support in SIC. Among them, we improved the countermeasure descriptions through the use of fine grained-based action-steps.","title":"Features"},{"location":"releases/2018-09-24/#fixes","text":"SIC customers interconnect their exporters to our detection engines by means of VPN connections. In order to reduce the impact of the various connection failures we encountered the last few days, we improved the overall resiliency of our VPN interconnections by means of a fast and automatic failover mechanism. We want to bring features and fixes to our customers as fast as possible while ensuring the quality of our products. One of our bottleneck in our release process was data migration on large sets of data. This release comes with our first data migration process which could be run as background tasks.","title":"Fixes"},{"location":"releases/2018-09-24/#resiliencies","text":"Our APIs are the foundations of our collaborative applications. This release puts under constant monitoring all our HTTP endpoints to track number of requests and their execution times. A Service Level Objective (SLO) is the key element of a Service Level Agreement (SLA) as it defines the measurement process. This release introduces and actively monitors the first SLO of SEKOIA.IO: our HTTP APIs success rate. Belt and suspenders! Our new shiny alert management system, based on Prometheus, provides a faster outage notifications mechanism.","title":"Resiliencies"},{"location":"releases/2018-09-24/#forecasts","text":"In SIC, we identified the need for a faster way to evaluate the operational impact of a detection rule. We designed a new page to expose statistics and aggregated information on all the alerts and incidents that are related to a selected rule. A key feature of SIC is its capacity of identifying the source and the target of a threat. To go even further, we offer a new page that consolidates all the information our detection engines collect on one source/target. For instance, this page will provide details on all the alerts, incidents and rules that are related to an IP address. SEKOIA.IO is built to support inter-application collaborations. As part of this, we designed an \u201caction card\u201d to ease user interactions between applications. More information in the next releases, stay tuned! If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Forecasts"},{"location":"releases/2018-10-01/","text":"01/10/2018 This release focuses on the SIC application with more collaboratives features and resiliency. Collaborative Applications A neat example of two applications that collaborate for a better user experience is the relationship between inThreat and SIC applications. We increased this collaboration by referencing inThreat indicators in the alert produced by SIC. A SIC operator can easily query the inThreat CTI database to improve his comprehension of a threat. Another illustration of such collaboration is the SIC \u201caction card\u201d. This new feature allows one to easily do actions based on a clicked data, such as filter the current display, create a new incident or query the inThreat application to trigger an investigation process. Resiliencies We updated our alert management tool to handle system and business alerts sent by our monitoring system. Besides, we took the opportunity to upgrade the entire monitoring and alerting systems to the latest upstream versions. We improved the overall security and stability of the platform by enabling continuous delivery of security and system updates on all of our servers. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"01/10/2018"},{"location":"releases/2018-10-01/#01102018","text":"This release focuses on the SIC application with more collaboratives features and resiliency.","title":"01/10/2018"},{"location":"releases/2018-10-01/#collaborative-applications","text":"A neat example of two applications that collaborate for a better user experience is the relationship between inThreat and SIC applications. We increased this collaboration by referencing inThreat indicators in the alert produced by SIC. A SIC operator can easily query the inThreat CTI database to improve his comprehension of a threat. Another illustration of such collaboration is the SIC \u201caction card\u201d. This new feature allows one to easily do actions based on a clicked data, such as filter the current display, create a new incident or query the inThreat application to trigger an investigation process.","title":"Collaborative Applications"},{"location":"releases/2018-10-01/#resiliencies","text":"We updated our alert management tool to handle system and business alerts sent by our monitoring system. Besides, we took the opportunity to upgrade the entire monitoring and alerting systems to the latest upstream versions. We improved the overall security and stability of the platform by enabling continuous delivery of security and system updates on all of our servers. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Resiliencies"},{"location":"releases/2018-10-08/","text":"08/10/2018 This release focuses on the SIC application with performance improvements and enhanced statistics to empower users in creating fine-grained based detection rules. SIC Application SIC security engines rely on user defined rules to perform their detection. These latter needs to be analyzed and adjusted by the teams running SOCs in order to enhance accuracy of raised alerts. To assess rules efficiency, the statistics API of the SIC application was improved to report more details on the effectiveness of detection rules. One can now assess the quality of its ruleset by querying how many alerts were raised given a rule and a specified time period. In addition to the quality of the detection, we also expect our engines to process security events on a real time basis. We significantly optimized our sighting generation process used by our correlation mechanisms to reduce the overall analysis duration. One more step forward our objective of a 1s processing time. As said before, we want to ensure high speed processing events and avoid bottlenecks across our engines. We have added new metrics to help us monitor any lag that could appear between the various components of our workflow. Hence, it helps us to identify if one particular component of the SIC workflow needs to be scaled up to provide more treatment capacity. Resiliencies A lot of our services rely on databases which have a sensible impact on the global responsiveness of the platform. In order to always maintain high performances on our databases, we reduced the amount of I/O on highly sollicited disks. We implemented this major change by adding new hard drives for data that doesn\u2019t require low latency (backups are a good example of such data). If you have any concerns, feel free to contact us at support@sekoia.io .","title":"08/10/2018"},{"location":"releases/2018-10-08/#08102018","text":"This release focuses on the SIC application with performance improvements and enhanced statistics to empower users in creating fine-grained based detection rules.","title":"08/10/2018"},{"location":"releases/2018-10-08/#sic-application","text":"SIC security engines rely on user defined rules to perform their detection. These latter needs to be analyzed and adjusted by the teams running SOCs in order to enhance accuracy of raised alerts. To assess rules efficiency, the statistics API of the SIC application was improved to report more details on the effectiveness of detection rules. One can now assess the quality of its ruleset by querying how many alerts were raised given a rule and a specified time period. In addition to the quality of the detection, we also expect our engines to process security events on a real time basis. We significantly optimized our sighting generation process used by our correlation mechanisms to reduce the overall analysis duration. One more step forward our objective of a 1s processing time. As said before, we want to ensure high speed processing events and avoid bottlenecks across our engines. We have added new metrics to help us monitor any lag that could appear between the various components of our workflow. Hence, it helps us to identify if one particular component of the SIC workflow needs to be scaled up to provide more treatment capacity.","title":"SIC Application"},{"location":"releases/2018-10-08/#resiliencies","text":"A lot of our services rely on databases which have a sensible impact on the global responsiveness of the platform. In order to always maintain high performances on our databases, we reduced the amount of I/O on highly sollicited disks. We implemented this major change by adding new hard drives for data that doesn\u2019t require low latency (backups are a good example of such data). If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Resiliencies"},{"location":"releases/2018-10-15/","text":"15/10/2018 This release brings many improvements on the alert contextualization processes and threats descriptions for better insights on the threats your assets are facing. InThreat Application Faster and more reliable indexation of our Cyber Threat Intelligence indicators. inThreat is Sekoia\u2019s Cyber Threat Intelligence (CTI) solution. One of its mission is provide a knowledge base related to cybercrime and cyberthreats which is fed by our collectors and by our users. To ensure the best operational capacity for this database, we perform a complete and structured indexing of all of the observation collected. Such operation is expensive, hence, to ensure the best user experience, we implemented asynchronous indexing of the collected CTI observations as proposed in the TAXIIv2 standard. Improved threat descriptions of our CTI indicators. Our Cyber Threat Intelligence knowledge base is used, amongst others, by SIC, the detection solution implemented on SEKOIA.IO. To ease the understanding of our indicators by all of our users, we have improved the description related to malwares, threat related tools and attack patterns. This let final users understand precisely the impact of an alert. Enhanced descriptions are well formatted, free of technical terms and contextualized with information regarding to their provenance. SIC Application Detection rules applied to multiple entities SIC security engines rely on user defined rules to perform their detection that apply to a given set of supervised entities. To simplify the day-to-day work of SIC operators, this release enables the creation of detections rules that easily apply to multiple entities. This new feature drastically reduces the required effort to create and maintain a fine grained-based detection strategy. New \u201crule statistics\u201d page in the SIC frontend Recently, SIC introduced statistics about detection rules. A new page that leverages these new metrics was designed to help SIC operators to improve their detection ruleset. Among others, this page introduces a detailed graph that shows how the number of alerts evolves given a period of time and if the rule raised high urgency alerts. Hosting New backup monitoring system Backup is a very important point for our infrastructure that must provide a way to quickly recover from any crash situation or retrieve erroneously removed data. To enhance the confidence we have in our database level backups, a new monitoring mechanism was developed from scratch. This monitoring allows our operational teams to be notified if one of our backup job failed to proceed automatically so they can intervene and dramatically reduce the risk of any data loss. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"15/10/2018"},{"location":"releases/2018-10-15/#15102018","text":"This release brings many improvements on the alert contextualization processes and threats descriptions for better insights on the threats your assets are facing.","title":"15/10/2018"},{"location":"releases/2018-10-15/#inthreat-application","text":"Faster and more reliable indexation of our Cyber Threat Intelligence indicators. inThreat is Sekoia\u2019s Cyber Threat Intelligence (CTI) solution. One of its mission is provide a knowledge base related to cybercrime and cyberthreats which is fed by our collectors and by our users. To ensure the best operational capacity for this database, we perform a complete and structured indexing of all of the observation collected. Such operation is expensive, hence, to ensure the best user experience, we implemented asynchronous indexing of the collected CTI observations as proposed in the TAXIIv2 standard. Improved threat descriptions of our CTI indicators. Our Cyber Threat Intelligence knowledge base is used, amongst others, by SIC, the detection solution implemented on SEKOIA.IO. To ease the understanding of our indicators by all of our users, we have improved the description related to malwares, threat related tools and attack patterns. This let final users understand precisely the impact of an alert. Enhanced descriptions are well formatted, free of technical terms and contextualized with information regarding to their provenance.","title":"InThreat Application"},{"location":"releases/2018-10-15/#sic-application","text":"","title":"SIC Application"},{"location":"releases/2018-10-15/#detection-rules-applied-to-multiple-entities","text":"SIC security engines rely on user defined rules to perform their detection that apply to a given set of supervised entities. To simplify the day-to-day work of SIC operators, this release enables the creation of detections rules that easily apply to multiple entities. This new feature drastically reduces the required effort to create and maintain a fine grained-based detection strategy.","title":"Detection rules applied to multiple entities"},{"location":"releases/2018-10-15/#new-rule-statistics-page-in-the-sic-frontend","text":"Recently, SIC introduced statistics about detection rules. A new page that leverages these new metrics was designed to help SIC operators to improve their detection ruleset. Among others, this page introduces a detailed graph that shows how the number of alerts evolves given a period of time and if the rule raised high urgency alerts.","title":"New \u201crule statistics\u201d page in the SIC frontend"},{"location":"releases/2018-10-15/#hosting","text":"","title":"Hosting"},{"location":"releases/2018-10-15/#new-backup-monitoring-system","text":"Backup is a very important point for our infrastructure that must provide a way to quickly recover from any crash situation or retrieve erroneously removed data. To enhance the confidence we have in our database level backups, a new monitoring mechanism was developed from scratch. This monitoring allows our operational teams to be notified if one of our backup job failed to proceed automatically so they can intervene and dramatically reduce the risk of any data loss. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"New backup monitoring system"},{"location":"releases/2018-10-29/","text":"29/10/2018 This release provides new investigation features for SIC operators to optimize their workload while improving the quality of their analysis. SIC Application New page to display either sources or targets implied in alerts In order to qualify alerts, SIC operators need to have more information about nodes involved in an alert. The new \u201csource/target\u201d page allows one to gain insights on a node. In addition to statistics such as the number of alerts raised by that node or the number of referenced incidents, It also provides the list of the most meaningful alerts and incidents that leads to that node. Bulk actions on alerts SIC operators often apply the same treatment to multiple alerts, for example to change their statuses. To enhance user experience, the ability to perform bulk actions on alerts. This features speeds up the work of SIC operators by avoiding the repetition of the same operation several times. It allows operators to focus more deeply on alerts. SEKOIA.IO Enhanced datastore performances and resiliency for Docker Swarm servers All our virtual machines, which includes servers that are taking part of the Docker Swarm cluster, stores their data on \u201cdatastores\u201d. In order to improve resiliency and performance from an I/O perspective, we have added new datastores. This means that if one of the datastore is overloaded, this won\u2019t have a negative impact on the whole platform. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"29/10/2018"},{"location":"releases/2018-10-29/#29102018","text":"This release provides new investigation features for SIC operators to optimize their workload while improving the quality of their analysis.","title":"29/10/2018"},{"location":"releases/2018-10-29/#sic-application","text":"","title":"SIC Application"},{"location":"releases/2018-10-29/#new-page-to-display-either-sources-or-targets-implied-in-alerts","text":"In order to qualify alerts, SIC operators need to have more information about nodes involved in an alert. The new \u201csource/target\u201d page allows one to gain insights on a node. In addition to statistics such as the number of alerts raised by that node or the number of referenced incidents, It also provides the list of the most meaningful alerts and incidents that leads to that node.","title":"New page to display either sources or targets implied in alerts"},{"location":"releases/2018-10-29/#bulk-actions-on-alerts","text":"SIC operators often apply the same treatment to multiple alerts, for example to change their statuses. To enhance user experience, the ability to perform bulk actions on alerts. This features speeds up the work of SIC operators by avoiding the repetition of the same operation several times. It allows operators to focus more deeply on alerts.","title":"Bulk actions on alerts"},{"location":"releases/2018-10-29/#sekoiaio","text":"","title":"SEKOIA.IO"},{"location":"releases/2018-10-29/#enhanced-datastore-performances-and-resiliency-for-docker-swarm-servers","text":"All our virtual machines, which includes servers that are taking part of the Docker Swarm cluster, stores their data on \u201cdatastores\u201d. In order to improve resiliency and performance from an I/O perspective, we have added new datastores. This means that if one of the datastore is overloaded, this won\u2019t have a negative impact on the whole platform. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Enhanced datastore performances and resiliency for Docker Swarm servers"},{"location":"releases/2018-12-03/","text":"03/12/2018 This release brings a set of major features such as the support for the Delegation of permissions and the first components of a new SIC detection engine SIC Application Profile pictures are very helpful elements in our interfaces. It allows SIC operators to easily know who did an action and gain some details on the underlying reasons. A simple but really effective optimization was done on our web pages to reduce the number of queries needed to retrieve profile pictures. A SIC operator can preset the severity of a rule through its definition in its related template. Ingested events are enriched with a short description, a source and a target. A SIC Operatior can leverage these information to get a clear and concise definition of the events that triggered the alert he is reviewing. A SIC operator can request a notification with the content of the alert for every updates made on an alert (new comments, status changes, counter-measure updates, \u2026). This notification is sent through the EventsAPI and is available as a stream. Two assets can now share the same name to support any naming strategy followed by the user. SEKOIA.IO There are avatars, API keys, and applications that require access to data and actions from other communities. To address this need, we added a Delegation mechanism to our APIs. A community can delegate a set of permissions to avatars or API keys of another community. The delegation is revocable at any time and community administrators can audit all delegations in their communities. A validity deadline can be set when creating the delegation. SEKOIA.io database architecture is now based on a central database, hosted on multiple replicated servers. This release brings an automatic server failure detection to trigger the re-election of the principal database among the available databases thus improving our SLOs. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"03/12/2018"},{"location":"releases/2018-12-03/#03122018","text":"This release brings a set of major features such as the support for the Delegation of permissions and the first components of a new SIC detection engine","title":"03/12/2018"},{"location":"releases/2018-12-03/#sic-application","text":"Profile pictures are very helpful elements in our interfaces. It allows SIC operators to easily know who did an action and gain some details on the underlying reasons. A simple but really effective optimization was done on our web pages to reduce the number of queries needed to retrieve profile pictures. A SIC operator can preset the severity of a rule through its definition in its related template. Ingested events are enriched with a short description, a source and a target. A SIC Operatior can leverage these information to get a clear and concise definition of the events that triggered the alert he is reviewing. A SIC operator can request a notification with the content of the alert for every updates made on an alert (new comments, status changes, counter-measure updates, \u2026). This notification is sent through the EventsAPI and is available as a stream. Two assets can now share the same name to support any naming strategy followed by the user.","title":"SIC Application"},{"location":"releases/2018-12-03/#sekoiaio","text":"There are avatars, API keys, and applications that require access to data and actions from other communities. To address this need, we added a Delegation mechanism to our APIs. A community can delegate a set of permissions to avatars or API keys of another community. The delegation is revocable at any time and community administrators can audit all delegations in their communities. A validity deadline can be set when creating the delegation. SEKOIA.io database architecture is now based on a central database, hosted on multiple replicated servers. This release brings an automatic server failure detection to trigger the re-election of the principal database among the available databases thus improving our SLOs. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"SEKOIA.IO"},{"location":"releases/2019-01-25/","text":"25/01/2019 This major release brings a large set of new features and bug fixes that improve the configuration, the detection and the qualification of security alerts. Inboarding We redesigned the inboarding process to simplify its execution and reduce frictions. A 7-day free trial period is made available at try.sekoia.io for new members. This trial can be extended up to 25days if the user validates trophies that are destributed along traditionnal user path. A contextual menu offers a precise vision of both validated and not validated trophies. The landing page of SEKOIA.IO has been refreshed to improve the user experience and promote the new usages of the platform. Configuration The user can define the list of the probes he/she wishes to connect to SEKOIA.IO. The intake feature automates the ingestion of events produced by connected probes. This version brings seven different supported formats of probes: Suricata, OpenSSH, HAProxy, Nginx, Netfilter, Squid. Detection A new detection engine is introduced with this version: the threshold engine. This major feature enables the creation fo rules to detect telemetric variations on both the number, the volume and the frequency of the events. Qualification The data model for an alert has been expanded to explicitly link a comment to a state change of the alert. This feature has been introduced so that a user can explain and justify any modification he/she introduced on an alert. A better description of an alert, the associated attack and its structure is offered to the user through the visualization of the estimated step in the Cyber KillChain the alert denotes. Reporting New metrics are offered to the user to improve his understanding of its orchestration. Among these indicators, we can cite the reporting of the type of handled events. The Alert Risk Indicator computation process has been updated to ease its comprehension. The computation is now performed on at most 10 days and only ongoing, closed and rejected alerts are considered. The new statistics API replaces the Stats API to provide a broader set of new counters, datasets and key risk indicators. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"25/01/2019"},{"location":"releases/2019-01-25/#25012019","text":"This major release brings a large set of new features and bug fixes that improve the configuration, the detection and the qualification of security alerts.","title":"25/01/2019"},{"location":"releases/2019-01-25/#inboarding","text":"We redesigned the inboarding process to simplify its execution and reduce frictions. A 7-day free trial period is made available at try.sekoia.io for new members. This trial can be extended up to 25days if the user validates trophies that are destributed along traditionnal user path. A contextual menu offers a precise vision of both validated and not validated trophies. The landing page of SEKOIA.IO has been refreshed to improve the user experience and promote the new usages of the platform.","title":"Inboarding"},{"location":"releases/2019-01-25/#configuration","text":"The user can define the list of the probes he/she wishes to connect to SEKOIA.IO. The intake feature automates the ingestion of events produced by connected probes. This version brings seven different supported formats of probes: Suricata, OpenSSH, HAProxy, Nginx, Netfilter, Squid.","title":"Configuration"},{"location":"releases/2019-01-25/#detection","text":"A new detection engine is introduced with this version: the threshold engine. This major feature enables the creation fo rules to detect telemetric variations on both the number, the volume and the frequency of the events.","title":"Detection"},{"location":"releases/2019-01-25/#qualification","text":"The data model for an alert has been expanded to explicitly link a comment to a state change of the alert. This feature has been introduced so that a user can explain and justify any modification he/she introduced on an alert. A better description of an alert, the associated attack and its structure is offered to the user through the visualization of the estimated step in the Cyber KillChain the alert denotes.","title":"Qualification"},{"location":"releases/2019-01-25/#reporting","text":"New metrics are offered to the user to improve his understanding of its orchestration. Among these indicators, we can cite the reporting of the type of handled events. The Alert Risk Indicator computation process has been updated to ease its comprehension. The computation is now performed on at most 10 days and only ongoing, closed and rejected alerts are considered. The new statistics API replaces the Stats API to provide a broader set of new counters, datasets and key risk indicators. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Reporting"},{"location":"releases/2019-05-06/","text":"06/05/2019 Because we care about your experience on our platform and we want to make it easier for you to track all of your cyberdefense operations, we are happy to announce the launch of our newest feature on SEKOIA.IO: the Security Performance dashboard. Security Performance is a key performance indicator (KPI) screen that allows users to assess, measure and evaluate the proper functioning of the Security Operations Center and all the defense operations happening on the platform. Thanks to a selection of performance indicators, users can visually enjoy a follow-up on the different parameters (rules/alerts/...) while being able to measure the impact of their decision-making in real time. From a technical point of view, the Security Performance is a board where graphs, figures and metrics are listed to track performance and progress while making sense of all the security performance measures. The data provided on Security Performance is available through the fresh Statistics API and its new endpoints listed in this documentation . Here are the main performance indicators available on the Security Performance feature: Number of new alerts. Number of new incidents. Backlog line shows the proportion of alerts that have been processed or completed. Auto processed alerts refers to the proportion of automatically processed alerts. Reaction Time displays the average time to start processing an alert. Rules refers to the number of detection rules that have been triggered. Alert Workflow Duration shows the time needed to acknowledge and investigate an alert upon the creation of this one. Precision map indicates the proportion of alerts validated by the operators compared to those rejected. It\u2019s an estimation of the accuracy of users\u2019 rules. Time to close shows the average time necessary to close/resolve an alert. Operators\u2019 Activity traces all actions performed by human operators. Time to resolution by severity helps operators distinguish between the most severe alerts and the less important ones by displaying severity and the time needed to resolve each one. Alert by category shows the most frequent categories of alerts. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"06/05/2019"},{"location":"releases/2019-05-06/#06052019","text":"Because we care about your experience on our platform and we want to make it easier for you to track all of your cyberdefense operations, we are happy to announce the launch of our newest feature on SEKOIA.IO: the Security Performance dashboard. Security Performance is a key performance indicator (KPI) screen that allows users to assess, measure and evaluate the proper functioning of the Security Operations Center and all the defense operations happening on the platform. Thanks to a selection of performance indicators, users can visually enjoy a follow-up on the different parameters (rules/alerts/...) while being able to measure the impact of their decision-making in real time. From a technical point of view, the Security Performance is a board where graphs, figures and metrics are listed to track performance and progress while making sense of all the security performance measures. The data provided on Security Performance is available through the fresh Statistics API and its new endpoints listed in this documentation . Here are the main performance indicators available on the Security Performance feature: Number of new alerts. Number of new incidents. Backlog line shows the proportion of alerts that have been processed or completed. Auto processed alerts refers to the proportion of automatically processed alerts. Reaction Time displays the average time to start processing an alert. Rules refers to the number of detection rules that have been triggered. Alert Workflow Duration shows the time needed to acknowledge and investigate an alert upon the creation of this one. Precision map indicates the proportion of alerts validated by the operators compared to those rejected. It\u2019s an estimation of the accuracy of users\u2019 rules. Time to close shows the average time necessary to close/resolve an alert. Operators\u2019 Activity traces all actions performed by human operators. Time to resolution by severity helps operators distinguish between the most severe alerts and the less important ones by displaying severity and the time needed to resolve each one. Alert by category shows the most frequent categories of alerts. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"06/05/2019"},{"location":"releases/2019-06-18/","text":"18/06/2019 The Fresh New UI At SEKOIA.IO, we feel committed to ensuring that our users have the best experience in terms of navigation and usability. This is why we are excited to announce the new UI as the first step of our long list of user interface enhancements. Main Menu Navigation The first thing we\u2019ve changed has to do with the overall navigation on the platform. We\u2019ve moved the main menu which was on the right to a bigger, more intuitive and more responsive one on the left, putting all the sections of the platform in one place to give the user a direct access to every functionality of our platform. The sticky menu will automatically expand when hovering over it, but users can choose to keep it open or close it by using the new Hamburger button \u201cKeep Open\u201d. Even if it is reduced, a set of icons will still be visible, making it easier to navigate from one page to the other. On top of each page, a breadcrumb has been integrated to let users know how one page is nested within other pages and to provide them with direct links to previous pages without having to bother with the back button. Before After Another change has to do with the display of the different tables on the platform. As you know, at SEKOIA.IO we use many complex tables with a lot of data on them. To draw the user's eye to the data that matters most, we've decided to make these tables responsive and thus taking a wider place on the screen depending on what kind of device the user is on. We also got rid of the blue border on the left of each table which used to distract from the data in those tables, and we changed some buttons and icons to have a more consistent design all over the platform. Before After We would like to inform our users that this upgrade will not degrade any existing functionality and does not require upgrade costs or system requirement changes. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"18/06/2019"},{"location":"releases/2019-06-18/#18062019","text":"","title":"18/06/2019"},{"location":"releases/2019-06-18/#the-fresh-new-ui","text":"At SEKOIA.IO, we feel committed to ensuring that our users have the best experience in terms of navigation and usability. This is why we are excited to announce the new UI as the first step of our long list of user interface enhancements.","title":"The Fresh New UI"},{"location":"releases/2019-06-18/#main-menu-navigation","text":"The first thing we\u2019ve changed has to do with the overall navigation on the platform. We\u2019ve moved the main menu which was on the right to a bigger, more intuitive and more responsive one on the left, putting all the sections of the platform in one place to give the user a direct access to every functionality of our platform. The sticky menu will automatically expand when hovering over it, but users can choose to keep it open or close it by using the new Hamburger button \u201cKeep Open\u201d. Even if it is reduced, a set of icons will still be visible, making it easier to navigate from one page to the other. On top of each page, a breadcrumb has been integrated to let users know how one page is nested within other pages and to provide them with direct links to previous pages without having to bother with the back button. Before After Another change has to do with the display of the different tables on the platform. As you know, at SEKOIA.IO we use many complex tables with a lot of data on them. To draw the user's eye to the data that matters most, we've decided to make these tables responsive and thus taking a wider place on the screen depending on what kind of device the user is on. We also got rid of the blue border on the left of each table which used to distract from the data in those tables, and we changed some buttons and icons to have a more consistent design all over the platform. Before After We would like to inform our users that this upgrade will not degrade any existing functionality and does not require upgrade costs or system requirement changes. If you have any concerns, feel free to contact us at support@sekoia.io .","title":"Main Menu Navigation"},{"location":"releases/2020-02-14/","text":"14/02/2020 This release page is now part of our newly designed website SEKOIA.IO on which you can find all the information about our products (presentation, use cases, pricing...) as well as a documentation and our contact if needed. Detection Rules There are some major changes that happened to the rules' page. First, we've added five new columns (entities, assets, total of produced alerts, rejected alerts and filtered alerts in the last 7 days) to add more context and comprehension to each rule. We've also put a toggle button on each row to quickly enable or disable a rule if needed. CTI and Correlation icons act now as the rule type's indicator, and if there is a compilation error, it can be clearly visible in the Status column (it's all red, you can't miss it). By clicking on a rule, we are redirected to a rule's main page which also got a radical makeover. A secondary menu is now visible on the left with all the rules listed as well as their current status to ease the navigation and prevent the user from having to go back to the rules table. A new banner is displayed at the top with the rule's main information like the name, type, and last update as well as an action button sticked on the right. Six collapsable sections are hanging under it to accommodate our users' needs to configure their rules as they please. We've included a new feature in the rule creation: alert filters , which enables our users to filter alerts given custom criteria by specifying an exclusion pattern. And last but not least, it is now possible to create a rule without a template, to save a rule configuration as a template and to use a pre-existing template to fill out information about a rule! Events page Before, we used to list only the last 50 events gathered by our intakes. Now, you can scroll indefinitely in your last events list, but also look for something specific in your events by using the dork language . Intakes Speaking of which, you can now see all of your intakes listed in a separate page with all the information you need to make sense of them. In addition to the intake format and intake key, you can now see the events and errors produced by the intake. If you haven't already, take a look at our on-going list of integrations and let us know if you don't find the format you're looking for. We can make them available for you. :-) User Center To enhance our users' experience on the platform, we created the \"User Center\". From there, it is possible to manage your profile as well as your numerous communities, members, roles, API keys, and delegations. You can access it through the contextual menu on the upper right of your screen.","title":"14/02/2020"},{"location":"releases/2020-02-14/#14022020","text":"This release page is now part of our newly designed website SEKOIA.IO on which you can find all the information about our products (presentation, use cases, pricing...) as well as a documentation and our contact if needed.","title":"14/02/2020"},{"location":"releases/2020-02-14/#detection-rules","text":"There are some major changes that happened to the rules' page. First, we've added five new columns (entities, assets, total of produced alerts, rejected alerts and filtered alerts in the last 7 days) to add more context and comprehension to each rule. We've also put a toggle button on each row to quickly enable or disable a rule if needed. CTI and Correlation icons act now as the rule type's indicator, and if there is a compilation error, it can be clearly visible in the Status column (it's all red, you can't miss it). By clicking on a rule, we are redirected to a rule's main page which also got a radical makeover. A secondary menu is now visible on the left with all the rules listed as well as their current status to ease the navigation and prevent the user from having to go back to the rules table. A new banner is displayed at the top with the rule's main information like the name, type, and last update as well as an action button sticked on the right. Six collapsable sections are hanging under it to accommodate our users' needs to configure their rules as they please. We've included a new feature in the rule creation: alert filters , which enables our users to filter alerts given custom criteria by specifying an exclusion pattern. And last but not least, it is now possible to create a rule without a template, to save a rule configuration as a template and to use a pre-existing template to fill out information about a rule!","title":"Detection Rules"},{"location":"releases/2020-02-14/#events-page","text":"Before, we used to list only the last 50 events gathered by our intakes. Now, you can scroll indefinitely in your last events list, but also look for something specific in your events by using the dork language .","title":"Events page"},{"location":"releases/2020-02-14/#intakes","text":"Speaking of which, you can now see all of your intakes listed in a separate page with all the information you need to make sense of them. In addition to the intake format and intake key, you can now see the events and errors produced by the intake. If you haven't already, take a look at our on-going list of integrations and let us know if you don't find the format you're looking for. We can make them available for you. :-)","title":"Intakes"},{"location":"releases/2020-02-14/#user-center","text":"To enhance our users' experience on the platform, we created the \"User Center\". From there, it is possible to manage your profile as well as your numerous communities, members, roles, API keys, and delegations. You can access it through the contextual menu on the upper right of your screen.","title":"User Center"},{"location":"releases/2020-07-07/","text":"07/07/2020 Improvement of the display of \"Observed Data\" on Alert page Significant work has been done on the display of the detail elements of an alert. The work mainly focused on \u201cObserved Data\u201d, which corresponds to the SEKOIA.IO internal representation of all the events collected. Here is the new display of an Observed Data: Metadata of an \"Observed Data\" A new block has been added at the top of the screen to display the metadata associated with an \"Observed Data\". It is thus possible to distinguish at first glance the type and format of the observed data, its relative date, as well as the entity for which the observation was made. Several redundant or technical information has been removed from this display area, but it is still possible to access all of the raw \"Observed Data\" information by clicking on the icon '<>'. Display of objects associated with \"Observed Data\" (SCO) Various changes have been made to the display of SCOs in an Observed Data. SCOs are the \u201cSTIX Cyber-observable Objects\u201d, corresponding to the observable details objects, such as IP addresses, domain names, ports, file names, e-mail addresses, etc. SCOs involved in raising the alert are now highlighted in blue and unrolled by default. We can therefore understand the alert more quickly and start analyzing it immediately. In addition, any \u201ctags\u201d associated with SCOs are displayed directly on the title of the SCO. These \u201ctags\u201d correspond to enrichments by SEKOIA.IO of the events received. A flag icon is displayed in place of the text for the \u201ctags\u201d designating countries. Raw events Raw events, as sent by your systems, are now available directly on the page describing an \"Observed Data\". Full raw event is available by clicking on icon '<>'.","title":"07/07/2020"},{"location":"releases/2020-07-07/#07072020","text":"","title":"07/07/2020"},{"location":"releases/2020-07-07/#improvement-of-the-display-of-observed-data-on-alert-page","text":"Significant work has been done on the display of the detail elements of an alert. The work mainly focused on \u201cObserved Data\u201d, which corresponds to the SEKOIA.IO internal representation of all the events collected. Here is the new display of an Observed Data:","title":"Improvement of the display of  \"Observed Data\" on Alert page"},{"location":"releases/2020-07-07/#metadata-of-an-observed-data","text":"A new block has been added at the top of the screen to display the metadata associated with an \"Observed Data\". It is thus possible to distinguish at first glance the type and format of the observed data, its relative date, as well as the entity for which the observation was made. Several redundant or technical information has been removed from this display area, but it is still possible to access all of the raw \"Observed Data\" information by clicking on the icon '<>'.","title":"Metadata of an \"Observed Data\""},{"location":"releases/2020-07-07/#display-of-objects-associated-with-observed-data-sco","text":"Various changes have been made to the display of SCOs in an Observed Data. SCOs are the \u201cSTIX Cyber-observable Objects\u201d, corresponding to the observable details objects, such as IP addresses, domain names, ports, file names, e-mail addresses, etc. SCOs involved in raising the alert are now highlighted in blue and unrolled by default. We can therefore understand the alert more quickly and start analyzing it immediately. In addition, any \u201ctags\u201d associated with SCOs are displayed directly on the title of the SCO. These \u201ctags\u201d correspond to enrichments by SEKOIA.IO of the events received. A flag icon is displayed in place of the text for the \u201ctags\u201d designating countries.","title":"Display of objects associated with \"Observed Data\" (SCO)"},{"location":"releases/2020-07-07/#raw-events","text":"Raw events, as sent by your systems, are now available directly on the page describing an \"Observed Data\". Full raw event is available by clicking on icon '<>'.","title":"Raw events"},{"location":"releases/2020-07-22/","text":"22/07/2020 Improved display of \u201cObserved Data\u201d on alert page Work on the detail page of an alert continued, always with the aim of facilitating the analysis of an alert and minimizing the presence of \u00ab noise \u00bb on the page. Differentiated display according to the types of SCO (STIX Cyber-observable Objects) The objects (SCO) that make up an \u00ab Observed Data \u00bb now have a separate display according to their type. Thus, an object of type network-traffic is better laid out and some extensions, such as http-request-ext, are displayed in a more readable way. Display of raw events formatted in JSON It is now possible to display the raw events in a dedicated window from the display of an \u00abObserved Data\u00bb. If the raw event is in JSON format, then the latter is formatted to make it easier to read. Adding and modifying \u00ab Alert Filters \u00bb When developping detection rules, it is now possible to add and modify \u00ab Alert Filters \u00bb. These filters, written in STIX patterning, make it possible to prevent the raising of alerts when certain criteria are met by an \u00ab observed data \u00bb. In the example below, the rule is used to raise an alert as soon as network traffic is detected by sysmon and the \u00ab alert filters \u00bb are used to indicate which are the legitimate processes to carry network traffic, for which there must be no raised alert. Two-factor authentication You can now activate two-factor authentication to strengthen the security of your user account on SEKOIA.IO. All you have to do is go to the management page of your user account (accessible by clicking on the icon at the top right of the application, choose 'Settings'). As a second factor of authentication, you can use a TOTP (time-based one-time password) application such as Google Authenticator, Authy, LastPass Authenticator or 1Password.","title":"22/07/2020"},{"location":"releases/2020-07-22/#22072020","text":"","title":"22/07/2020"},{"location":"releases/2020-07-22/#improved-display-of-observed-data-on-alert-page","text":"Work on the detail page of an alert continued, always with the aim of facilitating the analysis of an alert and minimizing the presence of \u00ab noise \u00bb on the page.","title":"Improved display of \u201cObserved Data\u201d on alert page"},{"location":"releases/2020-07-22/#differentiated-display-according-to-the-types-of-sco-stix-cyber-observable-objects","text":"The objects (SCO) that make up an \u00ab Observed Data \u00bb now have a separate display according to their type. Thus, an object of type network-traffic is better laid out and some extensions, such as http-request-ext, are displayed in a more readable way.","title":"Differentiated display according to the types of SCO (STIX Cyber-observable Objects)"},{"location":"releases/2020-07-22/#display-of-raw-events-formatted-in-json","text":"It is now possible to display the raw events in a dedicated window from the display of an \u00abObserved Data\u00bb. If the raw event is in JSON format, then the latter is formatted to make it easier to read.","title":"Display of raw events formatted in JSON"},{"location":"releases/2020-07-22/#adding-and-modifying-alert-filters","text":"When developping detection rules, it is now possible to add and modify \u00ab Alert Filters \u00bb. These filters, written in STIX patterning, make it possible to prevent the raising of alerts when certain criteria are met by an \u00ab observed data \u00bb. In the example below, the rule is used to raise an alert as soon as network traffic is detected by sysmon and the \u00ab alert filters \u00bb are used to indicate which are the legitimate processes to carry network traffic, for which there must be no raised alert.","title":"Adding and modifying \u00ab Alert Filters \u00bb"},{"location":"releases/2020-07-22/#two-factor-authentication","text":"You can now activate two-factor authentication to strengthen the security of your user account on SEKOIA.IO. All you have to do is go to the management page of your user account (accessible by clicking on the icon at the top right of the application, choose 'Settings'). As a second factor of authentication, you can use a TOTP (time-based one-time password) application such as Google Authenticator, Authy, LastPass Authenticator or 1Password.","title":"Two-factor authentication"},{"location":"releases/2020-09-10/","text":"10/09/2020 Case Management: New case tracking system The Operation Center now has a new feature called \u201cCase Management\u201d, allowing information relating to security records to be shared. A \u201ccase\u201d consists of a title, description, severity, and may be associated with alerts from the Operation Center. A \u201ccase\u201d has a lifecycle (currently, open or closed) and can be assigned to one or more people. This feature allows, for example, a security supervision team to escalate alerts to another team for clarification or to group alerts that seem related to facilitate analysis. \u201cCase Management\u201d replaces the old \u201cincident\u201d management mechanism. Among the many changes, we can mention the possibility of defining tags, assigning a case to one or more members of its community, adding comments or even associating alerts with a \u201ccase\u201d. Finally, the syntax of the description of a \u201ccase\u201d has been changed to use the Markdown language (with a graphical editor in the Operation Center). Old \u201cincidents\u201d have been converted to \u201ccases\u201d. The switch to the use of \u201ccases\u201d is completely transparent, access to old \u201cincidents\u201d is automatically redirected to the new implementation. Consultation of events: redesign of the page The page for viewing community events sent to SEKOIA.IO has been completely redesigned. It becomes more adaptable and more functional. This is the first work here, other improvements to this page will come later (search in longer periods, complex searches, etc.). Events table The table displaying the list of events now provides the following possibilities: Adding or removing a column (except for the date of the event which is mandatory); Move a column by dragging and dropping the title; Extension of an event and thus display it in ECS, STIX or raw format; Adding a filter on events from a table cell. Search in events Search fields in events now follow the \"ECS\" standard (Elastic Common Schema). Thus, to search for events with source IP address 127.0.0.1 , enter source.ip: \"127.0.0.1\" (and no longer source: \"127.0.0.1\" ). This change provides access to more fields of events stored in Elasticsearch. So to perform a search (admittedly useless) for DNS type events that requested the IPv6 address associated with google.com , we can use the following Dork query: dns.question.name:\"google.com\" AND dns.question.type:\"AAAA\" A simple search creation wizard has also been added. It allows you to create rules even without knowing the Dork syntax, used on SEKOIA.IO. Saving searches The new form now offers the possibility of saving searches and finding them easily. The backup is currently performed within the web browser, so it will not be possible to access the saved searches from another machine. SEKOIA.IO documentation The code from the SEKOIA.IO documentation has been moved to a public space hosted on GitHub. Now it is possible for anyone to suggest changes to the documentation very easily, by clicking on the pen icon. Many help topics for using SEKOIA.IO have also been added to the documentation. You can of course always send us your comments and expectations with regard to this documentation, by emailing support .","title":"10/09/2020"},{"location":"releases/2020-09-10/#10092020","text":"","title":"10/09/2020"},{"location":"releases/2020-09-10/#case-management-new-case-tracking-system","text":"The Operation Center now has a new feature called \u201cCase Management\u201d, allowing information relating to security records to be shared. A \u201ccase\u201d consists of a title, description, severity, and may be associated with alerts from the Operation Center. A \u201ccase\u201d has a lifecycle (currently, open or closed) and can be assigned to one or more people. This feature allows, for example, a security supervision team to escalate alerts to another team for clarification or to group alerts that seem related to facilitate analysis. \u201cCase Management\u201d replaces the old \u201cincident\u201d management mechanism. Among the many changes, we can mention the possibility of defining tags, assigning a case to one or more members of its community, adding comments or even associating alerts with a \u201ccase\u201d. Finally, the syntax of the description of a \u201ccase\u201d has been changed to use the Markdown language (with a graphical editor in the Operation Center). Old \u201cincidents\u201d have been converted to \u201ccases\u201d. The switch to the use of \u201ccases\u201d is completely transparent, access to old \u201cincidents\u201d is automatically redirected to the new implementation.","title":"Case Management: New case tracking system"},{"location":"releases/2020-09-10/#consultation-of-events-redesign-of-the-page","text":"The page for viewing community events sent to SEKOIA.IO has been completely redesigned. It becomes more adaptable and more functional. This is the first work here, other improvements to this page will come later (search in longer periods, complex searches, etc.).","title":"Consultation of events: redesign of the page"},{"location":"releases/2020-09-10/#events-table","text":"The table displaying the list of events now provides the following possibilities: Adding or removing a column (except for the date of the event which is mandatory); Move a column by dragging and dropping the title; Extension of an event and thus display it in ECS, STIX or raw format; Adding a filter on events from a table cell.","title":"Events table"},{"location":"releases/2020-09-10/#search-in-events","text":"Search fields in events now follow the \"ECS\" standard (Elastic Common Schema). Thus, to search for events with source IP address 127.0.0.1 , enter source.ip: \"127.0.0.1\" (and no longer source: \"127.0.0.1\" ). This change provides access to more fields of events stored in Elasticsearch. So to perform a search (admittedly useless) for DNS type events that requested the IPv6 address associated with google.com , we can use the following Dork query: dns.question.name:\"google.com\" AND dns.question.type:\"AAAA\" A simple search creation wizard has also been added. It allows you to create rules even without knowing the Dork syntax, used on SEKOIA.IO.","title":"Search in events"},{"location":"releases/2020-09-10/#saving-searches","text":"The new form now offers the possibility of saving searches and finding them easily. The backup is currently performed within the web browser, so it will not be possible to access the saved searches from another machine.","title":"Saving searches"},{"location":"releases/2020-09-10/#sekoiaio-documentation","text":"The code from the SEKOIA.IO documentation has been moved to a public space hosted on GitHub. Now it is possible for anyone to suggest changes to the documentation very easily, by clicking on the pen icon. Many help topics for using SEKOIA.IO have also been added to the documentation. You can of course always send us your comments and expectations with regard to this documentation, by emailing support .","title":"SEKOIA.IO documentation"},{"location":"releases/2021-01-06/","text":"2021-01-06: Operation Center\u2019s Configurable Dashboard System SEKOIA.IO\u2019s Operation Center now brings a new dashboard mechanism, that is fully configurable and adaptable to all needs. This new feature is now enabled per default for all SEKOIA.IO customers. Dashboards are composed of modular widgets that can be placed where you want. Widgets can be configured to specify the date range, applied filters, display, etc. SEKOIA.IO comes with a pre-configured dashboard that gives a synthetic view of the current community activity, either from an operational security perspective (risk level, number of alerts, etc.) or from an activity perspective (list of last posted comments, last created alerts, etc.). All SEKOIA.IO users are able to create new dashboards that fit their specific needs. It\u2019s also possible to clone an existing dashboard. Provided Widgets To Monitor SEKOIA.IO\u2019s Operation Center Activity SEKOIA.IO\u2019s Operation Center provides several widgets: Widget Name Description Screenshot Alerts List of alerts, optionally filtered by their status and sorted either by their urgency, their frequency, etc. Number of Alerts Count the number of alerts, optionally filtered by their status and by their associated entity Risk Level Global risk level (ARI) for the current community Cases List of cases, optionally filtered by their status and sorted either by their urgency or they last updated date. Number of Cases Count the number of cases, optionally filtered by their status Number of Events by Data Source Number of events collected by source of data displayed as a list, a doughnut or an histogram. Entities Overview List synthetic view of entities with for each one the risk level, number of alerts and the number of collected events. Last Comments List of comments posted on items such as alerts or cases. Top Observed Threats Show list threats (malware, tool or attack-pattern) observed in alerts. If Intelligence Center is accessible to the current user, then, CTI wdigets will be made available: last intelligence reports, number of known threats, etc.","title":"2021-01-06: Operation Center\u2019s Configurable Dashboard System"},{"location":"releases/2021-01-06/#2021-01-06-operation-centers-configurable-dashboard-system","text":"SEKOIA.IO\u2019s Operation Center now brings a new dashboard mechanism, that is fully configurable and adaptable to all needs. This new feature is now enabled per default for all SEKOIA.IO customers. Dashboards are composed of modular widgets that can be placed where you want. Widgets can be configured to specify the date range, applied filters, display, etc. SEKOIA.IO comes with a pre-configured dashboard that gives a synthetic view of the current community activity, either from an operational security perspective (risk level, number of alerts, etc.) or from an activity perspective (list of last posted comments, last created alerts, etc.). All SEKOIA.IO users are able to create new dashboards that fit their specific needs. It\u2019s also possible to clone an existing dashboard.","title":"2021-01-06: Operation Center\u2019s Configurable Dashboard System"},{"location":"releases/2021-01-06/#provided-widgets-to-monitor-sekoiaios-operation-center-activity","text":"SEKOIA.IO\u2019s Operation Center provides several widgets: Widget Name Description Screenshot Alerts List of alerts, optionally filtered by their status and sorted either by their urgency, their frequency, etc. Number of Alerts Count the number of alerts, optionally filtered by their status and by their associated entity Risk Level Global risk level (ARI) for the current community Cases List of cases, optionally filtered by their status and sorted either by their urgency or they last updated date. Number of Cases Count the number of cases, optionally filtered by their status Number of Events by Data Source Number of events collected by source of data displayed as a list, a doughnut or an histogram. Entities Overview List synthetic view of entities with for each one the risk level, number of alerts and the number of collected events. Last Comments List of comments posted on items such as alerts or cases. Top Observed Threats Show list threats (malware, tool or attack-pattern) observed in alerts. If Intelligence Center is accessible to the current user, then, CTI wdigets will be made available: last intelligence reports, number of known threats, etc.","title":"Provided Widgets To Monitor SEKOIA.IO\u2019s Operation Center Activity"},{"location":"releases/2021-04-01/","text":"2021-04-01 Detection rules: New rules catalog On April 6, a new feature will be deployed in the Operation Center: The \u201cRules Catalog\u201d .This functionnality will replace the \u201cRules\u201d and \u201cRules Templates\u201d pages. The rules currently applied to your communities will be migrated into this new detection rule management system without any action on your part. With this new feature, you will have access to a set of rules developped and maintained by SEKOIA that you can easily apply to your events. Many filtering possibilities will be available to select the rules and you will be able to have an overview of the MITRE ATT&CK techniques for which rules are activated. You will find more information about this new feature in our documentation available here . Consultation of events: Improvements of the page The hunting capabilities in the events sent to SEKOIA.IO continue to be improved. You can now save your most common searches and navigate in the history of your searches. A new field called \u201csmart description\u201d, currently being implemented for the different event formats, allows \u201chuman\u201d reading of events and rapid filtering on certain information. A feature for exporting search results in csv or json formats has also been added. New Blogpost: Hunting and detecting Cobalt Strike Our new blog post is out ! Discover how to ensure a proactive and defensive posture against Cobalt Strike, one of the most powerful pentesting tools hijacked by attackers in their numerous campaigns.","title":"2021-04-01"},{"location":"releases/2021-04-01/#2021-04-01","text":"","title":"2021-04-01"},{"location":"releases/2021-04-01/#detection-rules-new-rules-catalog","text":"On April 6, a new feature will be deployed in the Operation Center: The \u201cRules Catalog\u201d .This functionnality will replace the \u201cRules\u201d and \u201cRules Templates\u201d pages. The rules currently applied to your communities will be migrated into this new detection rule management system without any action on your part. With this new feature, you will have access to a set of rules developped and maintained by SEKOIA that you can easily apply to your events. Many filtering possibilities will be available to select the rules and you will be able to have an overview of the MITRE ATT&CK techniques for which rules are activated. You will find more information about this new feature in our documentation available here .","title":"Detection rules: New rules catalog"},{"location":"releases/2021-04-01/#consultation-of-events-improvements-of-the-page","text":"The hunting capabilities in the events sent to SEKOIA.IO continue to be improved. You can now save your most common searches and navigate in the history of your searches. A new field called \u201csmart description\u201d, currently being implemented for the different event formats, allows \u201chuman\u201d reading of events and rapid filtering on certain information. A feature for exporting search results in csv or json formats has also been added.","title":"Consultation of events: Improvements of the page"},{"location":"releases/2021-04-01/#new-blogpost-hunting-and-detecting-cobalt-strike","text":"Our new blog post is out ! Discover how to ensure a proactive and defensive posture against Cobalt Strike, one of the most powerful pentesting tools hijacked by attackers in their numerous campaigns.","title":"New Blogpost: Hunting and detecting Cobalt Strike"},{"location":"releases/2021-06-01/","text":"2021-06-01 Start your own graph exploration from scratch. A new option has been added to the Graph explorations feature. You can now create your own graph explorations starting from scratch by simply selecting the desired element(s) in the \"Add Object(s)\" section to start your investigation. When a graph exploration is saved, it is available to all members of your community. 2 extra FLINTs each month. In addition to the FLINTs you receive several times a week, SEKOIA delivers an analysis of the latest trends and operating methods used by cybercriminal or state-run groups operating ransomware for extortion or sabotage purposes in FLINT format on a bi-monthly frequency. 3 new SEKOIA C2 trackers in May. In the pursuit of current threat monitoring, we have set up 3 new SEKOIA trackers during the month of May. They allow us to track the Command & Control infrastructure of the APT Gamaredon group and new MalleableC2 (Cobalt Strike) profiles used by affiliates of Ransomware groups. 25 new detection rules added to the catalog! Since last month, 25 new detection rules have been implemented in the SEKOIA.IO rules catalog. Several of them are related to : Escalation of suspicious and malicious alerts produced by the SentinelOne EDR. Detection of phishing and spear-phishing threats, including the downloading of documents with specific file extensions and from domains with specific TLDs. Detection of techniques aimed at disabling security tools (antivirus for example) by certain malware (such as Agent Tesla). Detection of a particular DLL loading technique (Gziploader) of the IcedID malware Detection of the STRRAT Trojan that pretends to be a ransomware to extort its victims by creating a scheduled task. Detection of the Phorpiex botnet used to distribute spam and other malware (cryptocurrency miners and ransomware) via certain techniques used to replicate on removable media. Enable new verified rules automatically! Activating new rules verified by SEKOIA.IO analysts can now be done automatically. Simply activate the option in the rule catalog settings and choose the desired effort level. Find your rules faster and easier thanks to the filters! The new filters available on the rule catalog greatly simplify their review. Find activated and deactivated rules in your community using the filters \"Rule: Enabled\" and \"Rule: Disabled\" respectively. You can also use the \"Rule: Custom\" and \"Rule: Verified\" filters to find rules created by you as well as those added by SEKOIA analysts. These filters allow you to enable or disable all selected rules with a mouse click.","title":"2021-06-01"},{"location":"releases/2021-06-01/#2021-06-01","text":"","title":"2021-06-01"},{"location":"releases/2021-06-01/#start-your-own-graph-exploration-from-scratch","text":"A new option has been added to the Graph explorations feature. You can now create your own graph explorations starting from scratch by simply selecting the desired element(s) in the \"Add Object(s)\" section to start your investigation. When a graph exploration is saved, it is available to all members of your community.","title":"Start your own graph exploration from scratch."},{"location":"releases/2021-06-01/#2-extra-flints-each-month","text":"In addition to the FLINTs you receive several times a week, SEKOIA delivers an analysis of the latest trends and operating methods used by cybercriminal or state-run groups operating ransomware for extortion or sabotage purposes in FLINT format on a bi-monthly frequency.","title":"2 extra FLINTs each month."},{"location":"releases/2021-06-01/#3-new-sekoia-c2-trackers-in-may","text":"In the pursuit of current threat monitoring, we have set up 3 new SEKOIA trackers during the month of May. They allow us to track the Command & Control infrastructure of the APT Gamaredon group and new MalleableC2 (Cobalt Strike) profiles used by affiliates of Ransomware groups.","title":"3 new SEKOIA C2 trackers in May."},{"location":"releases/2021-06-01/#25-new-detection-rules-added-to-the-catalog","text":"Since last month, 25 new detection rules have been implemented in the SEKOIA.IO rules catalog. Several of them are related to : Escalation of suspicious and malicious alerts produced by the SentinelOne EDR. Detection of phishing and spear-phishing threats, including the downloading of documents with specific file extensions and from domains with specific TLDs. Detection of techniques aimed at disabling security tools (antivirus for example) by certain malware (such as Agent Tesla). Detection of a particular DLL loading technique (Gziploader) of the IcedID malware Detection of the STRRAT Trojan that pretends to be a ransomware to extort its victims by creating a scheduled task. Detection of the Phorpiex botnet used to distribute spam and other malware (cryptocurrency miners and ransomware) via certain techniques used to replicate on removable media.","title":"25 new detection rules added to the catalog!"},{"location":"releases/2021-06-01/#enable-new-verified-rules-automatically","text":"Activating new rules verified by SEKOIA.IO analysts can now be done automatically. Simply activate the option in the rule catalog settings and choose the desired effort level.","title":"Enable new verified rules automatically!"},{"location":"releases/2021-06-01/#find-your-rules-faster-and-easier-thanks-to-the-filters","text":"The new filters available on the rule catalog greatly simplify their review. Find activated and deactivated rules in your community using the filters \"Rule: Enabled\" and \"Rule: Disabled\" respectively. You can also use the \"Rule: Custom\" and \"Rule: Verified\" filters to find rules created by you as well as those added by SEKOIA analysts. These filters allow you to enable or disable all selected rules with a mouse click.","title":"Find your rules faster and easier thanks to the filters!"},{"location":"releases/2021-07-05/","text":"2021-07-05 SEKOIA.IO's new visual identity! SEKOIA.IO's new visual identity! \ud83c\udfa8 New colors on the homepage of app.sekoia.io and on the new website sekoia.io, a gradient on three major colors that represents the fusion of the three essential components of our core expertise: Cyber Threat Intelligence, Threat Detection and Remediation. Have a look at sekoia.io to discover our vision, our products and our partner program! 30 new detection rules added to the catalog! Since the previous month, 30 new rules verified by our analysts have been added to the SEKOIA.IO XDR rules catalog. To protect you against the TOP 10 most exploded vulnerabilities of the last two years, we strongly recommend you to activate the following rules: CVE-2018-13379 (Fortinet FortiOS) CVE-2019-2725 (Oracle WebLogic Server) CVE-2019-11510 (Pulse Secure Pulse Connect Secure (PCS)) CVE-2020-0688 (Microsoft Exchange Server) CVE 2018-11776 (Apache Struts 2) These vulnerabilities are exploited ahead of ransomware attacks but also cyber spying attacks to gain initial access into their victims' information systems. Cyber Threat intelligence New intelligence source: Hatching Triage \ud83d\udd0e Our CTI database includes a new source of technical intelligence with the integration of the European sandbox Hatching Triage. This new source will reinforce our coverage of the most active malware of the moment such as Cobalt Strike, Agent tesla, LokiBot or IcedID. It provides our CTI database with IPs / domain names of Command & Control (C2) and hashes of about twenty malware. A blogpost will very soon give details of this new integration with our partner Hatching Triage, stay tuned! Tracking of Chinese APT groups In June, we strengthened our monitoring of C2 malware infrastructures like ShadowPad used by several threat actors attributed to China like APT41 or Winnti Group. What's new in the User Center ? The invitations \u2709\ufe0f The change of the invitation process makes it easier to : Assigning multiple roles to users: Different roles can be assigned at the same time when sending the invitation. Adding a user known to SEKOIA.IO: You can add existing users in other communities directly to a new one without going through the email invitation and authentication process. Your community on SEKOIA.IO The new \"Your community\" page now allows you to see with a single click: The date of the first log-in for all community members. The activation of the double authentication factor for all community members.","title":"2021-07-05"},{"location":"releases/2021-07-05/#2021-07-05","text":"","title":"2021-07-05"},{"location":"releases/2021-07-05/#sekoiaios-new-visual-identity","text":"SEKOIA.IO's new visual identity! \ud83c\udfa8 New colors on the homepage of app.sekoia.io and on the new website sekoia.io, a gradient on three major colors that represents the fusion of the three essential components of our core expertise: Cyber Threat Intelligence, Threat Detection and Remediation. Have a look at sekoia.io to discover our vision, our products and our partner program!","title":"SEKOIA.IO's new visual identity!"},{"location":"releases/2021-07-05/#30-new-detection-rules-added-to-the-catalog","text":"Since the previous month, 30 new rules verified by our analysts have been added to the SEKOIA.IO XDR rules catalog. To protect you against the TOP 10 most exploded vulnerabilities of the last two years, we strongly recommend you to activate the following rules: CVE-2018-13379 (Fortinet FortiOS) CVE-2019-2725 (Oracle WebLogic Server) CVE-2019-11510 (Pulse Secure Pulse Connect Secure (PCS)) CVE-2020-0688 (Microsoft Exchange Server) CVE 2018-11776 (Apache Struts 2) These vulnerabilities are exploited ahead of ransomware attacks but also cyber spying attacks to gain initial access into their victims' information systems.","title":"30 new detection rules added to the catalog!"},{"location":"releases/2021-07-05/#cyber-threat-intelligence","text":"","title":"Cyber Threat intelligence"},{"location":"releases/2021-07-05/#new-intelligence-source-hatching-triage","text":"Our CTI database includes a new source of technical intelligence with the integration of the European sandbox Hatching Triage. This new source will reinforce our coverage of the most active malware of the moment such as Cobalt Strike, Agent tesla, LokiBot or IcedID. It provides our CTI database with IPs / domain names of Command & Control (C2) and hashes of about twenty malware. A blogpost will very soon give details of this new integration with our partner Hatching Triage, stay tuned!","title":"New intelligence source: Hatching Triage \ud83d\udd0e"},{"location":"releases/2021-07-05/#tracking-of-chinese-apt-groups","text":"In June, we strengthened our monitoring of C2 malware infrastructures like ShadowPad used by several threat actors attributed to China like APT41 or Winnti Group.","title":"Tracking of Chinese APT groups"},{"location":"releases/2021-07-05/#whats-new-in-the-user-center","text":"","title":"What's new in the User Center ?"},{"location":"releases/2021-07-05/#the-invitations","text":"The change of the invitation process makes it easier to : Assigning multiple roles to users: Different roles can be assigned at the same time when sending the invitation. Adding a user known to SEKOIA.IO: You can add existing users in other communities directly to a new one without going through the email invitation and authentication process.","title":"The invitations \u2709\ufe0f"},{"location":"releases/2021-07-05/#your-community-on-sekoiaio","text":"The new \"Your community\" page now allows you to see with a single click: The date of the first log-in for all community members. The activation of the double authentication factor for all community members.","title":"Your community on SEKOIA.IO"},{"location":"searching/dork/","text":"Dork Dork is a domain-specific language to generate search queries that integrate advanced search operators. This language offers to exceed filters available on APIs. Example On the Operation Center, on the events page, the following query will match all not failed events received from the start of January 1st, 2020 to the end of January 2nd, 2020: NOT(error_code:\"Failed\") AND timestamp:>=2020-01-01T00:00:00Z AND timestamp:<2020-01-03T00:00:00Z Syntax A dork query contains one or more terms. Each terms hold a field, an operator and a literal. e.g: id:\"ALWyJiGeJSiw\" These terms can be combined though the use of logical operators. Literals Type Format Example String \"\\w+\" \"value\" Number \\d+[.,]\\d* 17.23 Boolean 0 (false) or 1 (true) 0 Date yyyy[-mm[-dd]] 2020-01-01 DateTime yyyy-mm-ddThh:mm:ss[.sss][+-hh:mm|Z] 2020-01-01T12:23:45.2342+02:00 Operators String operators Operator Description Example : The field must exactly match the literal type:\"malware\" :^ The field must start with the literal type:^\"Mal\" :$ The field must end with the literal type:$\"ware\" :~ The field must partially match the literal type:~\"dicat\" If the field represents a list: Operator Description Example : The field must match one item of the list tag:\"binary\" Number operators Operator Description Example : The field must equal the literal urgency:100 := The field must equal the literal urgency:=100 :> The field must be less than literal urgency:>10 :< The field must be greater than literal urgency:<50 Boolean operators Operator Description Example : The field must equal the literal deleted:1 Date operators Operator Description Example : The field must equal the literal timestamp:2020-01-01 := The field must equal the literal timestamp:=2020-01-01T12:13:45Z :> The field must be less than literal timestamp:>2020-01-01T00:00:00Z :< The field must be greater than literal timestamp:<2020-12-31T23:59:59+12:00 Logical operators Operator Description Example AND Match if both terms are verified timestamp:>2020-01-01T00:00:00Z AND type:$\"ware\" OR Match if any terms are verified timestamp:>2020-01-01T00:00:00Z OR type:^\"mal\" NOT Inverse the result of the term NOT type:^\"mal\" Grouping operators Operator Description Example () Groups operands (timestamp:>2020-01-01T00:00:00Z OR type:$\"ware\") AND NOT type:^\"mal\"","title":"Dork Language"},{"location":"searching/dork/#dork","text":"Dork is a domain-specific language to generate search queries that integrate advanced search operators. This language offers to exceed filters available on APIs.","title":"Dork"},{"location":"searching/dork/#example","text":"On the Operation Center, on the events page, the following query will match all not failed events received from the start of January 1st, 2020 to the end of January 2nd, 2020: NOT(error_code:\"Failed\") AND timestamp:>=2020-01-01T00:00:00Z AND timestamp:<2020-01-03T00:00:00Z","title":"Example"},{"location":"searching/dork/#syntax","text":"A dork query contains one or more terms. Each terms hold a field, an operator and a literal. e.g: id:\"ALWyJiGeJSiw\" These terms can be combined though the use of logical operators.","title":"Syntax"},{"location":"searching/dork/#literals","text":"Type Format Example String \"\\w+\" \"value\" Number \\d+[.,]\\d* 17.23 Boolean 0 (false) or 1 (true) 0 Date yyyy[-mm[-dd]] 2020-01-01 DateTime yyyy-mm-ddThh:mm:ss[.sss][+-hh:mm|Z] 2020-01-01T12:23:45.2342+02:00","title":"Literals"},{"location":"searching/dork/#operators","text":"","title":"Operators"},{"location":"searching/dork/#string-operators","text":"Operator Description Example : The field must exactly match the literal type:\"malware\" :^ The field must start with the literal type:^\"Mal\" :$ The field must end with the literal type:$\"ware\" :~ The field must partially match the literal type:~\"dicat\" If the field represents a list: Operator Description Example : The field must match one item of the list tag:\"binary\"","title":"String operators"},{"location":"searching/dork/#number-operators","text":"Operator Description Example : The field must equal the literal urgency:100 := The field must equal the literal urgency:=100 :> The field must be less than literal urgency:>10 :< The field must be greater than literal urgency:<50","title":"Number operators"},{"location":"searching/dork/#boolean-operators","text":"Operator Description Example : The field must equal the literal deleted:1","title":"Boolean operators"},{"location":"searching/dork/#date-operators","text":"Operator Description Example : The field must equal the literal timestamp:2020-01-01 := The field must equal the literal timestamp:=2020-01-01T12:13:45Z :> The field must be less than literal timestamp:>2020-01-01T00:00:00Z :< The field must be greater than literal timestamp:<2020-12-31T23:59:59+12:00","title":"Date operators"},{"location":"searching/dork/#logical-operators","text":"Operator Description Example AND Match if both terms are verified timestamp:>2020-01-01T00:00:00Z AND type:$\"ware\" OR Match if any terms are verified timestamp:>2020-01-01T00:00:00Z OR type:^\"mal\" NOT Inverse the result of the term NOT type:^\"mal\"","title":"Logical operators"},{"location":"searching/dork/#grouping-operators","text":"Operator Description Example () Groups operands (timestamp:>2020-01-01T00:00:00Z OR type:$\"ware\") AND NOT type:^\"mal\"","title":"Grouping operators"},{"location":"searching/search_events/","text":"Events Search The Events page exposes a search capability to investigate and hunt on your events. The search queries must follow the dork language . Type your search query in the box above the list of events to find expected events. Fields The Tables below detail all the fields that can be used to narrow down your search. In addition to them, one can filter the events based on their timestamps with the timestamp field. Action name type description action.id number Action unique identifier action.name string Name of the action action.outcome string Outcome status of the action Entity name type description entity.name string Name of the entity entity.uuid string Unique identifier of the entity Event name type description event.intake_key string The intake key event.original string The original message before normalization event.dialect string The intake format event.outcome string The parsing status (success or failure) Error name type description error.code string If ingestion failed, this field hosts the error Network name type description network.protocol string L7 Network protocol name. ex. http, lumberjack, transport protocol. network.transport string Protocol Name corresponding to the field iana_number . Destination name type description destination.ip string IP address of the destination. (IPv4 or IPv6) destination.port number Port of the destination destination.domain string Destination domain destination.packets number Packets sent from the destination to the source. Source name type description source.ip string IP address of the source. (IPv4 or IPv6) source.port number Port of the source source.domain string Source domain source.packets number Packets sent from the source to the destination. HTTP name type description http.request.method string HTTP request method http.response.status_code string HTTP response status code URL name type description url.original string Unmodified original url as seen in the event source. url.full string Full unparsed URL. DNS name type description dns.question.name string The name being queried. dns.question.type string The type of record being queried. dns.response_code string The DNS response code. User name type description user.id string Unique identifier of the user. user.name string Short name or login of the user. user.email string User email address. User Agent name type description user_agent.original string Unparsed user_agent string. Process name type description process.pid number Process Id process.name string Process name process.executable string Absolute path to the process executable. process.cmdline string Full command line that started the process. process.working_directory string he working directory of the process. process.ppid number Parent process' pid. process.parent.name string Parent process' name process.parent.executable string Parent process' executable Example Get valid event, from November 22nd to November 23rd 2019, that are neither apache nor nginx logs: timestamp:>=\"2019-11-22\" AND timestamp:<\"2019-11-23\" AND event.outcome:\"success\" AND NOT(event.dialect:\"apache\" OR event.dialect:\"nginx\")","title":"Querying Operation Center Events"},{"location":"searching/search_events/#events","text":"","title":"Events"},{"location":"searching/search_events/#search","text":"The Events page exposes a search capability to investigate and hunt on your events. The search queries must follow the dork language . Type your search query in the box above the list of events to find expected events.","title":"Search"},{"location":"searching/search_events/#fields","text":"The Tables below detail all the fields that can be used to narrow down your search. In addition to them, one can filter the events based on their timestamps with the timestamp field.","title":"Fields"},{"location":"searching/search_events/#action","text":"name type description action.id number Action unique identifier action.name string Name of the action action.outcome string Outcome status of the action","title":"Action"},{"location":"searching/search_events/#entity","text":"name type description entity.name string Name of the entity entity.uuid string Unique identifier of the entity","title":"Entity"},{"location":"searching/search_events/#event","text":"name type description event.intake_key string The intake key event.original string The original message before normalization event.dialect string The intake format event.outcome string The parsing status (success or failure)","title":"Event"},{"location":"searching/search_events/#error","text":"name type description error.code string If ingestion failed, this field hosts the error","title":"Error"},{"location":"searching/search_events/#network","text":"name type description network.protocol string L7 Network protocol name. ex. http, lumberjack, transport protocol. network.transport string Protocol Name corresponding to the field iana_number .","title":"Network"},{"location":"searching/search_events/#destination","text":"name type description destination.ip string IP address of the destination. (IPv4 or IPv6) destination.port number Port of the destination destination.domain string Destination domain destination.packets number Packets sent from the destination to the source.","title":"Destination"},{"location":"searching/search_events/#source","text":"name type description source.ip string IP address of the source. (IPv4 or IPv6) source.port number Port of the source source.domain string Source domain source.packets number Packets sent from the source to the destination.","title":"Source"},{"location":"searching/search_events/#http","text":"name type description http.request.method string HTTP request method http.response.status_code string HTTP response status code","title":"HTTP"},{"location":"searching/search_events/#url","text":"name type description url.original string Unmodified original url as seen in the event source. url.full string Full unparsed URL.","title":"URL"},{"location":"searching/search_events/#dns","text":"name type description dns.question.name string The name being queried. dns.question.type string The type of record being queried. dns.response_code string The DNS response code.","title":"DNS"},{"location":"searching/search_events/#user","text":"name type description user.id string Unique identifier of the user. user.name string Short name or login of the user. user.email string User email address.","title":"User"},{"location":"searching/search_events/#user-agent","text":"name type description user_agent.original string Unparsed user_agent string.","title":"User Agent"},{"location":"searching/search_events/#process","text":"name type description process.pid number Process Id process.name string Process name process.executable string Absolute path to the process executable. process.cmdline string Full command line that started the process. process.working_directory string he working directory of the process. process.ppid number Parent process' pid. process.parent.name string Parent process' name process.parent.executable string Parent process' executable","title":"Process"},{"location":"searching/search_events/#example","text":"Get valid event, from November 22nd to November 23rd 2019, that are neither apache nor nginx logs: timestamp:>=\"2019-11-22\" AND timestamp:<\"2019-11-23\" AND event.outcome:\"success\" AND NOT(event.dialect:\"apache\" OR event.dialect:\"nginx\")","title":"Example"}]}